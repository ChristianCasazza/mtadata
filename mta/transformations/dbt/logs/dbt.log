[0m02:33:17.302734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a047173a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a02dc3e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a02dc2230>]}


============================== 02:33:17.307488 | 9f511418-8fb2-4e68-9237-8a00a96a1d69 ==============================
[0m02:33:17.307488 [info ] [MainThread]: Running with dbt=1.8.7
[0m02:33:17.307945 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m02:33:17.313966 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'LAKE_PATH'
[0m02:33:17.315197 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.0594247, "process_user_time": 1.052545, "process_kernel_time": 0.222653, "process_mem_max_rss": "91316", "process_in_blocks": "21608", "process_out_blocks": "16", "command_success": false}
[0m02:33:17.315748 [debug] [MainThread]: Command `dbt run` failed at 02:33:17.315660 after 0.06 seconds
[0m02:33:17.316047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a047173a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a02dc0b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a02dc3f10>]}
[0m02:33:17.316354 [debug] [MainThread]: Flushing usage events
[0m02:47:49.216352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe032e77370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0329fabc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe031522b00>]}


============================== 02:47:49.222119 | 5fbe0b15-d8f0-429e-80c0-eef581f953b4 ==============================
[0m02:47:49.222119 [info ] [MainThread]: Running with dbt=1.8.7
[0m02:47:49.222645 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m02:47:49.447584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0314e39a0>]}
[0m02:47:49.504796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0314494e0>]}
[0m02:47:49.508584 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m02:47:49.516232 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m02:47:49.629226 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:47:49.629618 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:47:49.657894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0314c6110>]}
[0m02:47:49.792037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe026078f40>]}
[0m02:47:49.792870 [info ] [MainThread]: Found 13 models, 9 sources, 416 macros
[0m02:47:49.793291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe02607bd60>]}
[0m02:47:49.795069 [info ] [MainThread]: 
[0m02:47:49.795634 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m02:47:49.800687 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m02:47:49.994908 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m02:47:49.995324 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m02:47:49.995817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:47:50.023022 [debug] [ThreadPool]: SQL status: OK in 0.027 seconds
[0m02:47:50.024117 [debug] [ThreadPool]: On list_mtastats: Close
[0m02:47:50.026522 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m02:47:50.027294 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m02:47:50.032078 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m02:47:50.032353 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m02:47:50.032617 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:47:50.041222 [debug] [ThreadPool]: SQL status: OK in 0.009 seconds
[0m02:47:50.042397 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m02:47:50.042880 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m02:47:50.043599 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m02:47:50.044025 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m02:47:50.044418 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m02:47:50.045577 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m02:47:50.046340 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m02:47:50.046600 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m02:47:50.046856 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m02:47:50.047341 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m02:47:50.047681 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m02:47:50.051770 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m02:47:50.056170 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m02:47:50.056547 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m02:47:50.056957 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:47:50.064627 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m02:47:50.065109 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m02:47:50.065434 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m02:47:50.086612 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m02:47:50.088175 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m02:47:50.091005 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m02:47:50.091369 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m02:47:50.094665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe026251f00>]}
[0m02:47:50.095089 [debug] [MainThread]: Using duckdb connection "master"
[0m02:47:50.095349 [debug] [MainThread]: On master: BEGIN
[0m02:47:50.095565 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:47:50.102295 [debug] [MainThread]: SQL status: OK in 0.007 seconds
[0m02:47:50.102606 [debug] [MainThread]: On master: COMMIT
[0m02:47:50.102876 [debug] [MainThread]: Using duckdb connection "master"
[0m02:47:50.103095 [debug] [MainThread]: On master: COMMIT
[0m02:47:50.103498 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m02:47:50.103724 [debug] [MainThread]: On master: Close
[0m02:47:50.105420 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m02:47:50.105753 [info ] [MainThread]: 
[0m02:47:50.119255 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m02:47:50.121074 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m02:47:50.120391 [info ] [Thread-1 (]: 1 of 13 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m02:47:50.122126 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m02:47:50.123617 [info ] [Thread-2 (]: 2 of 13 START sql table model main.bond_payment_info ........................... [RUN]
[0m02:47:50.125126 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m02:47:50.126252 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m02:47:50.127433 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m02:47:50.127993 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m02:47:50.128619 [debug] [Thread-7 (]: Began running node model.mta.labor_expenses_per_agency
[0m02:47:50.129830 [info ] [Thread-3 (]: 3 of 13 START sql table model main.busiest_specific_times ...................... [RUN]
[0m02:47:50.130763 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m02:47:50.131452 [debug] [Thread-8 (]: Began running node model.mta.largest_expense_differences_2023
[0m02:47:50.132220 [debug] [Thread-9 (]: Began running node model.mta.omny_adoption_by_station
[0m02:47:50.132943 [info ] [Thread-4 (]: 4 of 13 START sql table model main.daily_ridership ............................. [RUN]
[0m02:47:50.134212 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_increase
[0m02:47:50.134853 [debug] [Thread-11 ]: Began running node model.mta.subway_station_stats
[0m02:47:50.137115 [debug] [Thread-12 ]: Began running node model.mta.total_riders_per_station
[0m02:47:50.139030 [debug] [Thread-13 ]: Began running node model.mta.weekly_riders_per_station
[0m02:47:50.136601 [info ] [Thread-5 (]: 5 of 13 START sql table model main.expense_type_per_year ....................... [RUN]
[0m02:47:50.140439 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m02:47:50.141269 [info ] [Thread-6 (]: 6 of 13 START sql table model main.fare_class_boro ............................. [RUN]
[0m02:47:50.142106 [info ] [Thread-7 (]: 7 of 13 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m02:47:50.143133 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m02:47:50.143776 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m02:47:50.144565 [info ] [Thread-8 (]: 8 of 13 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m02:47:50.145421 [info ] [Thread-9 (]: 9 of 13 START sql table model main.omny_adoption_by_station .................... [RUN]
[0m02:47:50.146444 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m02:47:50.147369 [info ] [Thread-10 ]: 10 of 13 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m02:47:50.148061 [info ] [Thread-11 ]: 11 of 13 START sql table model main.subway_station_stats ....................... [RUN]
[0m02:47:50.149028 [info ] [Thread-12 ]: 12 of 13 START sql table model main.total_riders_per_station ................... [RUN]
[0m02:47:50.149610 [info ] [Thread-13 ]: 13 of 13 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m02:47:50.150399 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m02:47:50.158548 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m02:47:50.162166 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m02:47:50.164324 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m02:47:50.165045 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m02:47:50.167748 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m02:47:50.168549 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m02:47:50.169188 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m02:47:50.169747 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m02:47:50.170482 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m02:47:50.171181 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m02:47:50.172222 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m02:47:50.172901 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m02:47:50.173391 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m02:47:50.174009 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m02:47:50.174855 [debug] [Thread-7 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m02:47:50.180737 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m02:47:50.181995 [debug] [Thread-8 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m02:47:50.182907 [debug] [Thread-9 (]: Began compiling node model.mta.omny_adoption_by_station
[0m02:47:50.205290 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m02:47:50.206104 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_increase
[0m02:47:50.206810 [debug] [Thread-11 ]: Began compiling node model.mta.subway_station_stats
[0m02:47:50.207421 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m02:47:50.207874 [debug] [Thread-12 ]: Began compiling node model.mta.total_riders_per_station
[0m02:47:50.208322 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m02:47:50.208920 [debug] [Thread-13 ]: Began compiling node model.mta.weekly_riders_per_station
[0m02:47:50.217801 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m02:47:50.273942 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m02:47:50.276137 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m02:47:50.281734 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m02:47:50.284377 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m02:47:50.285199 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m02:47:50.287954 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m02:47:50.293143 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m02:47:50.304377 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m02:47:50.311023 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m02:47:50.330208 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m02:47:50.332799 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m02:47:50.333441 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m02:47:50.337323 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m02:47:50.339400 [debug] [Thread-9 (]: Began executing node model.mta.omny_adoption_by_station
[0m02:47:50.340384 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m02:47:50.340884 [debug] [Thread-8 (]: Began executing node model.mta.largest_expense_differences_2023
[0m02:47:50.341678 [debug] [Thread-7 (]: Began executing node model.mta.labor_expenses_per_agency
[0m02:47:50.342729 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_increase
[0m02:47:50.345601 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m02:47:50.345995 [debug] [Thread-11 ]: Began executing node model.mta.subway_station_stats
[0m02:47:50.346848 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m02:47:50.347321 [debug] [Thread-12 ]: Began executing node model.mta.total_riders_per_station
[0m02:47:50.348182 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m02:47:50.352178 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m02:47:50.353768 [debug] [Thread-13 ]: Began executing node model.mta.weekly_riders_per_station
[0m02:47:50.354338 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m02:47:50.354856 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m02:47:50.358694 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m02:47:50.362312 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m02:47:50.364718 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m02:47:50.369023 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m02:47:50.371741 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m02:47:50.374788 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m02:47:50.377706 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m02:47:50.378252 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m02:47:50.382954 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m02:47:50.384207 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m02:47:50.384705 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m02:47:50.385097 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m02:47:50.385594 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m02:47:50.386596 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m02:47:50.387068 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m02:47:50.389505 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m02:47:50.390948 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m02:47:50.391675 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m02:47:50.392115 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m02:47:50.392678 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m02:47:50.393495 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m02:47:50.393873 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m02:47:50.394318 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:47:50.395005 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: BEGIN
[0m02:47:50.395391 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m02:47:50.395885 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m02:47:50.396314 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m02:47:50.396762 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m02:47:50.397180 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: BEGIN
[0m02:47:50.397598 [debug] [Thread-11 ]: On model.mta.subway_station_stats: BEGIN
[0m02:47:50.398042 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m02:47:50.398458 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: BEGIN
[0m02:47:50.420847 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:47:50.421545 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m02:47:50.422572 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m02:47:50.423753 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m02:47:50.424192 [debug] [Thread-1 (]: SQL status: OK in 0.032 seconds
[0m02:47:50.425102 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m02:47:50.425670 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m02:47:50.426152 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m02:47:50.426652 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m02:47:50.427254 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m02:47:50.427985 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m02:47:50.428694 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m02:47:50.429118 [debug] [Thread-3 (]: SQL status: OK in 0.035 seconds
[0m02:47:50.430161 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m02:47:50.430669 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m02:47:50.432826 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m02:47:50.434531 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m02:47:50.435276 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m02:47:50.436291 [debug] [Thread-2 (]: SQL status: OK in 0.040 seconds
[0m02:47:50.439563 [debug] [Thread-4 (]: SQL status: OK in 0.019 seconds
[0m02:47:50.440465 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m02:47:50.441058 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m02:47:50.441628 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m02:47:50.442348 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m02:47:50.447892 [debug] [Thread-9 (]: SQL status: OK in 0.025 seconds
[0m02:47:50.449020 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m02:47:50.449482 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m02:47:50.453931 [debug] [Thread-5 (]: SQL status: OK in 0.029 seconds
[0m02:47:50.454599 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m02:47:50.455346 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m02:47:50.459657 [debug] [Thread-8 (]: SQL status: OK in 0.034 seconds
[0m02:47:50.460614 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m02:47:50.462275 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m02:47:50.469227 [debug] [Thread-10 ]: SQL status: OK in 0.043 seconds
[0m02:47:50.470043 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m02:47:50.470681 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m02:47:50.473926 [debug] [Thread-11 ]: SQL status: OK in 0.047 seconds
[0m02:47:50.475019 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m02:47:50.475646 [debug] [Thread-11 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m02:47:50.479325 [debug] [Thread-7 (]: SQL status: OK in 0.052 seconds
[0m02:47:50.479971 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m02:47:50.480549 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency,
    fiscal_year,
    expense_type,
    general_ledger
ORDER BY 
    agency, fiscal_year, expense_type, general_ledger
    );
  
  
[0m02:47:50.490070 [debug] [Thread-12 ]: SQL status: OK in 0.062 seconds
[0m02:47:50.491168 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m02:47:50.491934 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m02:47:50.499683 [debug] [Thread-6 (]: SQL status: OK in 0.071 seconds
[0m02:47:50.500462 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m02:47:50.501291 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m02:47:50.513465 [debug] [Thread-13 ]: SQL status: OK in 0.083 seconds
[0m02:47:50.514304 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m02:47:50.515059 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      with weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
)
SELECT 
    station_complex, 
    week_start, 
    total_weekly_ridership
FROM 
    weekly_ridership
ORDER BY 
    station_complex, 
    week_start
    );
  
  
[0m02:47:51.020368 [debug] [Thread-7 (]: SQL status: OK in 0.539 seconds
[0m02:47:51.039738 [debug] [Thread-5 (]: SQL status: OK in 0.581 seconds
[0m02:47:51.049240 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m02:47:51.058589 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m02:47:51.064176 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m02:47:51.065690 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m02:47:51.072472 [debug] [Thread-8 (]: SQL status: OK in 0.609 seconds
[0m02:47:51.083804 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m02:47:51.085042 [debug] [Thread-5 (]: SQL status: OK in 0.016 seconds
[0m02:47:51.085615 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m02:47:51.086176 [debug] [Thread-7 (]: SQL status: OK in 0.018 seconds
[0m02:47:51.294114 [debug] [Thread-4 (]: SQL status: OK in 0.845 seconds
[0m02:47:51.302369 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m02:47:51.304466 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m02:47:51.308863 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m02:47:51.309468 [debug] [Thread-8 (]: SQL status: OK in 0.197 seconds
[0m02:47:51.310216 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m02:47:51.310876 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m02:47:51.311516 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m02:47:51.313869 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m02:47:51.314576 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m02:47:51.315175 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m02:47:51.315966 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m02:47:51.317089 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m02:47:51.319275 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m02:47:51.321538 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m02:47:51.322124 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m02:47:51.322604 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m02:47:51.348310 [debug] [Thread-5 (]: SQL status: OK in 0.032 seconds
[0m02:47:51.360354 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m02:47:51.363786 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m02:47:51.366474 [debug] [Thread-7 (]: SQL status: OK in 0.050 seconds
[0m02:47:51.369842 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m02:47:51.370294 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m02:47:51.376818 [debug] [Thread-8 (]: SQL status: OK in 0.059 seconds
[0m02:47:51.379359 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m02:47:51.379887 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m02:47:51.475200 [debug] [Thread-4 (]: SQL status: OK in 0.152 seconds
[0m02:47:51.478512 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m02:47:51.479353 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m02:47:51.498305 [debug] [Thread-5 (]: SQL status: OK in 0.132 seconds
[0m02:47:51.502843 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m02:47:51.506591 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0260609a0>]}
[0m02:47:51.508325 [debug] [Thread-8 (]: SQL status: OK in 0.128 seconds
[0m02:47:51.509699 [info ] [Thread-5 (]: 5 of 13 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 1.35s]
[0m02:47:51.510646 [debug] [Thread-7 (]: SQL status: OK in 0.135 seconds
[0m02:47:51.512539 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: Close
[0m02:47:51.513164 [debug] [Thread-4 (]: SQL status: OK in 0.033 seconds
[0m02:47:51.514401 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m02:47:51.516581 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: Close
[0m02:47:51.517728 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0254bab60>]}
[0m02:47:51.519901 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m02:47:51.521563 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0254ba9b0>]}
[0m02:47:51.522801 [info ] [Thread-8 (]: 8 of 13 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 1.35s]
[0m02:47:51.523996 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe026062410>]}
[0m02:47:51.525232 [info ] [Thread-7 (]: 7 of 13 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 1.36s]
[0m02:47:51.526754 [debug] [Thread-8 (]: Finished running node model.mta.largest_expense_differences_2023
[0m02:47:51.527907 [info ] [Thread-4 (]: 4 of 13 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 1.38s]
[0m02:47:51.529193 [debug] [Thread-7 (]: Finished running node model.mta.labor_expenses_per_agency
[0m02:47:51.530794 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m02:47:52.200152 [debug] [Thread-2 (]: SQL status: OK in 1.757 seconds
[0m02:47:52.216394 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m02:47:52.217257 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m02:47:52.218484 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m02:47:52.226677 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m02:47:52.227751 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m02:47:52.228549 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m02:47:52.239242 [debug] [Thread-2 (]: SQL status: OK in 0.009 seconds
[0m02:47:52.247990 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m02:47:52.249221 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m02:47:52.250900 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m02:47:52.258463 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m02:47:52.260261 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0260259c0>]}
[0m02:47:52.261787 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 2.13s]
[0m02:47:52.264424 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m02:47:54.811870 [debug] [Thread-3 (]: SQL status: OK in 4.371 seconds
[0m02:47:54.828186 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m02:47:54.834188 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m02:47:54.840521 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m02:47:54.844564 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m02:47:54.845530 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m02:47:54.846379 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m02:47:54.858771 [debug] [Thread-3 (]: SQL status: OK in 0.011 seconds
[0m02:47:54.895232 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m02:47:54.902388 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m02:47:54.905596 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m02:47:54.908495 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m02:47:54.909695 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5fbe0b15-d8f0-429e-80c0-eef581f953b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0261cfee0>]}
[0m02:47:54.911533 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 4.77s]
[0m02:47:54.913150 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m02:47:56.008739 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.avg_riders_per_day. Details: Connection(type='duckdb', name='model.mta.avg_riders_per_day', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fe02479b4f0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m02:47:56.010981 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.avg_riders_per_day
[0m02:47:56.019875 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.fare_class_boro. Details: Connection(type='duckdb', name='model.mta.fare_class_boro', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fe02539b4c0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m02:47:56.021070 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.fare_class_boro
[0m02:47:56.022037 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.omny_adoption_by_station. Details: Connection(type='duckdb', name='model.mta.omny_adoption_by_station', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fe02472b160>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m02:47:56.023332 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.omny_adoption_by_station
[0m02:47:56.024759 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.omny_adoption_increase. Details: Connection(type='duckdb', name='model.mta.omny_adoption_increase', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fe02538fd60>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m02:47:56.025666 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.omny_adoption_increase
[0m02:47:56.026439 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.subway_station_stats. Details: Connection(type='duckdb', name='model.mta.subway_station_stats', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fe025399c90>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m02:47:56.027220 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.subway_station_stats
[0m02:47:56.028137 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.total_riders_per_station. Details: Connection(type='duckdb', name='model.mta.total_riders_per_station', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fe0253757b0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m02:47:56.028832 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.total_riders_per_station
[0m02:47:56.029457 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.weekly_riders_per_station. Details: Connection(type='duckdb', name='model.mta.weekly_riders_per_station', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fe025374790>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m02:47:56.030017 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.weekly_riders_per_station
[0m02:47:56.030598 [error] [MainThread]: CANCEL query model.mta.expense_type_per_year ................................... [[31mCANCEL[0m]
[0m02:47:56.031235 [error] [MainThread]: CANCEL query model.mta.avg_riders_per_day ...................................... [[31mCANCEL[0m]
[0m02:47:56.031707 [error] [MainThread]: CANCEL query model.mta.bond_payment_info ....................................... [[31mCANCEL[0m]
[0m02:47:56.032100 [error] [MainThread]: CANCEL query model.mta.busiest_specific_times .................................. [[31mCANCEL[0m]
[0m02:47:56.032586 [error] [MainThread]: CANCEL query model.mta.daily_ridership ......................................... [[31mCANCEL[0m]
[0m02:47:56.033194 [error] [MainThread]: CANCEL query model.mta.fare_class_boro ......................................... [[31mCANCEL[0m]
[0m02:47:56.033675 [error] [MainThread]: CANCEL query model.mta.labor_expenses_per_agency ............................... [[31mCANCEL[0m]
[0m02:47:56.034702 [error] [MainThread]: CANCEL query model.mta.largest_expense_differences_2023 ........................ [[31mCANCEL[0m]
[0m02:47:56.035523 [error] [MainThread]: CANCEL query model.mta.omny_adoption_by_station ................................ [[31mCANCEL[0m]
[0m02:47:56.036560 [error] [MainThread]: CANCEL query model.mta.omny_adoption_increase .................................. [[31mCANCEL[0m]
[0m02:47:56.037435 [error] [MainThread]: CANCEL query model.mta.subway_station_stats .................................... [[31mCANCEL[0m]
[0m02:47:56.037971 [error] [MainThread]: CANCEL query model.mta.total_riders_per_station ................................ [[31mCANCEL[0m]
[0m02:47:56.038392 [error] [MainThread]: CANCEL query model.mta.weekly_riders_per_station ............................... [[31mCANCEL[0m]
[0m02:47:56.053059 [debug] [Thread-13 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      with weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
)
SELECT 
    station_complex, 
    week_start, 
    total_weekly_ridership
FROM 
    weekly_ridership
ORDER BY 
    station_complex, 
    week_start
    );
  
  
[0m02:47:56.055317 [debug] [Thread-10 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m02:47:56.057051 [debug] [Thread-12 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m02:47:56.060853 [debug] [Thread-6 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m02:47:56.064249 [debug] [Thread-9 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m02:47:56.066215 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m02:47:56.067479 [debug] [Thread-13 ]: DuckDB adapter: Rolling back transaction.
[0m02:47:56.070219 [debug] [Thread-11 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m02:47:56.072290 [debug] [Thread-10 ]: DuckDB adapter: Rolling back transaction.
[0m02:47:56.073427 [debug] [Thread-12 ]: DuckDB adapter: Rolling back transaction.
[0m02:47:56.074139 [debug] [Thread-6 (]: DuckDB adapter: Rolling back transaction.
[0m02:47:56.074640 [debug] [Thread-9 (]: DuckDB adapter: Rolling back transaction.
[0m02:47:56.075161 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m02:47:56.076492 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: ROLLBACK
[0m02:47:56.077092 [debug] [Thread-11 ]: DuckDB adapter: Rolling back transaction.
[0m02:47:56.077965 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: ROLLBACK
[0m02:47:56.078889 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: ROLLBACK
[0m02:47:56.079959 [debug] [Thread-6 (]: On model.mta.fare_class_boro: ROLLBACK
[0m02:47:56.080693 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: ROLLBACK
[0m02:47:56.081819 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: ROLLBACK
[0m02:47:56.083761 [debug] [Thread-11 ]: On model.mta.subway_station_stats: ROLLBACK
[0m02:47:56.119904 [debug] [Thread-11 ]: Failed to rollback 'model.mta.subway_station_stats'
[0m02:47:56.121240 [debug] [Thread-1 (]: Failed to rollback 'model.mta.avg_riders_per_day'
[0m02:47:56.121731 [debug] [Thread-11 ]: On model.mta.subway_station_stats: Close
[0m02:47:56.122380 [debug] [Thread-13 ]: Failed to rollback 'model.mta.weekly_riders_per_station'
[0m02:47:56.123041 [debug] [Thread-9 (]: Failed to rollback 'model.mta.omny_adoption_by_station'
[0m02:47:56.123597 [debug] [Thread-6 (]: Failed to rollback 'model.mta.fare_class_boro'
[0m02:47:56.124148 [debug] [Thread-10 ]: Failed to rollback 'model.mta.omny_adoption_increase'
[0m02:47:56.124627 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m02:47:56.125118 [debug] [Thread-12 ]: Failed to rollback 'model.mta.total_riders_per_station'
[0m02:47:56.126198 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: Close
[0m02:47:56.127012 [debug] [Thread-11 ]: Runtime Error in model subway_station_stats (models/subway_station_stats.sql)
  INTERRUPT Error: Interrupted!
[0m02:47:56.127418 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: Close
[0m02:47:56.127783 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m02:47:56.128172 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: Close
[0m02:47:56.129339 [debug] [Thread-1 (]: Runtime Error in model avg_riders_per_day (models/avg_riders_per_day.sql)
  INTERRUPT Error: Interrupted!
[0m02:47:56.129692 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: Close
[0m04:48:11.184871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e1e2373a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e1de09db0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e1c8e2a40>]}


============================== 04:48:11.189124 | c8b17d74-2f13-4fb9-8b94-67584b1fa7b4 ==============================
[0m04:48:11.189124 [info ] [MainThread]: Running with dbt=1.8.7
[0m04:48:11.189606 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:48:11.194760 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'LAKE_PATH'
[0m04:48:11.197082 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.058016445, "process_user_time": 1.078374, "process_kernel_time": 0.405542, "process_mem_max_rss": "90988", "process_out_blocks": "8", "command_success": false, "process_in_blocks": "0"}
[0m04:48:11.197761 [debug] [MainThread]: Command `dbt run` failed at 04:48:11.197640 after 0.06 seconds
[0m04:48:11.198163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e1e2373a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e1c97ae00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e1c97b520>]}
[0m04:48:11.198603 [debug] [MainThread]: Flushing usage events
[0m04:49:24.174990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bda4e33d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bd8b8fa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bd8b8e230>]}


============================== 04:49:24.177331 | 797a8a5e-0376-4662-ae18-2a5d36c39116 ==============================
[0m04:49:24.177331 [info ] [MainThread]: Running with dbt=1.8.7
[0m04:49:24.177836 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:49:24.181466 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'LAKE_PATH'
[0m04:49:24.184154 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.053177968, "process_user_time": 0.872276, "process_kernel_time": 0.018363, "process_mem_max_rss": "91268", "process_out_blocks": "16", "command_success": false, "process_in_blocks": "0"}
[0m04:49:24.184904 [debug] [MainThread]: Command `dbt run` failed at 04:49:24.184736 after 0.05 seconds
[0m04:49:24.185991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bda4e33d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bd8c26dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bd8c274f0>]}
[0m04:49:24.186585 [debug] [MainThread]: Flushing usage events
[0m04:52:42.898642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd19afdf430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd199689c30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd19968b5b0>]}


============================== 04:52:42.900671 | b76fbfd5-4ea6-493c-aedd-11146832e425 ==============================
[0m04:52:42.900671 [info ] [MainThread]: Running with dbt=1.8.7
[0m04:52:42.901269 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m04:52:43.173939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd19b0a4a30>]}
[0m04:52:43.227653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1995fcd60>]}
[0m04:52:43.232720 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m04:52:43.245746 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m04:52:43.385134 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m04:52:43.385554 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m04:52:43.420532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd199629720>]}
[0m04:52:43.498137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18e10b6d0>]}
[0m04:52:43.498755 [info ] [MainThread]: Found 13 models, 9 sources, 416 macros
[0m04:52:43.499218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18e10b790>]}
[0m04:52:43.500967 [info ] [MainThread]: 
[0m04:52:43.501589 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m04:52:43.505936 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m04:52:43.593697 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m04:52:43.594162 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m04:52:43.594508 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:52:43.624413 [debug] [ThreadPool]: SQL status: OK in 0.030 seconds
[0m04:52:43.626335 [debug] [ThreadPool]: On list_mtastats: Close
[0m04:52:43.630284 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m04:52:43.630906 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m04:52:43.635567 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m04:52:43.635879 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m04:52:43.636160 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:43.646469 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m04:52:43.648298 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m04:52:43.648793 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m04:52:43.650521 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m04:52:43.650801 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m04:52:43.651105 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m04:52:43.651749 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m04:52:43.652449 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m04:52:43.652709 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m04:52:43.652972 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m04:52:43.653437 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m04:52:43.653727 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m04:52:43.657429 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m04:52:43.662767 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m04:52:43.663622 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m04:52:43.664081 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:52:43.670130 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m04:52:43.670611 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m04:52:43.671107 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m04:52:43.707374 [debug] [ThreadPool]: SQL status: OK in 0.036 seconds
[0m04:52:43.709346 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m04:52:43.711042 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m04:52:43.711379 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m04:52:43.715204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18e167760>]}
[0m04:52:43.715771 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:43.716183 [debug] [MainThread]: On master: BEGIN
[0m04:52:43.716560 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:52:43.739442 [debug] [MainThread]: SQL status: OK in 0.023 seconds
[0m04:52:43.740206 [debug] [MainThread]: On master: COMMIT
[0m04:52:43.740566 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:43.740871 [debug] [MainThread]: On master: COMMIT
[0m04:52:43.741596 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m04:52:43.742052 [debug] [MainThread]: On master: Close
[0m04:52:43.744593 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m04:52:43.745220 [info ] [MainThread]: 
[0m04:52:43.756602 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m04:52:43.757334 [info ] [Thread-1 (]: 1 of 13 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m04:52:43.758388 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m04:52:43.758791 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m04:52:43.765119 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m04:52:43.765868 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m04:52:43.767315 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m04:52:43.767794 [info ] [Thread-2 (]: 2 of 13 START sql table model main.bond_payment_info ........................... [RUN]
[0m04:52:43.768629 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m04:52:43.769359 [info ] [Thread-3 (]: 3 of 13 START sql table model main.busiest_specific_times ...................... [RUN]
[0m04:52:43.770289 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m04:52:43.770966 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m04:52:43.771614 [debug] [Thread-7 (]: Began running node model.mta.labor_expenses_per_agency
[0m04:52:43.772350 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m04:52:43.772766 [debug] [Thread-8 (]: Began running node model.mta.largest_expense_differences_2023
[0m04:52:43.773935 [debug] [Thread-9 (]: Began running node model.mta.omny_adoption_by_station
[0m04:52:43.774405 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m04:52:43.775091 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_increase
[0m04:52:43.775739 [info ] [Thread-4 (]: 4 of 13 START sql table model main.daily_ridership ............................. [RUN]
[0m04:52:43.776594 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m04:52:43.777197 [debug] [Thread-11 ]: Began running node model.mta.subway_station_stats
[0m04:52:43.778147 [info ] [Thread-5 (]: 5 of 13 START sql table model main.expense_type_per_year ....................... [RUN]
[0m04:52:43.779025 [debug] [Thread-12 ]: Began running node model.mta.total_riders_per_station
[0m04:52:43.780114 [debug] [Thread-13 ]: Began running node model.mta.weekly_riders_per_station
[0m04:52:43.782434 [info ] [Thread-6 (]: 6 of 13 START sql table model main.fare_class_boro ............................. [RUN]
[0m04:52:43.783809 [info ] [Thread-7 (]: 7 of 13 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m04:52:43.784556 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m04:52:43.785338 [info ] [Thread-8 (]: 8 of 13 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m04:52:43.786128 [info ] [Thread-9 (]: 9 of 13 START sql table model main.omny_adoption_by_station .................... [RUN]
[0m04:52:43.810043 [info ] [Thread-10 ]: 10 of 13 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m04:52:43.823480 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m04:52:43.827114 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m04:52:43.827904 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m04:52:43.828590 [info ] [Thread-11 ]: 11 of 13 START sql table model main.subway_station_stats ....................... [RUN]
[0m04:52:43.829341 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m04:52:43.830061 [info ] [Thread-12 ]: 12 of 13 START sql table model main.total_riders_per_station ................... [RUN]
[0m04:52:43.830884 [info ] [Thread-13 ]: 13 of 13 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m04:52:43.831872 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m04:52:43.832736 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m04:52:43.840463 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m04:52:43.841589 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m04:52:43.842503 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m04:52:43.843261 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m04:52:43.844416 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m04:52:43.909257 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m04:52:43.911043 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m04:52:43.911799 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m04:52:43.912637 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m04:52:43.913366 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m04:52:43.913964 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m04:52:43.914507 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m04:52:43.915038 [debug] [Thread-7 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m04:52:43.916693 [debug] [Thread-8 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m04:52:43.917209 [debug] [Thread-9 (]: Began compiling node model.mta.omny_adoption_by_station
[0m04:52:43.917794 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_increase
[0m04:52:43.924540 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m04:52:43.925486 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m04:52:43.925955 [debug] [Thread-11 ]: Began compiling node model.mta.subway_station_stats
[0m04:52:43.928269 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m04:52:43.928881 [debug] [Thread-12 ]: Began compiling node model.mta.total_riders_per_station
[0m04:52:43.929349 [debug] [Thread-13 ]: Began compiling node model.mta.weekly_riders_per_station
[0m04:52:43.932645 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m04:52:43.933124 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m04:52:43.933708 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m04:52:43.936835 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m04:52:43.945279 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m04:52:43.947123 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m04:52:43.959492 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m04:52:43.963676 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m04:52:43.972170 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m04:52:43.972885 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m04:52:43.978410 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m04:52:43.985612 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m04:52:43.991323 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m04:52:43.991967 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m04:52:43.992562 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m04:52:43.994243 [debug] [Thread-7 (]: Began executing node model.mta.labor_expenses_per_agency
[0m04:52:43.996244 [debug] [Thread-8 (]: Began executing node model.mta.largest_expense_differences_2023
[0m04:52:43.997293 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m04:52:44.000916 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m04:52:44.001985 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_increase
[0m04:52:44.003043 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m04:52:44.003481 [debug] [Thread-9 (]: Began executing node model.mta.omny_adoption_by_station
[0m04:52:44.003891 [debug] [Thread-11 ]: Began executing node model.mta.subway_station_stats
[0m04:52:44.012119 [debug] [Thread-13 ]: Began executing node model.mta.weekly_riders_per_station
[0m04:52:44.012576 [debug] [Thread-12 ]: Began executing node model.mta.total_riders_per_station
[0m04:52:44.016813 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m04:52:44.017884 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m04:52:44.023003 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m04:52:44.025825 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m04:52:44.026629 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m04:52:44.031081 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m04:52:44.031755 [debug] [Thread-1 (]: SQL status: OK in 0.040 seconds
[0m04:52:44.037202 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m04:52:44.037721 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m04:52:44.044818 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m04:52:44.048817 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m04:52:44.056223 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m04:52:44.059563 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m04:52:44.060333 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m04:52:44.061442 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m04:52:44.062839 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m04:52:44.063755 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m04:52:44.064283 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m04:52:44.064938 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m04:52:44.065511 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m04:52:44.066012 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m04:52:44.066481 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m04:52:44.067500 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m04:52:44.068476 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m04:52:44.069991 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m04:52:44.070554 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m04:52:44.071026 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m04:52:44.071747 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m04:52:44.072286 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m04:52:44.072859 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m04:52:44.073838 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m04:52:44.074590 [debug] [Thread-2 (]: SQL status: OK in 0.013 seconds
[0m04:52:44.075014 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: BEGIN
[0m04:52:44.075567 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m04:52:44.076069 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m04:52:44.076575 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: BEGIN
[0m04:52:44.077320 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m04:52:44.077798 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: BEGIN
[0m04:52:44.078250 [debug] [Thread-11 ]: On model.mta.subway_station_stats: BEGIN
[0m04:52:44.078678 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m04:52:44.079172 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m04:52:44.080894 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m04:52:44.081745 [debug] [Thread-3 (]: SQL status: OK in 0.013 seconds
[0m04:52:44.082452 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m04:52:44.082949 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m04:52:44.083869 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m04:52:44.084575 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m04:52:44.085309 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m04:52:44.086014 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m04:52:44.086859 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m04:52:44.087959 [debug] [Thread-4 (]: SQL status: OK in 0.012 seconds
[0m04:52:44.088477 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m04:52:44.089078 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m04:52:44.090854 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m04:52:44.091408 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m04:52:44.094364 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m04:52:44.095166 [debug] [Thread-5 (]: SQL status: OK in 0.018 seconds
[0m04:52:44.096229 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m04:52:44.096627 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m04:52:44.098092 [debug] [Thread-7 (]: SQL status: OK in 0.019 seconds
[0m04:52:44.098474 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m04:52:44.098902 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency,
    fiscal_year,
    expense_type,
    general_ledger
ORDER BY 
    agency, fiscal_year, expense_type, general_ledger
    );
  
  
[0m04:52:44.114780 [debug] [Thread-8 (]: SQL status: OK in 0.034 seconds
[0m04:52:44.115455 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m04:52:44.116023 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m04:52:44.131341 [debug] [Thread-10 ]: SQL status: OK in 0.048 seconds
[0m04:52:44.132064 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m04:52:44.133033 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m04:52:44.142772 [debug] [Thread-6 (]: SQL status: OK in 0.059 seconds
[0m04:52:44.143273 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m04:52:44.143936 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m04:52:44.158512 [debug] [Thread-9 (]: SQL status: OK in 0.074 seconds
[0m04:52:44.159083 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m04:52:44.159589 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m04:52:44.173111 [debug] [Thread-12 ]: SQL status: OK in 0.088 seconds
[0m04:52:44.173901 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m04:52:44.174382 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m04:52:44.185761 [debug] [Thread-11 ]: SQL status: OK in 0.100 seconds
[0m04:52:44.186719 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m04:52:44.187792 [debug] [Thread-11 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m04:52:44.208024 [debug] [Thread-13 ]: SQL status: OK in 0.121 seconds
[0m04:52:44.208739 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m04:52:44.209307 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      with weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
)
SELECT 
    station_complex, 
    week_start, 
    total_weekly_ridership
FROM 
    weekly_ridership
ORDER BY 
    station_complex, 
    week_start
    );
  
  
[0m04:52:44.734598 [debug] [Thread-5 (]: SQL status: OK in 0.637 seconds
[0m04:52:44.768007 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m04:52:44.768898 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m04:52:44.773955 [debug] [Thread-5 (]: SQL status: OK in 0.004 seconds
[0m04:52:44.995252 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m04:52:44.996160 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m04:52:44.996928 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m04:52:44.998461 [debug] [Thread-7 (]: SQL status: OK in 0.899 seconds
[0m04:52:45.002522 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m04:52:45.003136 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m04:52:45.004213 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m04:52:45.007974 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m04:52:45.009741 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m04:52:45.016492 [debug] [Thread-5 (]: SQL status: OK in 0.019 seconds
[0m04:52:45.017057 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m04:52:45.067925 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m04:52:45.068950 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m04:52:45.075195 [debug] [Thread-7 (]: SQL status: OK in 0.032 seconds
[0m04:52:45.090172 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m04:52:45.092073 [debug] [Thread-5 (]: SQL status: OK in 0.019 seconds
[0m04:52:45.092981 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m04:52:45.190172 [debug] [Thread-4 (]: SQL status: OK in 1.094 seconds
[0m04:52:45.191471 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m04:52:45.199738 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd19b06b550>]}
[0m04:52:45.200721 [info ] [Thread-5 (]: 5 of 13 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 1.37s]
[0m04:52:45.202752 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m04:52:45.203378 [debug] [Thread-7 (]: SQL status: OK in 0.014 seconds
[0m04:52:45.204816 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m04:52:45.205555 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m04:52:45.207326 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: Close
[0m04:52:45.209442 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18d3b8b20>]}
[0m04:52:45.210803 [info ] [Thread-7 (]: 7 of 13 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 1.38s]
[0m04:52:45.212018 [debug] [Thread-7 (]: Finished running node model.mta.labor_expenses_per_agency
[0m04:52:45.212987 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m04:52:45.218334 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m04:52:45.219037 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m04:52:45.219461 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m04:52:45.240386 [debug] [Thread-4 (]: SQL status: OK in 0.019 seconds
[0m04:52:45.245097 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m04:52:45.245923 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m04:52:45.248193 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m04:52:45.250286 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m04:52:45.251118 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18d5b4ac0>]}
[0m04:52:45.251866 [info ] [Thread-4 (]: 4 of 13 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 1.43s]
[0m04:52:45.252761 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m04:52:45.310290 [debug] [Thread-8 (]: SQL status: OK in 1.192 seconds
[0m04:52:45.318787 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m04:52:45.319928 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m04:52:45.322497 [debug] [Thread-8 (]: SQL status: OK in 0.002 seconds
[0m04:52:45.329216 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m04:52:45.330794 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m04:52:45.331433 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m04:52:45.336364 [debug] [Thread-8 (]: SQL status: OK in 0.004 seconds
[0m04:52:45.342232 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m04:52:45.343413 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m04:52:45.347935 [debug] [Thread-8 (]: SQL status: OK in 0.004 seconds
[0m04:52:45.351418 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: Close
[0m04:52:45.352935 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18d462f20>]}
[0m04:52:45.354424 [info ] [Thread-8 (]: 8 of 13 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 1.51s]
[0m04:52:45.355158 [debug] [Thread-8 (]: Finished running node model.mta.largest_expense_differences_2023
[0m04:52:45.868373 [debug] [Thread-2 (]: SQL status: OK in 1.776 seconds
[0m04:52:45.912886 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m04:52:45.913944 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m04:52:45.915168 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m04:52:45.918892 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m04:52:45.919888 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m04:52:45.920511 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m04:52:45.927012 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m04:52:45.964485 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m04:52:45.966123 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m04:52:45.968003 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m04:52:45.971932 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m04:52:45.973992 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18d463f10>]}
[0m04:52:45.980716 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 2.20s]
[0m04:52:45.985552 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m04:52:50.069046 [debug] [Thread-1 (]: SQL status: OK in 5.979 seconds
[0m04:52:50.228307 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m04:52:50.240776 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m04:52:50.256652 [debug] [Thread-1 (]: SQL status: OK in 0.012 seconds
[0m04:52:50.283742 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m04:52:50.284785 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m04:52:50.285538 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m04:52:50.334279 [debug] [Thread-1 (]: SQL status: OK in 0.048 seconds
[0m04:52:50.339850 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m04:52:50.340780 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m04:52:50.342312 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m04:52:50.347169 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m04:52:50.368932 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18e14ebc0>]}
[0m04:52:50.399869 [info ] [Thread-1 (]: 1 of 13 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 6.61s]
[0m04:52:50.431793 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m04:52:51.328598 [debug] [Thread-3 (]: SQL status: OK in 7.232 seconds
[0m04:52:51.371586 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m04:52:51.372763 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m04:52:51.390786 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m04:52:51.394264 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m04:52:51.395239 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m04:52:51.396064 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m04:52:51.407243 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m04:52:51.414537 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m04:52:51.415682 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m04:52:51.417245 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m04:52:51.443568 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m04:52:51.460574 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18e174f10>]}
[0m04:52:51.462422 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 7.68s]
[0m04:52:51.464574 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m04:52:53.410021 [debug] [Thread-13 ]: SQL status: OK in 9.200 seconds
[0m04:52:53.424260 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m04:52:53.425225 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m04:52:53.428177 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m04:52:53.431736 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m04:52:53.444890 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m04:52:53.452447 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m04:52:53.474991 [debug] [Thread-6 (]: SQL status: OK in 9.330 seconds
[0m04:52:53.485907 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m04:52:53.494678 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m04:52:53.496280 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m04:52:53.499293 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m04:52:53.500389 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m04:52:53.502786 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m04:52:53.501184 [debug] [Thread-13 ]: SQL status: OK in 0.047 seconds
[0m04:52:53.508964 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m04:52:53.520440 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m04:52:53.521828 [debug] [Thread-13 ]: SQL status: OK in 0.000 seconds
[0m04:52:53.524782 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: Close
[0m04:52:53.526713 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18d46cee0>]}
[0m04:52:53.528422 [info ] [Thread-13 ]: 13 of 13 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 9.61s]
[0m04:52:53.529815 [debug] [Thread-13 ]: Finished running node model.mta.weekly_riders_per_station
[0m04:52:53.550574 [debug] [Thread-6 (]: SQL status: OK in 0.046 seconds
[0m04:52:53.557028 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m04:52:53.558033 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m04:52:53.559403 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m04:52:53.562462 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m04:52:53.569853 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18d463670>]}
[0m04:52:53.571377 [info ] [Thread-6 (]: 6 of 13 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 9.74s]
[0m04:52:53.572949 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m04:52:54.570705 [debug] [Thread-10 ]: SQL status: OK in 10.437 seconds
[0m04:52:54.581235 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m04:52:54.582511 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m04:52:54.595639 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m04:52:54.602697 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: COMMIT
[0m04:52:54.603555 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m04:52:54.604171 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: COMMIT
[0m04:52:54.632859 [debug] [Thread-10 ]: SQL status: OK in 0.028 seconds
[0m04:52:54.643208 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m04:52:54.644123 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m04:52:54.645406 [debug] [Thread-10 ]: SQL status: OK in 0.000 seconds
[0m04:52:54.647838 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: Close
[0m04:52:54.648814 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18d46c370>]}
[0m04:52:54.649945 [info ] [Thread-10 ]: 10 of 13 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 10.81s]
[0m04:52:54.652881 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_increase
[0m04:52:54.722182 [debug] [Thread-9 (]: SQL status: OK in 10.562 seconds
[0m04:52:54.741061 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m04:52:54.741984 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m04:52:54.743383 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m04:52:54.746450 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: COMMIT
[0m04:52:54.747266 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m04:52:54.747906 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: COMMIT
[0m04:52:54.779558 [debug] [Thread-9 (]: SQL status: OK in 0.031 seconds
[0m04:52:54.786351 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m04:52:54.787345 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m04:52:54.788621 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m04:52:54.792908 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: Close
[0m04:52:54.794464 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd131902770>]}
[0m04:52:54.795706 [info ] [Thread-9 (]: 9 of 13 OK created sql table model main.omny_adoption_by_station ............... [[32mOK[0m in 10.95s]
[0m04:52:54.796989 [debug] [Thread-9 (]: Finished running node model.mta.omny_adoption_by_station
[0m04:52:55.224802 [debug] [Thread-12 ]: SQL status: OK in 11.050 seconds
[0m04:52:55.231001 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m04:52:55.231972 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m04:52:55.233442 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m04:52:55.236624 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: COMMIT
[0m04:52:55.237373 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m04:52:55.238195 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: COMMIT
[0m04:52:55.244275 [debug] [Thread-12 ]: SQL status: OK in 0.005 seconds
[0m04:52:55.249245 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m04:52:55.250373 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m04:52:55.251864 [debug] [Thread-12 ]: SQL status: OK in 0.000 seconds
[0m04:52:55.255679 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: Close
[0m04:52:55.257243 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18d46cdf0>]}
[0m04:52:55.260277 [info ] [Thread-12 ]: 12 of 13 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 11.34s]
[0m04:52:55.262618 [debug] [Thread-12 ]: Finished running node model.mta.total_riders_per_station
[0m04:52:57.795180 [debug] [Thread-11 ]: SQL status: OK in 13.606 seconds
[0m04:52:57.798521 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m04:52:57.798937 [debug] [Thread-11 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m04:52:57.799992 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m04:52:57.801558 [debug] [Thread-11 ]: On model.mta.subway_station_stats: COMMIT
[0m04:52:57.801917 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m04:52:57.802247 [debug] [Thread-11 ]: On model.mta.subway_station_stats: COMMIT
[0m04:52:57.807876 [debug] [Thread-11 ]: SQL status: OK in 0.005 seconds
[0m04:52:57.810263 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m04:52:57.810668 [debug] [Thread-11 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m04:52:57.811450 [debug] [Thread-11 ]: SQL status: OK in 0.000 seconds
[0m04:52:57.812914 [debug] [Thread-11 ]: On model.mta.subway_station_stats: Close
[0m04:52:57.928091 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b76fbfd5-4ea6-493c-aedd-11146832e425', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18d46cb80>]}
[0m04:52:57.929022 [info ] [Thread-11 ]: 11 of 13 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 14.02s]
[0m04:52:57.929686 [debug] [Thread-11 ]: Finished running node model.mta.subway_station_stats
[0m04:52:57.941238 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:57.941854 [debug] [MainThread]: On master: BEGIN
[0m04:52:57.942349 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m04:52:57.967797 [debug] [MainThread]: SQL status: OK in 0.025 seconds
[0m04:52:57.968817 [debug] [MainThread]: On master: COMMIT
[0m04:52:57.969175 [debug] [MainThread]: Using duckdb connection "master"
[0m04:52:57.969420 [debug] [MainThread]: On master: COMMIT
[0m04:52:57.970255 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m04:52:57.970564 [debug] [MainThread]: On master: Close
[0m04:52:57.973232 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:52:57.973522 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m04:52:57.973780 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m04:52:57.974003 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m04:52:57.974276 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m04:52:57.974495 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m04:52:57.974698 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m04:52:57.974983 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m04:52:57.975181 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m04:52:57.975379 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m04:52:57.975576 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m04:52:57.975761 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m04:52:57.975946 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m04:52:57.976132 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m04:52:57.977361 [info ] [MainThread]: 
[0m04:52:57.978062 [info ] [MainThread]: Finished running 13 table models in 0 hours 0 minutes and 14.48 seconds (14.48s).
[0m04:52:57.980673 [debug] [MainThread]: Command end result
[0m04:52:58.052593 [info ] [MainThread]: 
[0m04:52:58.054286 [info ] [MainThread]: [32mCompleted successfully[0m
[0m04:52:58.054795 [info ] [MainThread]: 
[0m04:52:58.055592 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m04:52:58.062967 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.208805, "process_user_time": 141.3524, "process_kernel_time": 30.339373, "process_mem_max_rss": "678484", "process_in_blocks": "59120", "process_out_blocks": "15456"}
[0m04:52:58.065413 [debug] [MainThread]: Command `dbt run` succeeded at 04:52:58.065215 after 15.21 seconds
[0m04:52:58.066815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd19afdf430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18e167760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd18e35ecb0>]}
[0m04:52:58.067344 [debug] [MainThread]: Flushing usage events
[0m05:04:24.178463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc44a434c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc430ee5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc430ee6e0>]}


============================== 05:04:24.181989 | f38d9ea3-389d-4346-8e92-11abcec2b402 ==============================
[0m05:04:24.181989 [info ] [MainThread]: Running with dbt=1.8.7
[0m05:04:24.182524 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m05:04:24.393875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc4303fc40>]}
[0m05:04:24.448819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc430226b0>]}
[0m05:04:24.451849 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m05:04:24.459951 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m05:04:24.546360 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m05:04:24.546866 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m05:04:24.577368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc43094970>]}
[0m05:04:24.663573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc431f1b40>]}
[0m05:04:24.664142 [info ] [MainThread]: Found 13 models, 9 sources, 416 macros
[0m05:04:24.664532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc431f3820>]}
[0m05:04:24.666170 [info ] [MainThread]: 
[0m05:04:24.666767 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m05:04:24.671176 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m05:04:24.737632 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m05:04:24.738548 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m05:04:24.738942 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:04:24.763394 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m05:04:24.764629 [debug] [ThreadPool]: On list_mtastats: Close
[0m05:04:24.767587 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m05:04:24.768298 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m05:04:24.773551 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m05:04:24.773870 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m05:04:24.774148 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:04:24.781704 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m05:04:24.782866 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m05:04:24.783166 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m05:04:24.783635 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m05:04:24.783902 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m05:04:24.784125 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m05:04:24.784832 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m05:04:24.785495 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m05:04:24.785745 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m05:04:24.785996 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m05:04:24.786439 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m05:04:24.786727 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m05:04:24.792764 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m05:04:24.798184 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m05:04:24.798871 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m05:04:24.799267 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:04:24.807445 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m05:04:24.808022 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m05:04:24.808375 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m05:04:24.827072 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m05:04:24.828951 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m05:04:24.831590 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m05:04:24.831974 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m05:04:24.838338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc43138f10>]}
[0m05:04:24.838893 [debug] [MainThread]: Using duckdb connection "master"
[0m05:04:24.839207 [debug] [MainThread]: On master: BEGIN
[0m05:04:24.839465 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:04:24.855890 [debug] [MainThread]: SQL status: OK in 0.016 seconds
[0m05:04:24.856366 [debug] [MainThread]: On master: COMMIT
[0m05:04:24.856650 [debug] [MainThread]: Using duckdb connection "master"
[0m05:04:24.856883 [debug] [MainThread]: On master: COMMIT
[0m05:04:24.857330 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m05:04:24.857626 [debug] [MainThread]: On master: Close
[0m05:04:24.859423 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m05:04:24.859833 [info ] [MainThread]: 
[0m05:04:24.868405 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m05:04:24.869102 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m05:04:24.869988 [info ] [Thread-1 (]: 1 of 13 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m05:04:24.870756 [info ] [Thread-2 (]: 2 of 13 START sql table model main.bond_payment_info ........................... [RUN]
[0m05:04:24.871925 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m05:04:24.872504 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m05:04:24.873118 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m05:04:24.873631 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m05:04:24.874207 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m05:04:24.874692 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m05:04:24.875250 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m05:04:24.876192 [debug] [Thread-7 (]: Began running node model.mta.labor_expenses_per_agency
[0m05:04:24.877045 [debug] [Thread-8 (]: Began running node model.mta.largest_expense_differences_2023
[0m05:04:24.877498 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m05:04:24.875814 [info ] [Thread-3 (]: 3 of 13 START sql table model main.busiest_specific_times ...................... [RUN]
[0m05:04:24.878079 [debug] [Thread-9 (]: Began running node model.mta.omny_adoption_by_station
[0m05:04:24.878701 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_increase
[0m05:04:24.879929 [debug] [Thread-11 ]: Began running node model.mta.subway_station_stats
[0m05:04:24.881182 [debug] [Thread-12 ]: Began running node model.mta.total_riders_per_station
[0m05:04:24.888999 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m05:04:24.890085 [debug] [Thread-13 ]: Began running node model.mta.weekly_riders_per_station
[0m05:04:24.879407 [info ] [Thread-4 (]: 4 of 13 START sql table model main.daily_ridership ............................. [RUN]
[0m05:04:24.893748 [info ] [Thread-5 (]: 5 of 13 START sql table model main.expense_type_per_year ....................... [RUN]
[0m05:04:24.895312 [info ] [Thread-6 (]: 6 of 13 START sql table model main.fare_class_boro ............................. [RUN]
[0m05:04:24.896171 [info ] [Thread-7 (]: 7 of 13 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m05:04:24.896984 [info ] [Thread-8 (]: 8 of 13 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m05:04:24.901283 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m05:04:24.902486 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m05:04:24.903664 [info ] [Thread-9 (]: 9 of 13 START sql table model main.omny_adoption_by_station .................... [RUN]
[0m05:04:24.905118 [info ] [Thread-10 ]: 10 of 13 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m05:04:24.906514 [info ] [Thread-11 ]: 11 of 13 START sql table model main.subway_station_stats ....................... [RUN]
[0m05:04:24.907386 [info ] [Thread-12 ]: 12 of 13 START sql table model main.total_riders_per_station ................... [RUN]
[0m05:04:24.908467 [info ] [Thread-13 ]: 13 of 13 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m05:04:24.909501 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m05:04:24.910335 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m05:04:24.910875 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m05:04:24.911713 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m05:04:24.912371 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m05:04:24.912891 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m05:04:24.913886 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m05:04:24.914552 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m05:04:24.915324 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m05:04:24.915984 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m05:04:24.916574 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m05:04:24.917275 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m05:04:24.918086 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m05:04:24.918725 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m05:04:24.919424 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m05:04:24.932402 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m05:04:24.950152 [debug] [Thread-7 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m05:04:24.951495 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m05:04:24.952359 [debug] [Thread-8 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m05:04:25.013490 [debug] [Thread-9 (]: Began compiling node model.mta.omny_adoption_by_station
[0m05:04:25.015859 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m05:04:25.016458 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_increase
[0m05:04:25.016963 [debug] [Thread-11 ]: Began compiling node model.mta.subway_station_stats
[0m05:04:25.019794 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m05:04:25.020321 [debug] [Thread-12 ]: Began compiling node model.mta.total_riders_per_station
[0m05:04:25.020815 [debug] [Thread-13 ]: Began compiling node model.mta.weekly_riders_per_station
[0m05:04:25.024780 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m05:04:25.028277 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m05:04:25.033229 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m05:04:25.035788 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m05:04:25.038917 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m05:04:25.041961 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m05:04:25.042519 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:04:25.046859 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m05:04:25.047377 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m05:04:25.050119 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m05:04:25.052788 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m05:04:25.055394 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m05:04:25.055969 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:04:25.058140 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m05:04:25.058673 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m05:04:25.059483 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m05:04:25.060001 [debug] [Thread-7 (]: Began executing node model.mta.labor_expenses_per_agency
[0m05:04:25.060793 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m05:04:25.062282 [debug] [Thread-8 (]: Began executing node model.mta.largest_expense_differences_2023
[0m05:04:25.063097 [debug] [Thread-9 (]: Began executing node model.mta.omny_adoption_by_station
[0m05:04:25.066738 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m05:04:25.067283 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_increase
[0m05:04:25.068219 [debug] [Thread-11 ]: Began executing node model.mta.subway_station_stats
[0m05:04:25.069462 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m05:04:25.070035 [debug] [Thread-12 ]: Began executing node model.mta.total_riders_per_station
[0m05:04:25.073522 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m05:04:25.074074 [debug] [Thread-13 ]: Began executing node model.mta.weekly_riders_per_station
[0m05:04:25.077756 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m05:04:25.083724 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m05:04:25.087570 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m05:04:25.088287 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:04:25.091555 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m05:04:25.095790 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m05:04:25.096937 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:04:25.100296 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m05:04:25.103664 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m05:04:25.104275 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m05:04:25.108319 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m05:04:25.111976 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m05:04:25.112811 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:04:25.113496 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:04:25.113981 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:04:25.123810 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:04:25.124552 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:04:25.125403 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m05:04:25.126094 [debug] [Thread-1 (]: SQL status: OK in 0.038 seconds
[0m05:04:25.126738 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:04:25.127771 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:04:25.128910 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:04:25.130237 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:04:25.130857 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m05:04:25.131393 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m05:04:25.132159 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m05:04:25.132694 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:04:25.133413 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m05:04:25.134013 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: BEGIN
[0m05:04:25.134452 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m05:04:25.135117 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:04:25.135778 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m05:04:25.136540 [debug] [Thread-11 ]: On model.mta.subway_station_stats: BEGIN
[0m05:04:25.137320 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: BEGIN
[0m05:04:25.138051 [debug] [Thread-2 (]: SQL status: OK in 0.034 seconds
[0m05:04:25.138922 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: BEGIN
[0m05:04:25.139405 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m05:04:25.140111 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m05:04:25.140798 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m05:04:25.141471 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m05:04:25.141957 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m05:04:25.142584 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m05:04:25.143743 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m05:04:25.144323 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m05:04:25.144798 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m05:04:25.145403 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m05:04:25.146053 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:04:25.146653 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m05:04:25.148592 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m05:04:25.149064 [debug] [Thread-3 (]: SQL status: OK in 0.015 seconds
[0m05:04:25.150807 [debug] [Thread-4 (]: SQL status: OK in 0.011 seconds
[0m05:04:25.155076 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m05:04:25.155655 [debug] [Thread-6 (]: SQL status: OK in 0.015 seconds
[0m05:04:25.156798 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:04:25.157317 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:04:25.161558 [debug] [Thread-5 (]: SQL status: OK in 0.021 seconds
[0m05:04:25.162372 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:04:25.163368 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m05:04:25.164217 [debug] [Thread-7 (]: SQL status: OK in 0.022 seconds
[0m05:04:25.164870 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m05:04:25.165497 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:04:25.166137 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m05:04:25.168233 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:04:25.169282 [debug] [Thread-9 (]: SQL status: OK in 0.027 seconds
[0m05:04:25.174134 [debug] [Thread-8 (]: SQL status: OK in 0.030 seconds
[0m05:04:25.174806 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m05:04:25.176806 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency,
    fiscal_year,
    expense_type,
    general_ledger
ORDER BY 
    agency, fiscal_year, expense_type, general_ledger
    );
  
  
[0m05:04:25.177438 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:04:25.178398 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:04:25.179965 [debug] [Thread-11 ]: SQL status: OK in 0.035 seconds
[0m05:04:25.181740 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m05:04:25.183058 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m05:04:25.184198 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:04:25.190535 [debug] [Thread-11 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m05:04:25.191903 [debug] [Thread-10 ]: SQL status: OK in 0.046 seconds
[0m05:04:25.193642 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:04:25.194520 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m05:04:25.197503 [debug] [Thread-12 ]: SQL status: OK in 0.050 seconds
[0m05:04:25.198539 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:04:25.198999 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m05:04:25.214417 [debug] [Thread-13 ]: SQL status: OK in 0.066 seconds
[0m05:04:25.215105 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:04:25.215573 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      with weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
)
SELECT 
    station_complex, 
    week_start, 
    total_weekly_ridership
FROM 
    weekly_ridership
ORDER BY 
    station_complex, 
    week_start
    );
  
  
[0m05:04:25.537362 [debug] [Thread-4 (]: SQL status: OK in 0.366 seconds
[0m05:04:25.555200 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:04:25.556172 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m05:04:25.559568 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m05:04:25.586543 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m05:04:25.587670 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:04:25.588143 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m05:04:25.593256 [debug] [Thread-7 (]: SQL status: OK in 0.412 seconds
[0m05:04:25.597361 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:04:25.598620 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m05:04:25.600945 [debug] [Thread-7 (]: SQL status: OK in 0.002 seconds
[0m05:04:25.603178 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m05:04:25.604237 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:04:25.604750 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m05:04:25.605479 [debug] [Thread-4 (]: SQL status: OK in 0.017 seconds
[0m05:04:25.623540 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:04:25.624633 [debug] [Thread-7 (]: SQL status: OK in 0.019 seconds
[0m05:04:25.625555 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m05:04:25.633444 [debug] [Thread-7 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:04:25.634760 [debug] [Thread-5 (]: SQL status: OK in 0.455 seconds
[0m05:04:25.635794 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m05:04:25.642439 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:04:25.645253 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m05:04:25.643975 [debug] [Thread-4 (]: SQL status: OK in 0.009 seconds
[0m05:04:25.646214 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m05:04:25.650230 [debug] [Thread-7 (]: On model.mta.labor_expenses_per_agency: Close
[0m05:04:25.652982 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m05:04:25.658104 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc44acf5e0>]}
[0m05:04:25.659588 [debug] [Thread-5 (]: SQL status: OK in 0.005 seconds
[0m05:04:25.661031 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc37c52f50>]}
[0m05:04:25.662244 [info ] [Thread-7 (]: 7 of 13 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 0.74s]
[0m05:04:25.666231 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m05:04:25.667523 [info ] [Thread-4 (]: 4 of 13 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 0.75s]
[0m05:04:25.669917 [debug] [Thread-7 (]: Finished running node model.mta.labor_expenses_per_agency
[0m05:04:25.670939 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:04:25.672119 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m05:04:25.673945 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m05:04:25.680972 [debug] [Thread-5 (]: SQL status: OK in 0.005 seconds
[0m05:04:25.685671 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:04:25.686783 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m05:04:25.694126 [debug] [Thread-5 (]: SQL status: OK in 0.005 seconds
[0m05:04:25.697347 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m05:04:25.698617 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc37c51c00>]}
[0m05:04:25.700346 [info ] [Thread-5 (]: 5 of 13 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 0.79s]
[0m05:04:25.702652 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m05:04:25.704153 [debug] [Thread-8 (]: SQL status: OK in 0.515 seconds
[0m05:04:25.709592 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:04:25.712783 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m05:04:25.715435 [debug] [Thread-8 (]: SQL status: OK in 0.001 seconds
[0m05:04:25.718594 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m05:04:25.719449 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:04:25.721940 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m05:04:25.731278 [debug] [Thread-8 (]: SQL status: OK in 0.007 seconds
[0m05:04:25.736739 [debug] [Thread-8 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:04:25.737885 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m05:04:25.739181 [debug] [Thread-8 (]: SQL status: OK in 0.000 seconds
[0m05:04:25.741576 [debug] [Thread-8 (]: On model.mta.largest_expense_differences_2023: Close
[0m05:04:25.743514 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc370b6200>]}
[0m05:04:25.745183 [info ] [Thread-8 (]: 8 of 13 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 0.83s]
[0m05:04:25.748382 [debug] [Thread-8 (]: Finished running node model.mta.largest_expense_differences_2023
[0m05:04:26.137994 [debug] [Thread-2 (]: SQL status: OK in 0.979 seconds
[0m05:04:26.144137 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:04:26.146724 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m05:04:26.169729 [debug] [Thread-2 (]: SQL status: OK in 0.022 seconds
[0m05:04:26.212140 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m05:04:26.213210 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:04:26.220679 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m05:04:26.246454 [debug] [Thread-2 (]: SQL status: OK in 0.025 seconds
[0m05:04:26.251969 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:04:26.254357 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m05:04:26.256685 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m05:04:26.260092 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m05:04:26.280911 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc37c505b0>]}
[0m05:04:26.283527 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 1.41s]
[0m05:04:26.286659 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m05:04:27.933781 [debug] [Thread-1 (]: SQL status: OK in 2.782 seconds
[0m05:04:27.939995 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:04:27.940995 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m05:04:27.942745 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m05:04:27.946686 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m05:04:27.953954 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:04:27.956794 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m05:04:27.990387 [debug] [Thread-1 (]: SQL status: OK in 0.033 seconds
[0m05:04:27.995176 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:04:27.996433 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m05:04:27.997568 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m05:04:28.001605 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m05:04:28.022230 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc37c517b0>]}
[0m05:04:28.024039 [info ] [Thread-1 (]: 1 of 13 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 3.15s]
[0m05:04:28.025089 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m05:04:29.626936 [debug] [Thread-3 (]: SQL status: OK in 4.459 seconds
[0m05:04:29.633951 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:04:29.635500 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m05:04:29.637886 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m05:04:29.641121 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m05:04:29.642069 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:04:29.642718 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m05:04:29.651466 [debug] [Thread-3 (]: SQL status: OK in 0.008 seconds
[0m05:04:29.656966 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:04:29.657778 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m05:04:29.658943 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m05:04:29.661368 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m05:04:29.662562 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc36f84850>]}
[0m05:04:29.664439 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 4.76s]
[0m05:04:29.666823 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m05:04:34.219368 [debug] [Thread-13 ]: SQL status: OK in 9.002 seconds
[0m05:04:34.227636 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:04:34.270512 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m05:04:34.272266 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m05:04:34.275562 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m05:04:34.276535 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:04:34.277404 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m05:04:34.316061 [debug] [Thread-13 ]: SQL status: OK in 0.022 seconds
[0m05:04:34.326110 [debug] [Thread-13 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:04:34.327525 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m05:04:34.329124 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m05:04:34.332335 [debug] [Thread-13 ]: On model.mta.weekly_riders_per_station: Close
[0m05:04:34.333575 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc36f74e50>]}
[0m05:04:34.335563 [info ] [Thread-13 ]: 13 of 13 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 9.42s]
[0m05:04:34.337448 [debug] [Thread-13 ]: Finished running node model.mta.weekly_riders_per_station
[0m05:04:34.564470 [debug] [Thread-10 ]: SQL status: OK in 9.369 seconds
[0m05:04:34.570172 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:04:34.571080 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m05:04:34.573239 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m05:04:34.577925 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: COMMIT
[0m05:04:34.578798 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:04:34.579484 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: COMMIT
[0m05:04:34.586095 [debug] [Thread-10 ]: SQL status: OK in 0.006 seconds
[0m05:04:34.590624 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:04:34.591427 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m05:04:34.592603 [debug] [Thread-10 ]: SQL status: OK in 0.000 seconds
[0m05:04:34.597895 [debug] [Thread-10 ]: On model.mta.omny_adoption_increase: Close
[0m05:04:34.610892 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc370b6740>]}
[0m05:04:34.612364 [info ] [Thread-10 ]: 10 of 13 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 9.70s]
[0m05:04:34.613606 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_increase
[0m05:04:34.951975 [debug] [Thread-11 ]: SQL status: OK in 9.759 seconds
[0m05:04:34.957782 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:04:34.958618 [debug] [Thread-11 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m05:04:34.959941 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m05:04:34.962895 [debug] [Thread-11 ]: On model.mta.subway_station_stats: COMMIT
[0m05:04:34.963681 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:04:34.964373 [debug] [Thread-11 ]: On model.mta.subway_station_stats: COMMIT
[0m05:04:34.990183 [debug] [Thread-11 ]: SQL status: OK in 0.025 seconds
[0m05:04:34.995998 [debug] [Thread-11 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:04:34.998463 [debug] [Thread-11 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m05:04:35.003174 [debug] [Thread-11 ]: SQL status: OK in 0.002 seconds
[0m05:04:35.006320 [debug] [Thread-11 ]: On model.mta.subway_station_stats: Close
[0m05:04:35.030620 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc370b5990>]}
[0m05:04:35.032089 [info ] [Thread-11 ]: 11 of 13 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 10.11s]
[0m05:04:35.033261 [debug] [Thread-11 ]: Finished running node model.mta.subway_station_stats
[0m05:04:35.466726 [debug] [Thread-12 ]: SQL status: OK in 10.266 seconds
[0m05:04:35.474841 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:04:35.475812 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m05:04:35.477173 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m05:04:35.480683 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: COMMIT
[0m05:04:35.481602 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:04:35.482288 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: COMMIT
[0m05:04:35.504901 [debug] [Thread-12 ]: SQL status: OK in 0.022 seconds
[0m05:04:35.509349 [debug] [Thread-12 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:04:35.510133 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m05:04:35.511176 [debug] [Thread-12 ]: SQL status: OK in 0.000 seconds
[0m05:04:35.513309 [debug] [Thread-12 ]: On model.mta.total_riders_per_station: Close
[0m05:04:35.514310 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdb8aaa9d50>]}
[0m05:04:35.515527 [info ] [Thread-12 ]: 12 of 13 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 10.60s]
[0m05:04:35.516529 [debug] [Thread-12 ]: Finished running node model.mta.total_riders_per_station
[0m05:04:35.828772 [debug] [Thread-9 (]: SQL status: OK in 10.643 seconds
[0m05:04:35.833928 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:04:35.834790 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m05:04:35.836102 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m05:04:35.838867 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: COMMIT
[0m05:04:35.839666 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:04:35.840453 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: COMMIT
[0m05:04:35.864998 [debug] [Thread-9 (]: SQL status: OK in 0.024 seconds
[0m05:04:35.869838 [debug] [Thread-9 (]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:04:35.870535 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m05:04:35.871566 [debug] [Thread-9 (]: SQL status: OK in 0.000 seconds
[0m05:04:35.873797 [debug] [Thread-9 (]: On model.mta.omny_adoption_by_station: Close
[0m05:04:35.874822 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc370b66b0>]}
[0m05:04:35.875972 [info ] [Thread-9 (]: 9 of 13 OK created sql table model main.omny_adoption_by_station ............... [[32mOK[0m in 10.96s]
[0m05:04:35.876964 [debug] [Thread-9 (]: Finished running node model.mta.omny_adoption_by_station
[0m05:04:36.218819 [debug] [Thread-6 (]: SQL status: OK in 11.043 seconds
[0m05:04:36.222930 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:04:36.223616 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m05:04:36.225194 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m05:04:36.227539 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m05:04:36.228113 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:04:36.228587 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m05:04:36.232127 [debug] [Thread-6 (]: SQL status: OK in 0.003 seconds
[0m05:04:36.234269 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:04:36.234639 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m05:04:36.235240 [debug] [Thread-6 (]: SQL status: OK in 0.000 seconds
[0m05:04:36.236515 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m05:04:36.313418 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f38d9ea3-389d-4346-8e92-11abcec2b402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc370b5d80>]}
[0m05:04:36.314224 [info ] [Thread-6 (]: 6 of 13 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 11.40s]
[0m05:04:36.314841 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m05:04:36.318575 [debug] [MainThread]: Using duckdb connection "master"
[0m05:04:36.318960 [debug] [MainThread]: On master: BEGIN
[0m05:04:36.319282 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:04:36.325326 [debug] [MainThread]: SQL status: OK in 0.006 seconds
[0m05:04:36.325773 [debug] [MainThread]: On master: COMMIT
[0m05:04:36.326075 [debug] [MainThread]: Using duckdb connection "master"
[0m05:04:36.326342 [debug] [MainThread]: On master: COMMIT
[0m05:04:36.327000 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m05:04:36.327310 [debug] [MainThread]: On master: Close
[0m05:04:36.329344 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:04:36.329631 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m05:04:36.329960 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m05:04:36.330243 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m05:04:36.330501 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m05:04:36.330752 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m05:04:36.330987 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m05:04:36.331204 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m05:04:36.331404 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m05:04:36.331592 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m05:04:36.331782 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m05:04:36.331968 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m05:04:36.332157 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m05:04:36.332344 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m05:04:36.332748 [info ] [MainThread]: 
[0m05:04:36.333100 [info ] [MainThread]: Finished running 13 table models in 0 hours 0 minutes and 11.67 seconds (11.67s).
[0m05:04:36.334696 [debug] [MainThread]: Command end result
[0m05:04:36.360755 [info ] [MainThread]: 
[0m05:04:36.361448 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:04:36.361876 [info ] [MainThread]: 
[0m05:04:36.362264 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 TOTAL=13
[0m05:04:36.363253 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.235546, "process_user_time": 145.34872, "process_kernel_time": 8.473576, "process_mem_max_rss": "818404", "process_in_blocks": "119424", "process_out_blocks": "15464"}
[0m05:04:36.363842 [debug] [MainThread]: Command `dbt run` succeeded at 05:04:36.363734 after 12.24 seconds
[0m05:04:36.364234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc44a434c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc3451fb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc430228c0>]}
[0m05:04:36.364634 [debug] [MainThread]: Flushing usage events
[0m05:24:14.539658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff0e304f370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff0e2bd2bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff0e16fab00>]}


============================== 05:24:14.547133 | dc7bda9b-b77f-4f7c-a03e-d5ea0120e8ec ==============================
[0m05:24:14.547133 [info ] [MainThread]: Running with dbt=1.8.7
[0m05:24:14.547632 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:24:14.552563 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'LAKE_PATH'
[0m05:24:14.553762 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.06644556, "process_user_time": 0.873807, "process_kernel_time": 0.524284, "process_mem_max_rss": "91268", "process_in_blocks": "8", "process_out_blocks": "16", "command_success": false}
[0m05:24:14.554254 [debug] [MainThread]: Command `dbt run` failed at 05:24:14.554143 after 0.07 seconds
[0m05:24:14.554604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff0e304f370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff0e1792dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff0e17934f0>]}
[0m05:24:14.554966 [debug] [MainThread]: Flushing usage events
[0m05:25:04.967902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc3c323400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc3bd003a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc3a9d5ea0>]}


============================== 05:25:04.970266 | 35beca91-1b5b-4899-aa82-f6b59aabe584 ==============================
[0m05:25:04.970266 [info ] [MainThread]: Running with dbt=1.8.7
[0m05:25:04.970780 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m05:25:05.198785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc3a97a0e0>]}
[0m05:25:05.258995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc3b62e800>]}
[0m05:25:05.264108 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m05:25:05.273051 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m05:25:05.413705 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m05:25:05.414326 [debug] [MainThread]: Partial parsing: added file: mta://models/forecast_accuracy_2023.sql
[0m05:25:05.668572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f4d4100>]}
[0m05:25:05.742423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f5016f0>]}
[0m05:25:05.743194 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m05:25:05.743898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f501b10>]}
[0m05:25:05.745871 [info ] [MainThread]: 
[0m05:25:05.746516 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m05:25:05.752207 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m05:25:05.873150 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m05:25:05.873790 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m05:25:05.874232 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:25:05.894658 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m05:25:05.896389 [debug] [ThreadPool]: On list_mtastats: Close
[0m05:25:05.898703 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m05:25:05.899360 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m05:25:05.903871 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m05:25:05.904177 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m05:25:05.904430 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:25:05.910454 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m05:25:05.911656 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m05:25:05.911932 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m05:25:05.912417 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m05:25:05.912653 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m05:25:05.912889 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m05:25:05.913375 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m05:25:05.913947 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m05:25:05.914178 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m05:25:05.914412 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m05:25:05.914811 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m05:25:05.915058 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m05:25:05.917818 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m05:25:05.922365 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m05:25:05.922660 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m05:25:05.922958 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m05:25:05.929438 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m05:25:05.929979 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m05:25:05.930332 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m05:25:05.950701 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m05:25:05.952099 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m05:25:05.952772 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m05:25:05.953027 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m05:25:05.956369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc3c60ecb0>]}
[0m05:25:05.956744 [debug] [MainThread]: Using duckdb connection "master"
[0m05:25:05.956995 [debug] [MainThread]: On master: BEGIN
[0m05:25:05.957216 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:25:05.964713 [debug] [MainThread]: SQL status: OK in 0.007 seconds
[0m05:25:05.965130 [debug] [MainThread]: On master: COMMIT
[0m05:25:05.965446 [debug] [MainThread]: Using duckdb connection "master"
[0m05:25:05.965671 [debug] [MainThread]: On master: COMMIT
[0m05:25:05.966048 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m05:25:05.966296 [debug] [MainThread]: On master: Close
[0m05:25:05.968011 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m05:25:05.968361 [info ] [MainThread]: 
[0m05:25:05.977721 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m05:25:05.978371 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m05:25:05.978874 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m05:25:05.979502 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m05:25:05.980205 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m05:25:05.980723 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m05:25:05.981442 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m05:25:05.982002 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m05:25:05.982573 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m05:25:05.983284 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m05:25:05.983775 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m05:25:05.984328 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m05:25:05.984974 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m05:25:05.986010 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m05:25:05.987302 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m05:25:05.988914 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m05:25:05.991196 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m05:25:05.992074 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m05:25:05.992710 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m05:25:05.993577 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m05:25:05.994287 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m05:25:05.994830 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m05:25:05.995470 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m05:25:05.995939 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m05:25:05.996580 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m05:25:05.997060 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m05:25:05.997867 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m05:25:05.998460 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m05:25:05.999054 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m05:25:05.999683 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m05:25:06.000453 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m05:25:06.001253 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m05:25:06.002270 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m05:25:06.004529 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m05:25:06.014976 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m05:25:06.015672 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m05:25:06.016448 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m05:25:06.017009 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m05:25:06.017592 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m05:25:06.018297 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m05:25:06.018898 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m05:25:06.026211 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m05:25:06.029634 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m05:25:06.030838 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m05:25:06.031637 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m05:25:06.032192 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m05:25:06.032687 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m05:25:06.033285 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m05:25:06.034117 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m05:25:06.034678 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m05:25:06.035346 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m05:25:06.035878 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m05:25:06.036490 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m05:25:06.037045 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m05:25:06.048117 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m05:25:06.049288 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m05:25:06.050097 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m05:25:06.050636 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m05:25:06.051132 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m05:25:06.051584 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m05:25:06.055610 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m05:25:06.063287 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m05:25:06.063952 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m05:25:06.066725 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m05:25:06.067250 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m05:25:06.067763 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m05:25:06.070029 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m05:25:06.091558 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m05:25:06.107536 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m05:25:06.112826 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m05:25:06.113552 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m05:25:06.117368 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m05:25:06.120681 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m05:25:06.123494 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m05:25:06.127799 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m05:25:06.129044 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m05:25:06.129791 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m05:25:06.132207 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m05:25:06.132728 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m05:25:06.134707 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m05:25:06.135567 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m05:25:06.138704 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m05:25:06.139627 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m05:25:06.141947 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:25:06.142565 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:25:06.143572 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m05:25:06.150673 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:25:06.151234 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m05:25:06.152143 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m05:25:06.154518 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m05:25:06.155030 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m05:25:06.158110 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m05:25:06.161613 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m05:25:06.162474 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m05:25:06.166159 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m05:25:06.166799 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m05:25:06.167237 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m05:25:06.167809 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m05:25:06.170510 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m05:25:06.171091 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:25:06.171625 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m05:25:06.175029 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m05:25:06.176011 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:25:06.178490 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m05:25:06.179440 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:25:06.183459 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m05:25:06.185551 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m05:25:06.186019 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:25:06.186539 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:25:06.188897 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m05:25:06.189536 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:25:06.190122 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m05:25:06.190875 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m05:25:06.191428 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m05:25:06.192253 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m05:25:06.192861 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:25:06.193975 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:25:06.194627 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m05:25:06.195369 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:25:06.195802 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m05:25:06.196480 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m05:25:06.196900 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:25:06.197430 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m05:25:06.206341 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:25:06.206910 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m05:25:06.207793 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m05:25:06.209005 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m05:25:06.209927 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m05:25:06.210541 [debug] [Thread-1 (]: SQL status: OK in 0.021 seconds
[0m05:25:06.211052 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m05:25:06.211534 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m05:25:06.212011 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m05:25:06.212525 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m05:25:06.213164 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m05:25:06.214655 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m05:25:06.215666 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m05:25:06.216050 [debug] [Thread-3 (]: SQL status: OK in 0.026 seconds
[0m05:25:06.216853 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m05:25:06.217425 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m05:25:06.217934 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:25:06.218834 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m05:25:06.220434 [debug] [Thread-2 (]: SQL status: OK in 0.029 seconds
[0m05:25:06.221250 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m05:25:06.222162 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m05:25:06.222796 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:25:06.223857 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m05:25:06.225127 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:25:06.225615 [debug] [Thread-4 (]: SQL status: OK in 0.019 seconds
[0m05:25:06.226615 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m05:25:06.227730 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m05:25:06.228678 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:25:06.229253 [debug] [Thread-5 (]: SQL status: OK in 0.021 seconds
[0m05:25:06.231692 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m05:25:06.232771 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:25:06.234179 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m05:25:06.234897 [debug] [Thread-6 (]: SQL status: OK in 0.024 seconds
[0m05:25:06.235672 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:25:06.236244 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m05:25:06.238111 [debug] [Thread-7 (]: SQL status: OK in 0.026 seconds
[0m05:25:06.238843 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m05:25:06.239895 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m05:25:06.244622 [debug] [Thread-9 (]: SQL status: OK in 0.032 seconds
[0m05:25:06.245356 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:25:06.246043 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m05:25:06.251731 [debug] [Thread-8 (]: SQL status: OK in 0.037 seconds
[0m05:25:06.252570 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:25:06.253289 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency,
    fiscal_year,
    expense_type,
    general_ledger
ORDER BY 
    agency, fiscal_year, expense_type, general_ledger
    );
  
  
[0m05:25:06.254390 [debug] [Thread-10 ]: SQL status: OK in 0.038 seconds
[0m05:25:06.255386 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:25:06.256010 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m05:25:06.261680 [debug] [Thread-12 ]: SQL status: OK in 0.044 seconds
[0m05:25:06.263038 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:25:06.264461 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m05:25:06.267710 [debug] [Thread-11 ]: SQL status: OK in 0.049 seconds
[0m05:25:06.268275 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:25:06.268870 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m05:25:06.277764 [debug] [Thread-13 ]: SQL status: OK in 0.056 seconds
[0m05:25:06.278485 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:25:06.279223 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m05:25:06.283852 [debug] [Thread-14 ]: SQL status: OK in 0.062 seconds
[0m05:25:06.284449 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:25:06.284946 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      with weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
)
SELECT 
    station_complex, 
    week_start, 
    total_weekly_ridership
FROM 
    weekly_ridership
ORDER BY 
    station_complex, 
    week_start
    );
  
  
[0m05:25:06.326294 [debug] [Thread-4 (]: SQL status: OK in 0.092 seconds
[0m05:25:06.340873 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:25:06.341754 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m05:25:06.343348 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m05:25:06.367365 [debug] [Thread-5 (]: SQL status: OK in 0.133 seconds
[0m05:25:06.384720 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:25:06.396461 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m05:25:06.397989 [debug] [Thread-8 (]: SQL status: OK in 0.144 seconds
[0m05:25:06.398790 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m05:25:06.399354 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:25:06.404237 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:25:06.405672 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m05:25:06.406321 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m05:25:06.407775 [debug] [Thread-5 (]: SQL status: OK in 0.003 seconds
[0m05:25:06.409903 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m05:25:06.410508 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:25:06.411068 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m05:25:06.411801 [debug] [Thread-8 (]: SQL status: OK in 0.005 seconds
[0m05:25:06.413669 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m05:25:06.414079 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:25:06.414425 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m05:25:06.435456 [debug] [Thread-4 (]: SQL status: OK in 0.028 seconds
[0m05:25:06.448077 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m05:25:06.448871 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m05:25:06.451239 [debug] [Thread-5 (]: SQL status: OK in 0.040 seconds
[0m05:25:06.465809 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m05:25:06.466882 [debug] [Thread-8 (]: SQL status: OK in 0.052 seconds
[0m05:25:06.467554 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m05:25:06.468309 [debug] [Thread-9 (]: SQL status: OK in 0.222 seconds
[0m05:25:06.485424 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m05:25:06.487341 [debug] [Thread-4 (]: SQL status: OK in 0.037 seconds
[0m05:25:06.502943 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:25:06.503639 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m05:25:06.511819 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m05:25:06.512579 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m05:25:06.514489 [debug] [Thread-5 (]: SQL status: OK in 0.027 seconds
[0m05:25:06.517592 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f56df90>]}
[0m05:25:06.521624 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m05:25:06.522131 [debug] [Thread-8 (]: SQL status: OK in 0.007 seconds
[0m05:25:06.524037 [debug] [Thread-9 (]: SQL status: OK in 0.006 seconds
[0m05:25:06.523165 [info ] [Thread-4 (]: 4 of 14 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 0.52s]
[0m05:25:06.525199 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f56c9a0>]}
[0m05:25:06.526851 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m05:25:06.528767 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m05:25:06.529640 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m05:25:06.530621 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 0.52s]
[0m05:25:06.531912 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f56d270>]}
[0m05:25:06.532678 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:25:06.534034 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m05:25:06.534879 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 0.51s]
[0m05:25:06.535581 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m05:25:06.537130 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m05:25:06.543595 [debug] [Thread-9 (]: SQL status: OK in 0.005 seconds
[0m05:25:06.555886 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m05:25:06.557664 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m05:25:06.562935 [debug] [Thread-9 (]: SQL status: OK in 0.003 seconds
[0m05:25:06.569612 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m05:25:06.570633 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f579ff0>]}
[0m05:25:06.571564 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 0.54s]
[0m05:25:06.572356 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m05:25:06.810417 [debug] [Thread-7 (]: SQL status: OK in 0.568 seconds
[0m05:25:06.836719 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m05:25:06.837604 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m05:25:06.838912 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m05:25:06.849690 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m05:25:06.850348 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m05:25:06.850810 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m05:25:06.856929 [debug] [Thread-7 (]: SQL status: OK in 0.006 seconds
[0m05:25:06.864821 [debug] [Thread-2 (]: SQL status: OK in 0.634 seconds
[0m05:25:06.871239 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m05:25:06.883029 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:25:06.883701 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m05:25:06.884427 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m05:25:06.887264 [debug] [Thread-7 (]: SQL status: OK in 0.002 seconds
[0m05:25:06.889247 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m05:25:06.890067 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m05:25:06.890896 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f3fd780>]}
[0m05:25:06.892921 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m05:25:06.894020 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 0.87s]
[0m05:25:06.894689 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:25:06.895476 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m05:25:06.896167 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m05:25:06.902529 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m05:25:06.906763 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m05:25:06.907394 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m05:25:06.909618 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m05:25:06.911545 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m05:25:06.912513 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc3a92ace0>]}
[0m05:25:06.913595 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 0.93s]
[0m05:25:06.914992 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m05:25:10.921067 [debug] [Thread-1 (]: SQL status: OK in 4.674 seconds
[0m05:25:10.953096 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:25:10.954219 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m05:25:10.973118 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m05:25:10.976885 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m05:25:10.977643 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:25:10.978202 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m05:25:10.994148 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m05:25:10.998518 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m05:25:10.999388 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m05:25:11.000778 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m05:25:11.004419 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m05:25:11.006354 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f56c1f0>]}
[0m05:25:11.021820 [info ] [Thread-1 (]: 1 of 14 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 5.02s]
[0m05:25:11.027379 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m05:25:11.878341 [debug] [Thread-3 (]: SQL status: OK in 5.648 seconds
[0m05:25:11.884622 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:25:11.886277 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m05:25:11.904010 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m05:25:11.907850 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m05:25:11.908871 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:25:11.909685 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m05:25:11.915245 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m05:25:11.923758 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m05:25:11.925872 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m05:25:11.927530 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m05:25:11.930958 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m05:25:11.932395 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f56e530>]}
[0m05:25:11.933899 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 5.95s]
[0m05:25:11.935500 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m05:25:17.084499 [debug] [Thread-14 ]: SQL status: OK in 10.799 seconds
[0m05:25:17.112943 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:25:17.120415 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m05:25:17.122064 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m05:25:17.125232 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m05:25:17.126120 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:25:17.126763 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m05:25:17.273643 [debug] [Thread-14 ]: SQL status: OK in 0.146 seconds
[0m05:25:17.280968 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m05:25:17.281819 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m05:25:17.282920 [debug] [Thread-14 ]: SQL status: OK in 0.000 seconds
[0m05:25:17.286983 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m05:25:17.310781 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f579ba0>]}
[0m05:25:17.312497 [info ] [Thread-14 ]: 14 of 14 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 11.27s]
[0m05:25:17.313945 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m05:25:17.895249 [debug] [Thread-11 ]: SQL status: OK in 11.625 seconds
[0m05:25:17.922727 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:25:17.923535 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m05:25:17.924774 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m05:25:17.927470 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m05:25:17.928128 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:25:17.928770 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m05:25:17.935531 [debug] [Thread-11 ]: SQL status: OK in 0.006 seconds
[0m05:25:17.941889 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m05:25:17.942939 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m05:25:17.944393 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m05:25:17.947247 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: Close
[0m05:25:17.948499 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb4b755990>]}
[0m05:25:17.970418 [info ] [Thread-11 ]: 11 of 14 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 11.92s]
[0m05:25:17.971704 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_increase
[0m05:25:17.979375 [debug] [Thread-10 ]: SQL status: OK in 11.722 seconds
[0m05:25:17.987217 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:25:17.988169 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m05:25:18.001990 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m05:25:18.022389 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m05:25:18.024084 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:25:18.025713 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m05:25:18.039647 [debug] [Thread-10 ]: SQL status: OK in 0.013 seconds
[0m05:25:18.044217 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m05:25:18.060296 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m05:25:18.061761 [debug] [Thread-10 ]: SQL status: OK in 0.000 seconds
[0m05:25:18.064035 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: Close
[0m05:25:18.065029 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f57a1d0>]}
[0m05:25:18.066013 [info ] [Thread-10 ]: 10 of 14 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 12.03s]
[0m05:25:18.066929 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_by_station
[0m05:25:18.173511 [debug] [Thread-12 ]: SQL status: OK in 11.907 seconds
[0m05:25:18.179530 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:25:18.180667 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m05:25:18.182229 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m05:25:18.186444 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m05:25:18.187485 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:25:18.188421 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m05:25:18.235878 [debug] [Thread-12 ]: SQL status: OK in 0.028 seconds
[0m05:25:18.240889 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m05:25:18.241696 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m05:25:18.242925 [debug] [Thread-12 ]: SQL status: OK in 0.000 seconds
[0m05:25:18.245192 [debug] [Thread-12 ]: On model.mta.subway_station_stats: Close
[0m05:25:18.246101 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f56cb50>]}
[0m05:25:18.247085 [info ] [Thread-12 ]: 12 of 14 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 12.21s]
[0m05:25:18.247995 [debug] [Thread-12 ]: Finished running node model.mta.subway_station_stats
[0m05:25:19.114724 [debug] [Thread-13 ]: SQL status: OK in 12.835 seconds
[0m05:25:19.121144 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:25:19.122138 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m05:25:19.123435 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m05:25:19.127783 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m05:25:19.129038 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:25:19.130505 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m05:25:19.150237 [debug] [Thread-13 ]: SQL status: OK in 0.019 seconds
[0m05:25:19.154906 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m05:25:19.155747 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m05:25:19.158921 [debug] [Thread-13 ]: SQL status: OK in 0.002 seconds
[0m05:25:19.162021 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: Close
[0m05:25:19.163458 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc2f56ce20>]}
[0m05:25:19.166429 [info ] [Thread-13 ]: 13 of 14 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 13.13s]
[0m05:25:19.167843 [debug] [Thread-13 ]: Finished running node model.mta.total_riders_per_station
[0m05:25:19.240229 [debug] [Thread-6 (]: SQL status: OK in 13.003 seconds
[0m05:25:19.245286 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:25:19.245782 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m05:25:19.246816 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m05:25:19.248307 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m05:25:19.248678 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:25:19.249021 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m05:25:19.256151 [debug] [Thread-6 (]: SQL status: OK in 0.007 seconds
[0m05:25:19.258646 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m05:25:19.259070 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m05:25:19.259892 [debug] [Thread-6 (]: SQL status: OK in 0.000 seconds
[0m05:25:19.261120 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m05:25:19.383608 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35beca91-1b5b-4899-aa82-f6b59aabe584', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb4b769210>]}
[0m05:25:19.384488 [info ] [Thread-6 (]: 6 of 14 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 13.38s]
[0m05:25:19.385089 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m05:25:19.390657 [debug] [MainThread]: Using duckdb connection "master"
[0m05:25:19.391037 [debug] [MainThread]: On master: BEGIN
[0m05:25:19.391287 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:25:19.406588 [debug] [MainThread]: SQL status: OK in 0.015 seconds
[0m05:25:19.407096 [debug] [MainThread]: On master: COMMIT
[0m05:25:19.407437 [debug] [MainThread]: Using duckdb connection "master"
[0m05:25:19.407705 [debug] [MainThread]: On master: COMMIT
[0m05:25:19.408286 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m05:25:19.408589 [debug] [MainThread]: On master: Close
[0m05:25:19.410928 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:25:19.411265 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m05:25:19.411578 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m05:25:19.411817 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m05:25:19.412049 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m05:25:19.412265 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m05:25:19.412472 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m05:25:19.412687 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m05:25:19.412881 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m05:25:19.413095 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m05:25:19.413298 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m05:25:19.413489 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m05:25:19.413697 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m05:25:19.413915 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m05:25:19.414111 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m05:25:19.414680 [info ] [MainThread]: 
[0m05:25:19.415039 [info ] [MainThread]: Finished running 14 table models in 0 hours 0 minutes and 13.67 seconds (13.67s).
[0m05:25:19.417014 [debug] [MainThread]: Command end result
[0m05:25:19.453994 [info ] [MainThread]: 
[0m05:25:19.455306 [info ] [MainThread]: [32mCompleted successfully[0m
[0m05:25:19.456222 [info ] [MainThread]: 
[0m05:25:19.458321 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m05:25:19.462109 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 14.547945, "process_user_time": 144.40848, "process_kernel_time": 29.051752, "process_mem_max_rss": "798136", "process_in_blocks": "288", "process_out_blocks": "17608"}
[0m05:25:19.462824 [debug] [MainThread]: Command `dbt run` succeeded at 05:25:19.462698 after 14.55 seconds
[0m05:25:19.463260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc3c323400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb4b7230d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc3b62e800>]}
[0m05:25:19.463669 [debug] [MainThread]: Flushing usage events
[0m10:10:20.188675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fd2c7460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fb979e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fb978c70>]}


============================== 10:10:20.198487 | fc6e0bef-e5fc-412f-921a-a8a8a5832331 ==============================
[0m10:10:20.198487 [info ] [MainThread]: Running with dbt=1.8.7
[0m10:10:20.199481 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'version_check': 'True', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:10:20.544610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fd38ce20>]}
[0m10:10:20.625393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fb8b2c20>]}
[0m10:10:20.630239 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m10:10:20.649210 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m10:10:20.819866 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:10:20.820418 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:10:20.864985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62f046c130>]}
[0m10:10:21.004829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62f042f7f0>]}
[0m10:10:21.005752 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m10:10:21.006298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fd6a3be0>]}
[0m10:10:21.008720 [info ] [MainThread]: 
[0m10:10:21.009590 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m10:10:21.017098 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m10:10:21.133662 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m10:10:21.134536 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m10:10:21.135219 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:10:21.158847 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m10:10:21.160504 [debug] [ThreadPool]: On list_mtastats: Close
[0m10:10:21.164216 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m10:10:21.165645 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m10:10:21.172964 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m10:10:21.173442 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m10:10:21.173832 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:10:21.183051 [debug] [ThreadPool]: SQL status: OK in 0.009 seconds
[0m10:10:21.184593 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m10:10:21.185006 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m10:10:21.185704 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m10:10:21.186103 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m10:10:21.186628 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m10:10:21.187867 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m10:10:21.188813 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m10:10:21.189226 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m10:10:21.189598 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m10:10:21.191530 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m10:10:21.192294 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m10:10:21.197172 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m10:10:21.204943 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m10:10:21.205443 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m10:10:21.205829 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:10:21.213512 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m10:10:21.213988 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m10:10:21.214449 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m10:10:21.245949 [debug] [ThreadPool]: SQL status: OK in 0.031 seconds
[0m10:10:21.247997 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m10:10:21.250612 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m10:10:21.251111 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m10:10:21.255863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fb9bcb20>]}
[0m10:10:21.256620 [debug] [MainThread]: Using duckdb connection "master"
[0m10:10:21.257064 [debug] [MainThread]: On master: BEGIN
[0m10:10:21.257436 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:10:21.266446 [debug] [MainThread]: SQL status: OK in 0.009 seconds
[0m10:10:21.266909 [debug] [MainThread]: On master: COMMIT
[0m10:10:21.267295 [debug] [MainThread]: Using duckdb connection "master"
[0m10:10:21.267669 [debug] [MainThread]: On master: COMMIT
[0m10:10:21.268771 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m10:10:21.269575 [debug] [MainThread]: On master: Close
[0m10:10:21.272706 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m10:10:21.273437 [info ] [MainThread]: 
[0m10:10:21.285365 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m10:10:21.286369 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m10:10:21.287436 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m10:10:21.288999 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m10:10:21.289950 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m10:10:21.292258 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m10:10:21.293203 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m10:10:21.291469 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m10:10:21.294008 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m10:10:21.294993 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m10:10:21.296047 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m10:10:21.297299 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m10:10:21.296795 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m10:10:21.298493 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m10:10:21.299547 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m10:10:21.300306 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m10:10:21.301055 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m10:10:21.301762 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m10:10:21.302487 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m10:10:21.303699 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m10:10:21.303189 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m10:10:21.305116 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m10:10:21.315958 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m10:10:21.317464 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m10:10:21.318490 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m10:10:21.319386 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m10:10:21.320461 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m10:10:21.321533 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m10:10:21.322429 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m10:10:21.323190 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m10:10:21.324415 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m10:10:21.325596 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m10:10:21.327153 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m10:10:21.329315 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m10:10:21.330348 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m10:10:21.327915 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m10:10:21.332021 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m10:10:21.332832 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m10:10:21.333695 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m10:10:21.334480 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m10:10:21.335070 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m10:10:21.335938 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m10:10:21.336780 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m10:10:21.337530 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m10:10:21.338274 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m10:10:21.339099 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m10:10:21.339945 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m10:10:21.340725 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m10:10:21.345497 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m10:10:21.346621 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m10:10:21.347335 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m10:10:21.348031 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m10:10:21.354267 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m10:10:21.366003 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m10:10:21.432298 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m10:10:21.442760 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m10:10:21.443551 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m10:10:21.444296 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m10:10:21.449025 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m10:10:21.449779 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m10:10:21.450379 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m10:10:21.453637 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m10:10:21.454573 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m10:10:21.457870 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m10:10:21.461792 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m10:10:21.464843 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m10:10:21.465473 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m10:10:21.469296 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m10:10:21.473459 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m10:10:21.480500 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m10:10:21.481282 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m10:10:21.479633 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m10:10:21.486835 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m10:10:21.491533 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m10:10:21.497352 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m10:10:21.498189 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m10:10:21.500503 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m10:10:21.506951 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m10:10:21.506501 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m10:10:21.508896 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m10:10:21.510739 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m10:10:21.517920 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m10:10:21.518804 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m10:10:21.519624 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m10:10:21.520425 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m10:10:21.522333 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m10:10:21.523561 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m10:10:21.524893 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m10:10:21.530324 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m10:10:21.536919 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m10:10:21.542213 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m10:10:21.543760 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m10:10:21.549246 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m10:10:21.554354 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m10:10:21.555164 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m10:10:21.556320 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:10:21.557216 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m10:10:21.562016 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m10:10:21.566656 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m10:10:21.572165 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m10:10:21.576972 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m10:10:21.581438 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m10:10:21.586314 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m10:10:21.587734 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m10:10:21.588740 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m10:10:21.590283 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m10:10:21.591593 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m10:10:21.592738 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m10:10:21.603794 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m10:10:21.605118 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m10:10:21.607359 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m10:10:21.611326 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m10:10:21.612192 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m10:10:21.613671 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m10:10:21.614860 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m10:10:21.615736 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m10:10:21.617346 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m10:10:21.618291 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m10:10:21.619235 [debug] [Thread-1 (]: SQL status: OK in 0.063 seconds
[0m10:10:21.620253 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:10:21.621231 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m10:10:21.622192 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m10:10:21.623139 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m10:10:21.624085 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m10:10:21.625068 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m10:10:21.626034 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m10:10:21.627003 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m10:10:21.628052 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m10:10:21.629015 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:10:21.629961 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m10:10:21.630937 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m10:10:21.631925 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m10:10:21.632798 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m10:10:21.634342 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m10:10:21.635368 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m10:10:21.636510 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m10:10:21.638139 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m10:10:21.640629 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m10:10:21.641817 [debug] [Thread-2 (]: SQL status: OK in 0.021 seconds
[0m10:10:21.643094 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m10:10:21.644245 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m10:10:21.645989 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m10:10:21.647889 [debug] [Thread-3 (]: SQL status: OK in 0.024 seconds
[0m10:10:21.648998 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m10:10:21.650721 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m10:10:21.653703 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m10:10:21.655468 [debug] [Thread-4 (]: SQL status: OK in 0.026 seconds
[0m10:10:21.658775 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m10:10:21.661935 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m10:10:21.663099 [debug] [Thread-6 (]: SQL status: OK in 0.031 seconds
[0m10:10:21.663614 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m10:10:21.664369 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m10:10:21.668212 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m10:10:21.669220 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m10:10:21.670604 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m10:10:21.673500 [debug] [Thread-7 (]: SQL status: OK in 0.038 seconds
[0m10:10:21.674052 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m10:10:21.674779 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m10:10:21.676172 [debug] [Thread-5 (]: SQL status: OK in 0.042 seconds
[0m10:10:21.676876 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m10:10:21.677382 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m10:10:21.681434 [debug] [Thread-11 ]: SQL status: OK in 0.041 seconds
[0m10:10:21.682109 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m10:10:21.682884 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m10:10:21.683628 [debug] [Thread-8 (]: SQL status: OK in 0.047 seconds
[0m10:10:21.685439 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m10:10:21.686338 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency,
    fiscal_year,
    expense_type,
    general_ledger
ORDER BY 
    agency, fiscal_year, expense_type, general_ledger
    );
  
  
[0m10:10:21.688431 [debug] [Thread-10 ]: SQL status: OK in 0.050 seconds
[0m10:10:21.689093 [debug] [Thread-9 (]: SQL status: OK in 0.046 seconds
[0m10:10:21.690425 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m10:10:21.689579 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m10:10:21.690883 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m10:10:21.692267 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m10:10:21.699482 [debug] [Thread-12 ]: SQL status: OK in 0.055 seconds
[0m10:10:21.700020 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m10:10:21.700527 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m10:10:21.736212 [debug] [Thread-13 ]: SQL status: OK in 0.090 seconds
[0m10:10:21.737684 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m10:10:21.738790 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m10:10:21.763393 [debug] [Thread-14 ]: SQL status: OK in 0.114 seconds
[0m10:10:21.764340 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m10:10:21.764890 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      with weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
)
SELECT 
    station_complex, 
    week_start, 
    total_weekly_ridership
FROM 
    weekly_ridership
ORDER BY 
    station_complex, 
    week_start
    );
  
  
[0m10:10:21.958971 [debug] [Thread-4 (]: SQL status: OK in 0.287 seconds
[0m10:10:21.998706 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m10:10:22.000004 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m10:10:22.006564 [debug] [Thread-4 (]: SQL status: OK in 0.005 seconds
[0m10:10:22.080327 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m10:10:22.082019 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m10:10:22.082805 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m10:10:22.113325 [debug] [Thread-4 (]: SQL status: OK in 0.029 seconds
[0m10:10:22.125693 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m10:10:22.128091 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m10:10:22.131402 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m10:10:22.135888 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m10:10:22.138277 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62f04977f0>]}
[0m10:10:22.139942 [info ] [Thread-4 (]: 4 of 14 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 0.81s]
[0m10:10:22.141122 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m10:10:22.437869 [debug] [Thread-7 (]: SQL status: OK in 0.762 seconds
[0m10:10:22.443565 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m10:10:22.445047 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m10:10:22.446948 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m10:10:22.449775 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m10:10:22.450619 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m10:10:22.451164 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m10:10:22.458056 [debug] [Thread-7 (]: SQL status: OK in 0.006 seconds
[0m10:10:22.463841 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m10:10:22.464864 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m10:10:22.466809 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m10:10:22.471170 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m10:10:22.472179 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62f0497d30>]}
[0m10:10:22.473153 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 1.14s]
[0m10:10:22.474124 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m10:10:22.509108 [debug] [Thread-5 (]: SQL status: OK in 0.831 seconds
[0m10:10:22.534540 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m10:10:22.535745 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m10:10:22.537941 [debug] [Thread-5 (]: SQL status: OK in 0.001 seconds
[0m10:10:22.544616 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m10:10:22.545457 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m10:10:22.547388 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m10:10:22.565489 [debug] [Thread-5 (]: SQL status: OK in 0.017 seconds
[0m10:10:22.582793 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m10:10:22.586970 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m10:10:22.595533 [debug] [Thread-8 (]: SQL status: OK in 0.900 seconds
[0m10:10:22.597999 [debug] [Thread-5 (]: SQL status: OK in 0.002 seconds
[0m10:10:22.617921 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m10:10:22.633302 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62f0497460>]}
[0m10:10:22.638070 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m10:10:22.641879 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 1.30s]
[0m10:10:22.643282 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m10:10:22.644631 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m10:10:22.647696 [debug] [Thread-8 (]: SQL status: OK in 0.002 seconds
[0m10:10:22.654797 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m10:10:22.655869 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m10:10:22.656842 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m10:10:22.665985 [debug] [Thread-8 (]: SQL status: OK in 0.008 seconds
[0m10:10:22.704008 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m10:10:22.705098 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m10:10:22.715622 [debug] [Thread-8 (]: SQL status: OK in 0.009 seconds
[0m10:10:22.721790 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m10:10:22.725603 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62ef8c62f0>]}
[0m10:10:22.727365 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 1.39s]
[0m10:10:22.729036 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m10:10:22.777340 [debug] [Thread-9 (]: SQL status: OK in 1.081 seconds
[0m10:10:22.815084 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m10:10:22.816248 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m10:10:22.818574 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m10:10:22.828908 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m10:10:22.830010 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m10:10:22.830877 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m10:10:22.877481 [debug] [Thread-9 (]: SQL status: OK in 0.045 seconds
[0m10:10:22.891369 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m10:10:22.892483 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m10:10:22.909011 [debug] [Thread-9 (]: SQL status: OK in 0.015 seconds
[0m10:10:22.911713 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m10:10:22.912855 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62ec362170>]}
[0m10:10:22.914011 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 1.58s]
[0m10:10:22.915073 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m10:10:23.509910 [debug] [Thread-2 (]: SQL status: OK in 1.844 seconds
[0m10:10:23.516008 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m10:10:23.517441 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m10:10:23.519207 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m10:10:23.522857 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m10:10:23.524122 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m10:10:23.524941 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m10:10:23.563531 [debug] [Thread-2 (]: SQL status: OK in 0.038 seconds
[0m10:10:23.568763 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m10:10:23.570252 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m10:10:23.572251 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m10:10:23.575215 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m10:10:23.576465 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62f07ec310>]}
[0m10:10:23.577788 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 2.27s]
[0m10:10:23.579444 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m10:10:24.575837 [debug] [Thread-3 (]: SQL status: OK in 2.906 seconds
[0m10:10:24.583425 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m10:10:24.584507 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m10:10:24.588580 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m10:10:24.594607 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m10:10:24.595767 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m10:10:24.597048 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m10:10:24.608434 [debug] [Thread-3 (]: SQL status: OK in 0.010 seconds
[0m10:10:24.613514 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m10:10:24.614787 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m10:10:24.619378 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m10:10:24.622551 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m10:10:24.624075 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62f0496230>]}
[0m10:10:24.625655 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 3.30s]
[0m10:10:24.627177 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m10:10:28.808951 [debug] [Thread-6 (]: SQL status: OK in 7.136 seconds
[0m10:10:28.821240 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m10:10:28.825700 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m10:10:28.827994 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m10:10:28.837135 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m10:10:28.843016 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m10:10:28.844070 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m10:10:28.856294 [debug] [Thread-6 (]: SQL status: OK in 0.011 seconds
[0m10:10:28.863149 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m10:10:28.870310 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m10:10:28.871713 [debug] [Thread-6 (]: SQL status: OK in 0.000 seconds
[0m10:10:28.874575 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m10:10:28.875973 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62f0497430>]}
[0m10:10:28.881604 [info ] [Thread-6 (]: 6 of 14 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 7.54s]
[0m10:10:28.883322 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m10:10:30.024489 [debug] [Thread-14 ]: SQL status: OK in 8.259 seconds
[0m10:10:30.032136 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m10:10:30.033566 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m10:10:30.035209 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m10:10:30.038667 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m10:10:30.054639 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m10:10:30.055791 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m10:10:30.144820 [debug] [Thread-14 ]: SQL status: OK in 0.088 seconds
[0m10:10:30.152753 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m10:10:30.153962 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m10:10:30.155654 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m10:10:30.159844 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m10:10:30.165849 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f624e1834c0>]}
[0m10:10:30.167328 [info ] [Thread-14 ]: 14 of 14 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 8.82s]
[0m10:10:30.168755 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m10:10:30.562844 [debug] [Thread-13 ]: SQL status: OK in 8.823 seconds
[0m10:10:30.569468 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m10:10:30.573332 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m10:10:30.575078 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m10:10:30.578794 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m10:10:30.579708 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m10:10:30.580464 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m10:10:30.590926 [debug] [Thread-13 ]: SQL status: OK in 0.010 seconds
[0m10:10:30.595921 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m10:10:30.614791 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m10:10:30.616183 [debug] [Thread-13 ]: SQL status: OK in 0.000 seconds
[0m10:10:30.618734 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: Close
[0m10:10:30.619774 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62ec3a82e0>]}
[0m10:10:30.621113 [info ] [Thread-13 ]: 13 of 14 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 9.28s]
[0m10:10:30.622680 [debug] [Thread-13 ]: Finished running node model.mta.total_riders_per_station
[0m10:10:32.827605 [debug] [Thread-1 (]: SQL status: OK in 11.167 seconds
[0m10:10:32.833970 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m10:10:32.835975 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m10:10:32.845846 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m10:10:32.849546 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m10:10:32.851771 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m10:10:32.852847 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m10:10:32.881237 [debug] [Thread-1 (]: SQL status: OK in 0.027 seconds
[0m10:10:32.886533 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m10:10:32.887604 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m10:10:32.889040 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m10:10:32.892513 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m10:10:32.893789 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fb97b4f0>]}
[0m10:10:32.915247 [info ] [Thread-1 (]: 1 of 14 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 11.61s]
[0m10:10:32.916560 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m10:10:33.038182 [debug] [Thread-11 ]: SQL status: OK in 11.354 seconds
[0m10:10:33.044683 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m10:10:33.045970 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m10:10:33.048034 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m10:10:33.051603 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m10:10:33.052854 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m10:10:33.053669 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m10:10:33.065173 [debug] [Thread-11 ]: SQL status: OK in 0.011 seconds
[0m10:10:33.069997 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m10:10:33.070770 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m10:10:33.071921 [debug] [Thread-11 ]: SQL status: OK in 0.000 seconds
[0m10:10:33.074217 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: Close
[0m10:10:33.075202 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62ef8c6920>]}
[0m10:10:33.076309 [info ] [Thread-11 ]: 11 of 14 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 11.74s]
[0m10:10:33.077297 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_increase
[0m10:10:35.613029 [debug] [Thread-10 ]: SQL status: OK in 13.917 seconds
[0m10:10:35.622140 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m10:10:35.624399 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m10:10:35.627319 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m10:10:35.631550 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m10:10:35.632224 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m10:10:35.632752 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m10:10:35.637489 [debug] [Thread-10 ]: SQL status: OK in 0.004 seconds
[0m10:10:35.642498 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m10:10:35.643092 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m10:10:35.644551 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m10:10:35.646439 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: Close
[0m10:10:35.647404 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f624e1f9cf0>]}
[0m10:10:35.648386 [info ] [Thread-10 ]: 10 of 14 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 14.31s]
[0m10:10:35.649291 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_by_station
[0m10:10:35.741069 [debug] [Thread-12 ]: SQL status: OK in 14.040 seconds
[0m10:10:35.745105 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m10:10:35.745648 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m10:10:35.746671 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m10:10:35.748525 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m10:10:35.749090 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m10:10:35.749603 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m10:10:35.754670 [debug] [Thread-12 ]: SQL status: OK in 0.005 seconds
[0m10:10:35.757637 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m10:10:35.758163 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m10:10:35.759076 [debug] [Thread-12 ]: SQL status: OK in 0.000 seconds
[0m10:10:35.760715 [debug] [Thread-12 ]: On model.mta.subway_station_stats: Close
[0m10:10:35.879855 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc6e0bef-e5fc-412f-921a-a8a8a5832331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62ef8c6050>]}
[0m10:10:35.880985 [info ] [Thread-12 ]: 12 of 14 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 14.54s]
[0m10:10:35.881894 [debug] [Thread-12 ]: Finished running node model.mta.subway_station_stats
[0m10:10:35.888838 [debug] [MainThread]: Using duckdb connection "master"
[0m10:10:35.889527 [debug] [MainThread]: On master: BEGIN
[0m10:10:35.889986 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:10:35.906200 [debug] [MainThread]: SQL status: OK in 0.016 seconds
[0m10:10:35.906865 [debug] [MainThread]: On master: COMMIT
[0m10:10:35.907486 [debug] [MainThread]: Using duckdb connection "master"
[0m10:10:35.907892 [debug] [MainThread]: On master: COMMIT
[0m10:10:35.908492 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m10:10:35.908930 [debug] [MainThread]: On master: Close
[0m10:10:35.912236 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:10:35.912685 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m10:10:35.913048 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m10:10:35.913392 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m10:10:35.913711 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m10:10:35.914044 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m10:10:35.914456 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m10:10:35.914878 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m10:10:35.915457 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m10:10:35.915755 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m10:10:35.916041 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m10:10:35.916329 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m10:10:35.916611 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m10:10:35.916892 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m10:10:35.917169 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m10:10:35.917856 [info ] [MainThread]: 
[0m10:10:35.918372 [info ] [MainThread]: Finished running 14 table models in 0 hours 0 minutes and 14.91 seconds (14.91s).
[0m10:10:35.920637 [debug] [MainThread]: Command end result
[0m10:10:35.956222 [info ] [MainThread]: 
[0m10:10:35.956978 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:10:35.957452 [info ] [MainThread]: 
[0m10:10:35.957913 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m10:10:35.959633 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.864572, "process_user_time": 176.44043, "process_kernel_time": 20.621336, "process_mem_max_rss": "664184", "process_in_blocks": "1120792", "process_out_blocks": "16704"}
[0m10:10:35.960418 [debug] [MainThread]: Command `dbt run` succeeded at 10:10:35.960283 after 15.87 seconds
[0m10:10:35.960949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fd2c7460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62f06fa4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62fd38ce20>]}
[0m10:10:35.961429 [debug] [MainThread]: Flushing usage events
[0m11:04:11.179403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24cde77460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24cc6d5e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24cc6d4c70>]}


============================== 11:04:11.185503 | f5984e91-474d-47fa-89be-c525326924ee ==============================
[0m11:04:11.185503 [info ] [MainThread]: Running with dbt=1.8.7
[0m11:04:11.186603 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m11:04:11.603046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24cdf34e20>]}
[0m11:04:11.700765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c9286c20>]}
[0m11:04:11.712058 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m11:04:11.735431 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m11:04:11.933508 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:04:11.934159 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:04:11.979507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c1020130>]}
[0m11:04:12.125033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c11db7f0>]}
[0m11:04:12.125765 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m11:04:12.126286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24ce253be0>]}
[0m11:04:12.129151 [info ] [MainThread]: 
[0m11:04:12.130044 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:04:12.137602 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m11:04:12.256002 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m11:04:12.256580 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m11:04:12.256977 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:04:12.296923 [debug] [ThreadPool]: SQL status: OK in 0.040 seconds
[0m11:04:12.298689 [debug] [ThreadPool]: On list_mtastats: Close
[0m11:04:12.301705 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m11:04:12.302452 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m11:04:12.310061 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m11:04:12.310538 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m11:04:12.310913 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:04:12.321546 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m11:04:12.323447 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m11:04:12.323980 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m11:04:12.324707 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:04:12.325100 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m11:04:12.325444 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m11:04:12.326250 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:04:12.327239 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m11:04:12.327639 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m11:04:12.327989 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m11:04:12.328562 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:04:12.329001 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m11:04:12.333726 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m11:04:12.340766 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m11:04:12.341390 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m11:04:12.341792 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:04:12.352844 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m11:04:12.353365 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m11:04:12.353739 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m11:04:12.400632 [debug] [ThreadPool]: SQL status: OK in 0.046 seconds
[0m11:04:12.402646 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m11:04:12.404534 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m11:04:12.404973 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m11:04:12.409249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24cc518b20>]}
[0m11:04:12.409844 [debug] [MainThread]: Using duckdb connection "master"
[0m11:04:12.410223 [debug] [MainThread]: On master: BEGIN
[0m11:04:12.410558 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:04:12.429754 [debug] [MainThread]: SQL status: OK in 0.019 seconds
[0m11:04:12.430353 [debug] [MainThread]: On master: COMMIT
[0m11:04:12.430775 [debug] [MainThread]: Using duckdb connection "master"
[0m11:04:12.431139 [debug] [MainThread]: On master: COMMIT
[0m11:04:12.431729 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:04:12.432145 [debug] [MainThread]: On master: Close
[0m11:04:12.434499 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m11:04:12.435049 [info ] [MainThread]: 
[0m11:04:12.446447 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m11:04:12.447204 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m11:04:12.448517 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m11:04:12.449615 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m11:04:12.447956 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m11:04:12.450688 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m11:04:12.451349 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m11:04:12.451999 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m11:04:12.452926 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m11:04:12.453662 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m11:04:12.454720 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m11:04:12.455916 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m11:04:12.456759 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m11:04:12.457522 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m11:04:12.458987 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m11:04:12.460645 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m11:04:12.458477 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m11:04:12.461872 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m11:04:12.462993 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m11:04:12.463863 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m11:04:12.465236 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m11:04:12.466038 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m11:04:12.466815 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m11:04:12.467554 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m11:04:12.468526 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m11:04:12.469255 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m11:04:12.470054 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m11:04:12.470708 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m11:04:12.472566 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m11:04:12.471606 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m11:04:12.473505 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m11:04:12.474509 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m11:04:12.475176 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m11:04:12.475843 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m11:04:12.476781 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m11:04:12.477525 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m11:04:12.478387 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m11:04:12.482309 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m11:04:12.483260 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m11:04:12.484341 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m11:04:12.485163 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m11:04:12.485958 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m11:04:12.496023 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m11:04:12.499861 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m11:04:12.500950 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m11:04:12.501930 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m11:04:12.506140 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m11:04:12.506936 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m11:04:12.507610 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m11:04:12.508487 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m11:04:12.509562 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m11:04:12.510441 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m11:04:12.514105 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m11:04:12.514854 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m11:04:12.515580 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m11:04:12.516166 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m11:04:12.517026 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m11:04:12.522121 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m11:04:12.522921 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m11:04:12.523847 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m11:04:12.524502 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m11:04:12.529338 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m11:04:12.586400 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m11:04:12.587216 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m11:04:12.591016 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m11:04:12.595184 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m11:04:12.598544 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m11:04:12.601748 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m11:04:12.606298 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m11:04:12.606926 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m11:04:12.611704 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m11:04:12.616899 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m11:04:12.623499 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m11:04:12.658625 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m11:04:12.670534 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m11:04:12.680573 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m11:04:12.686430 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m11:04:12.694820 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m11:04:12.696507 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m11:04:12.697781 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m11:04:12.698922 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m11:04:12.699706 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m11:04:12.701121 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m11:04:12.714389 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m11:04:12.716964 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m11:04:12.718614 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m11:04:12.708110 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m11:04:12.720428 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m11:04:12.721844 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m11:04:12.728390 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m11:04:12.730519 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m11:04:12.731689 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m11:04:12.741179 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m11:04:12.746632 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m11:04:12.753242 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m11:04:12.760061 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m11:04:12.762155 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m11:04:12.767279 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m11:04:12.772018 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m11:04:12.773386 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m11:04:12.779673 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m11:04:12.782101 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m11:04:12.790256 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m11:04:12.790979 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m11:04:12.791829 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m11:04:12.792787 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m11:04:12.794106 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m11:04:12.794838 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m11:04:12.796371 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m11:04:12.797458 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m11:04:12.798080 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m11:04:12.798638 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m11:04:12.800194 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m11:04:12.803567 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:04:12.804874 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m11:04:12.805475 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m11:04:12.806248 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m11:04:12.806921 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:04:12.807660 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m11:04:12.808452 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m11:04:12.809272 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:04:12.810140 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m11:04:12.810996 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m11:04:12.811854 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:04:12.812743 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m11:04:12.813630 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m11:04:12.814551 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m11:04:12.892181 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m11:04:12.893055 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m11:04:12.893781 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m11:04:12.894655 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m11:04:12.895222 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m11:04:12.896001 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m11:04:12.896731 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m11:04:12.897590 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m11:04:12.898559 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m11:04:12.899702 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m11:04:12.900496 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m11:04:12.901050 [debug] [Thread-2 (]: SQL status: OK in 0.097 seconds
[0m11:04:12.901644 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m11:04:12.902213 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m11:04:12.904308 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m11:04:12.905653 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m11:04:12.913390 [debug] [Thread-1 (]: SQL status: OK in 0.106 seconds
[0m11:04:12.913939 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m11:04:12.914457 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m11:04:12.916057 [debug] [Thread-3 (]: SQL status: OK in 0.107 seconds
[0m11:04:12.916539 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m11:04:12.916953 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m11:04:12.921631 [debug] [Thread-4 (]: SQL status: OK in 0.110 seconds
[0m11:04:12.922176 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m11:04:12.922715 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m11:04:12.940870 [debug] [Thread-5 (]: SQL status: OK in 0.123 seconds
[0m11:04:12.941821 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m11:04:12.942540 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m11:04:12.945937 [debug] [Thread-6 (]: SQL status: OK in 0.051 seconds
[0m11:04:12.946488 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m11:04:12.947041 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m11:04:12.957928 [debug] [Thread-10 ]: SQL status: OK in 0.062 seconds
[0m11:04:12.958515 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m11:04:12.958996 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m11:04:12.973598 [debug] [Thread-8 (]: SQL status: OK in 0.077 seconds
[0m11:04:12.974257 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m11:04:12.974744 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency,
    fiscal_year,
    expense_type,
    general_ledger
ORDER BY 
    agency, fiscal_year, expense_type, general_ledger
    );
  
  
[0m11:04:12.981731 [debug] [Thread-11 ]: SQL status: OK in 0.084 seconds
[0m11:04:12.982345 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m11:04:12.982846 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m11:04:13.004380 [debug] [Thread-7 (]: SQL status: OK in 0.106 seconds
[0m11:04:13.005007 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m11:04:13.005654 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m11:04:13.072979 [debug] [Thread-14 ]: SQL status: OK in 0.173 seconds
[0m11:04:13.073626 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m11:04:13.074134 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      with weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
)
SELECT 
    station_complex, 
    week_start, 
    total_weekly_ridership
FROM 
    weekly_ridership
ORDER BY 
    station_complex, 
    week_start
    );
  
  
[0m11:04:13.114101 [debug] [Thread-9 (]: SQL status: OK in 0.214 seconds
[0m11:04:13.114858 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m11:04:13.115377 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m11:04:13.133722 [debug] [Thread-12 ]: SQL status: OK in 0.232 seconds
[0m11:04:13.135594 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m11:04:13.136180 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m11:04:13.171411 [debug] [Thread-13 ]: SQL status: OK in 0.269 seconds
[0m11:04:13.172386 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m11:04:13.173137 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m11:04:13.768824 [debug] [Thread-4 (]: SQL status: OK in 0.845 seconds
[0m11:04:13.894496 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m11:04:13.895219 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m11:04:13.903249 [debug] [Thread-4 (]: SQL status: OK in 0.007 seconds
[0m11:04:14.039464 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m11:04:14.040200 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m11:04:14.041925 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m11:04:14.085440 [debug] [Thread-4 (]: SQL status: OK in 0.041 seconds
[0m11:04:14.107330 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m11:04:14.108057 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m11:04:14.110095 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:04:14.114450 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m11:04:14.116462 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c104ab60>]}
[0m11:04:14.117322 [info ] [Thread-4 (]: 4 of 14 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 1.64s]
[0m11:04:14.118183 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m11:04:14.301995 [debug] [Thread-5 (]: SQL status: OK in 1.359 seconds
[0m11:04:14.306459 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m11:04:14.307184 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m11:04:14.310878 [debug] [Thread-5 (]: SQL status: OK in 0.002 seconds
[0m11:04:14.315520 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m11:04:14.316975 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m11:04:14.318842 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m11:04:14.325549 [debug] [Thread-5 (]: SQL status: OK in 0.006 seconds
[0m11:04:14.329338 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m11:04:14.343071 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m11:04:14.348609 [debug] [Thread-5 (]: SQL status: OK in 0.004 seconds
[0m11:04:14.359590 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m11:04:14.360598 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c104a770>]}
[0m11:04:14.361567 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 1.88s]
[0m11:04:14.362424 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m11:04:14.446517 [debug] [Thread-8 (]: SQL status: OK in 1.471 seconds
[0m11:04:14.524749 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m11:04:14.525759 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m11:04:14.527093 [debug] [Thread-8 (]: SQL status: OK in 0.001 seconds
[0m11:04:14.541080 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m11:04:14.541832 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m11:04:14.542306 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m11:04:14.562966 [debug] [Thread-8 (]: SQL status: OK in 0.020 seconds
[0m11:04:14.608883 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m11:04:14.610096 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m11:04:14.612301 [debug] [Thread-8 (]: SQL status: OK in 0.002 seconds
[0m11:04:14.616414 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m11:04:14.617374 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c049f040>]}
[0m11:04:14.618479 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 2.14s]
[0m11:04:14.619653 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m11:04:15.033431 [debug] [Thread-9 (]: SQL status: OK in 1.917 seconds
[0m11:04:15.138104 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m11:04:15.194878 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m11:04:15.255711 [debug] [Thread-9 (]: SQL status: OK in 0.060 seconds
[0m11:04:15.259658 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m11:04:15.260762 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m11:04:15.261551 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m11:04:15.270338 [debug] [Thread-9 (]: SQL status: OK in 0.008 seconds
[0m11:04:15.320236 [debug] [Thread-7 (]: SQL status: OK in 2.296 seconds
[0m11:04:15.327225 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m11:04:15.328292 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m11:04:15.329870 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m11:04:15.332676 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m11:04:15.335137 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c049d960>]}
[0m11:04:15.336826 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 2.85s]
[0m11:04:15.359180 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m11:04:15.386744 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m11:04:15.390727 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m11:04:15.392926 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m11:04:15.407209 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m11:04:15.408610 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m11:04:15.411657 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m11:04:15.530329 [debug] [Thread-7 (]: SQL status: OK in 0.117 seconds
[0m11:04:15.538027 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m11:04:15.546953 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m11:04:15.549048 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m11:04:15.552154 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m11:04:15.554928 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f241fad9570>]}
[0m11:04:15.565222 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 3.08s]
[0m11:04:15.566725 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m11:04:16.501768 [debug] [Thread-3 (]: SQL status: OK in 3.584 seconds
[0m11:04:16.509872 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m11:04:16.511019 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m11:04:16.513848 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m11:04:16.519291 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m11:04:16.520350 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m11:04:16.520837 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m11:04:16.528701 [debug] [Thread-3 (]: SQL status: OK in 0.007 seconds
[0m11:04:16.532725 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m11:04:16.533838 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m11:04:16.536444 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:04:16.539059 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m11:04:16.540070 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c03e6950>]}
[0m11:04:16.541067 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 4.07s]
[0m11:04:16.542558 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m11:04:16.619378 [debug] [Thread-2 (]: SQL status: OK in 3.713 seconds
[0m11:04:16.627327 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m11:04:16.629432 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m11:04:16.631470 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:04:16.634546 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m11:04:16.635157 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m11:04:16.635612 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m11:04:16.642159 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m11:04:16.646726 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m11:04:16.647924 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m11:04:16.649695 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:04:16.653499 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m11:04:16.655099 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c104bf10>]}
[0m11:04:16.656516 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 4.19s]
[0m11:04:16.657447 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m11:04:23.079088 [debug] [Thread-14 ]: SQL status: OK in 10.004 seconds
[0m11:04:23.106474 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m11:04:23.113384 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m11:04:23.115438 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m11:04:23.126164 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m11:04:23.127660 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m11:04:23.128613 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m11:04:23.144229 [debug] [Thread-13 ]: SQL status: OK in 9.970 seconds
[0m11:04:23.154767 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m11:04:23.156094 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m11:04:23.159960 [debug] [Thread-13 ]: SQL status: OK in 0.003 seconds
[0m11:04:23.163472 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m11:04:23.164865 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m11:04:23.165654 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m11:04:23.188474 [debug] [Thread-14 ]: SQL status: OK in 0.059 seconds
[0m11:04:23.200967 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m11:04:23.203659 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m11:04:23.215502 [debug] [Thread-13 ]: SQL status: OK in 0.049 seconds
[0m11:04:23.216508 [debug] [Thread-14 ]: SQL status: OK in 0.012 seconds
[0m11:04:23.225978 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m11:04:23.231074 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m11:04:23.255025 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c02c5150>]}
[0m11:04:23.257277 [info ] [Thread-14 ]: 14 of 14 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 10.75s]
[0m11:04:23.259092 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m11:04:23.260317 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m11:04:23.261929 [debug] [Thread-13 ]: SQL status: OK in 0.000 seconds
[0m11:04:23.265786 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: Close
[0m11:04:23.267222 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c027c1c0>]}
[0m11:04:23.268669 [info ] [Thread-13 ]: 13 of 14 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 10.77s]
[0m11:04:23.270117 [debug] [Thread-13 ]: Finished running node model.mta.total_riders_per_station
[0m11:04:23.750350 [debug] [Thread-11 ]: SQL status: OK in 10.766 seconds
[0m11:04:23.756481 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m11:04:23.758269 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m11:04:23.760001 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m11:04:23.779986 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m11:04:23.781175 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m11:04:23.782098 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m11:04:23.816743 [debug] [Thread-11 ]: SQL status: OK in 0.034 seconds
[0m11:04:23.825679 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m11:04:23.847227 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m11:04:23.850698 [debug] [Thread-11 ]: SQL status: OK in 0.002 seconds
[0m11:04:23.856363 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: Close
[0m11:04:23.858018 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c0255f60>]}
[0m11:04:23.860126 [info ] [Thread-11 ]: 11 of 14 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 11.37s]
[0m11:04:23.861506 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_increase
[0m11:04:28.194117 [debug] [Thread-10 ]: SQL status: OK in 15.234 seconds
[0m11:04:28.201267 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m11:04:28.202296 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m11:04:28.204026 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m11:04:28.207914 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m11:04:28.208790 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m11:04:28.209512 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m11:04:28.243304 [debug] [Thread-10 ]: SQL status: OK in 0.033 seconds
[0m11:04:28.248022 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m11:04:28.248938 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m11:04:28.250202 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m11:04:28.253031 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: Close
[0m11:04:28.254889 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c049ddb0>]}
[0m11:04:28.274430 [info ] [Thread-10 ]: 10 of 14 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 15.77s]
[0m11:04:28.277353 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_by_station
[0m11:04:28.464852 [debug] [Thread-6 (]: SQL status: OK in 15.515 seconds
[0m11:04:28.479230 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m11:04:28.480862 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m11:04:28.483855 [debug] [Thread-6 (]: SQL status: OK in 0.002 seconds
[0m11:04:28.492182 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m11:04:28.493855 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m11:04:28.495292 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m11:04:28.513246 [debug] [Thread-6 (]: SQL status: OK in 0.016 seconds
[0m11:04:28.521256 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m11:04:28.522634 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m11:04:28.545418 [debug] [Thread-6 (]: SQL status: OK in 0.021 seconds
[0m11:04:28.552279 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m11:04:28.555488 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c10495d0>]}
[0m11:04:28.558216 [info ] [Thread-6 (]: 6 of 14 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 16.08s]
[0m11:04:28.560897 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m11:04:29.319145 [debug] [Thread-12 ]: SQL status: OK in 16.182 seconds
[0m11:04:29.326673 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m11:04:29.327755 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m11:04:29.330359 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m11:04:29.334758 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m11:04:29.335663 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m11:04:29.336438 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m11:04:29.345707 [debug] [Thread-12 ]: SQL status: OK in 0.008 seconds
[0m11:04:29.353240 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m11:04:29.354292 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m11:04:29.356162 [debug] [Thread-1 (]: SQL status: OK in 16.440 seconds
[0m11:04:29.363200 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m11:04:29.364089 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m11:04:29.364944 [debug] [Thread-12 ]: SQL status: OK in 0.008 seconds
[0m11:04:29.368070 [debug] [Thread-12 ]: On model.mta.subway_station_stats: Close
[0m11:04:29.369771 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f241fb108b0>]}
[0m11:04:29.370766 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m11:04:29.371947 [info ] [Thread-12 ]: 12 of 14 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 16.87s]
[0m11:04:29.375991 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m11:04:29.377344 [debug] [Thread-12 ]: Finished running node model.mta.subway_station_stats
[0m11:04:29.378167 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m11:04:29.379910 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m11:04:29.398860 [debug] [Thread-1 (]: SQL status: OK in 0.018 seconds
[0m11:04:29.403357 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m11:04:29.404125 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m11:04:29.405449 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:04:29.408671 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m11:04:29.611107 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5984e91-474d-47fa-89be-c525326924ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c0366140>]}
[0m11:04:29.612354 [info ] [Thread-1 (]: 1 of 14 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 17.15s]
[0m11:04:29.613378 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m11:04:29.651236 [debug] [MainThread]: Using duckdb connection "master"
[0m11:04:29.652239 [debug] [MainThread]: On master: BEGIN
[0m11:04:29.653738 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:04:29.718063 [debug] [MainThread]: SQL status: OK in 0.065 seconds
[0m11:04:29.718924 [debug] [MainThread]: On master: COMMIT
[0m11:04:29.719448 [debug] [MainThread]: Using duckdb connection "master"
[0m11:04:29.719909 [debug] [MainThread]: On master: COMMIT
[0m11:04:29.720672 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:04:29.721209 [debug] [MainThread]: On master: Close
[0m11:04:29.727458 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:04:29.728152 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m11:04:29.728652 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m11:04:29.729116 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m11:04:29.729545 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m11:04:29.729953 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m11:04:29.730373 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m11:04:29.730773 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m11:04:29.731168 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m11:04:29.731800 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m11:04:29.732204 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m11:04:29.732593 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m11:04:29.732980 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m11:04:29.733376 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m11:04:29.733762 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m11:04:29.737967 [info ] [MainThread]: 
[0m11:04:29.739639 [info ] [MainThread]: Finished running 14 table models in 0 hours 0 minutes and 17.61 seconds (17.61s).
[0m11:04:29.745313 [debug] [MainThread]: Command end result
[0m11:04:29.860100 [info ] [MainThread]: 
[0m11:04:29.861171 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:04:29.861780 [info ] [MainThread]: 
[0m11:04:29.862922 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m11:04:29.876183 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 18.794596, "process_user_time": 178.96394, "process_kernel_time": 44.26046, "process_mem_max_rss": "740168", "process_out_blocks": "16704", "process_in_blocks": "0"}
[0m11:04:29.885389 [debug] [MainThread]: Command `dbt run` succeeded at 11:04:29.884926 after 18.81 seconds
[0m11:04:29.887665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24cde77460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c100a8c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24c92c5810>]}
[0m11:04:29.888359 [debug] [MainThread]: Flushing usage events
[0m11:04:39.938977 [debug] [MainThread]: Error sending anonymous usage statistics. Disabling tracking for this execution. If you wish to permanently disable tracking, see: https://docs.getdbt.com/reference/global-configs#send-anonymous-usage-stats.
[0m17:20:51.371437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2df4373d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ddae8be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ddae8b50>]}


============================== 17:20:51.378989 | 880fb0e3-5e5d-485d-aca6-8ae6857a22dd ==============================
[0m17:20:51.378989 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:20:51.379371 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:20:51.384556 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'LAKE_PATH'
[0m17:20:51.387212 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.06890812, "process_user_time": 1.052992, "process_kernel_time": 0.35762, "process_mem_max_rss": "91144", "process_in_blocks": "34816", "process_out_blocks": "16", "command_success": false}
[0m17:20:51.387729 [debug] [MainThread]: Command `dbt run` failed at 17:20:51.387638 after 0.07 seconds
[0m17:20:51.388093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2df4373d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ddb82ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ddb83610>]}
[0m17:20:51.388411 [debug] [MainThread]: Flushing usage events
[0m17:21:32.498698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f490abd73a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f490a7a9db0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f490928aa40>]}


============================== 17:21:32.500832 | e2b4737e-bb11-43cb-890c-7e98b066c8c2 ==============================
[0m17:21:32.500832 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:21:32.501340 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:21:32.713507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4909250d60>]}
[0m17:21:32.764905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4909200790>]}
[0m17:21:32.768688 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m17:21:32.780997 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m17:21:32.961186 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:21:32.961854 [debug] [MainThread]: Partial parsing: updated file: mta://models/labor_expenses_per_agency.sql
[0m17:21:33.194842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdce0130>]}
[0m17:21:33.298138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd2fac0>]}
[0m17:21:33.298637 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m17:21:33.299019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd2f8b0>]}
[0m17:21:33.300533 [info ] [MainThread]: 
[0m17:21:33.301057 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m17:21:33.304997 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m17:21:33.467298 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m17:21:33.467691 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m17:21:33.467997 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:21:33.490506 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m17:21:33.492322 [debug] [ThreadPool]: On list_mtastats: Close
[0m17:21:33.494749 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m17:21:33.495208 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m17:21:33.499963 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m17:21:33.500282 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m17:21:33.500553 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:21:33.507692 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m17:21:33.509258 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m17:21:33.509656 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m17:21:33.510806 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m17:21:33.511147 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m17:21:33.511456 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m17:21:33.512555 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m17:21:33.513229 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m17:21:33.513474 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m17:21:33.513731 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m17:21:33.514159 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m17:21:33.514467 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m17:21:33.517749 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m17:21:33.521970 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m17:21:33.522293 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m17:21:33.522571 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:21:33.529421 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m17:21:33.529842 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m17:21:33.530189 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m17:21:33.547328 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m17:21:33.548982 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m17:21:33.550497 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m17:21:33.550806 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m17:21:33.554072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f490aec2c50>]}
[0m17:21:33.554544 [debug] [MainThread]: Using duckdb connection "master"
[0m17:21:33.554816 [debug] [MainThread]: On master: BEGIN
[0m17:21:33.555040 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:21:33.560473 [debug] [MainThread]: SQL status: OK in 0.005 seconds
[0m17:21:33.560809 [debug] [MainThread]: On master: COMMIT
[0m17:21:33.561078 [debug] [MainThread]: Using duckdb connection "master"
[0m17:21:33.561315 [debug] [MainThread]: On master: COMMIT
[0m17:21:33.561765 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m17:21:33.562029 [debug] [MainThread]: On master: Close
[0m17:21:33.564176 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m17:21:33.564689 [info ] [MainThread]: 
[0m17:21:33.574940 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m17:21:33.575508 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m17:21:33.575980 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m17:21:33.576493 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m17:21:33.577292 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m17:21:33.577970 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m17:21:33.578700 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m17:21:33.579359 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m17:21:33.579985 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m17:21:33.580403 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m17:21:33.580841 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m17:21:33.581326 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m17:21:33.581927 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m17:21:33.582323 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m17:21:33.582992 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m17:21:33.583813 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m17:21:33.584683 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m17:21:33.585534 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m17:21:33.586104 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m17:21:33.587082 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m17:21:33.588670 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m17:21:33.589636 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m17:21:33.590424 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m17:21:33.591225 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m17:21:33.591996 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m17:21:33.592791 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m17:21:33.593425 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m17:21:33.593962 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m17:21:33.594609 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m17:21:33.595223 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m17:21:33.595869 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m17:21:33.596474 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m17:21:33.597043 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m17:21:33.597744 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m17:21:33.598399 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m17:21:33.599767 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m17:21:33.609797 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m17:21:33.610592 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m17:21:33.611277 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m17:21:33.611827 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m17:21:33.615033 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m17:21:33.616284 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m17:21:33.617785 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m17:21:33.618926 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m17:21:33.619740 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m17:21:33.622844 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m17:21:33.623901 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m17:21:33.624595 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m17:21:33.625146 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m17:21:33.625628 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m17:21:33.626246 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m17:21:33.626726 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m17:21:33.627168 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m17:21:33.627867 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m17:21:33.628504 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m17:21:33.635087 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m17:21:33.636311 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m17:21:33.637030 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m17:21:33.637684 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m17:21:33.638235 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m17:21:33.638580 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m17:21:33.639214 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m17:21:33.639803 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m17:21:33.641859 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m17:21:33.644927 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m17:21:33.648995 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m17:21:33.652591 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m17:21:33.655729 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m17:21:33.657829 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m17:21:33.686225 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m17:21:33.694900 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m17:21:33.695590 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m17:21:33.697715 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m17:21:33.699521 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m17:21:33.701730 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m17:21:33.703536 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m17:21:33.706315 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m17:21:33.707637 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m17:21:33.708060 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m17:21:33.709865 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m17:21:33.710519 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m17:21:33.710882 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m17:21:33.713714 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m17:21:33.714117 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m17:21:33.714968 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m17:21:33.716116 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m17:21:33.716946 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m17:21:33.717672 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m17:21:33.720869 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m17:21:33.721864 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m17:21:33.722232 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m17:21:33.724423 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m17:21:33.724860 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m17:21:33.726921 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m17:21:33.729090 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m17:21:33.732716 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m17:21:33.736367 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m17:21:33.739541 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m17:21:33.740200 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m17:21:33.743662 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m17:21:33.746547 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m17:21:33.747502 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m17:21:33.750313 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m17:21:33.750891 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m17:21:33.751318 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m17:21:33.751868 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m17:21:33.753209 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m17:21:33.754245 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m17:21:33.754957 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m17:21:33.756154 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m17:21:33.756994 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m17:21:33.757462 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:21:33.757832 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m17:21:33.758778 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m17:21:33.759212 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m17:21:33.759728 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m17:21:33.760281 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m17:21:33.760721 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m17:21:33.761124 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m17:21:33.761932 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m17:21:33.762385 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m17:21:33.762823 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m17:21:33.763264 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m17:21:33.763724 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m17:21:33.764131 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m17:21:33.777147 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m17:21:33.777842 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m17:21:33.778424 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m17:21:33.779150 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m17:21:33.779934 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m17:21:33.780471 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m17:21:33.781201 [debug] [Thread-1 (]: SQL status: OK in 0.024 seconds
[0m17:21:33.781911 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m17:21:33.782400 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:21:33.782966 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m17:21:33.783874 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m17:21:33.784589 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m17:21:33.785375 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m17:21:33.785937 [debug] [Thread-2 (]: SQL status: OK in 0.025 seconds
[0m17:21:33.786450 [debug] [Thread-3 (]: SQL status: OK in 0.024 seconds
[0m17:21:33.786831 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m17:21:33.787649 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m17:21:33.788204 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m17:21:33.788668 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m17:21:33.790861 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m17:21:33.791336 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m17:21:33.791693 [debug] [Thread-4 (]: SQL status: OK in 0.013 seconds
[0m17:21:33.792535 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m17:21:33.793031 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m17:21:33.793644 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m17:21:33.794505 [debug] [Thread-6 (]: SQL status: OK in 0.015 seconds
[0m17:21:33.794922 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m17:21:33.797200 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m17:21:33.798630 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m17:21:33.799488 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m17:21:33.800426 [debug] [Thread-9 (]: SQL status: OK in 0.019 seconds
[0m17:21:33.802062 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m17:21:33.802509 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m17:21:33.803615 [debug] [Thread-7 (]: SQL status: OK in 0.021 seconds
[0m17:21:33.803972 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m17:21:33.805269 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m17:21:33.812176 [debug] [Thread-5 (]: SQL status: OK in 0.029 seconds
[0m17:21:33.812767 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m17:21:33.813493 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m17:21:33.815934 [debug] [Thread-10 ]: SQL status: OK in 0.032 seconds
[0m17:21:33.816388 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m17:21:33.816791 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m17:21:33.820391 [debug] [Thread-8 (]: SQL status: OK in 0.036 seconds
[0m17:21:33.825876 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m17:21:33.826419 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m17:21:33.840792 [debug] [Thread-11 ]: SQL status: OK in 0.055 seconds
[0m17:21:33.842181 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m17:21:33.842657 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m17:21:33.850934 [debug] [Thread-13 ]: SQL status: OK in 0.064 seconds
[0m17:21:33.851455 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m17:21:33.851942 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m17:21:33.858614 [debug] [Thread-12 ]: SQL status: OK in 0.071 seconds
[0m17:21:33.859151 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m17:21:33.859638 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m17:21:33.864120 [debug] [Thread-14 ]: SQL status: OK in 0.076 seconds
[0m17:21:33.864785 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m17:21:33.865237 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      with weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
)
SELECT 
    station_complex, 
    week_start, 
    total_weekly_ridership
FROM 
    weekly_ridership
ORDER BY 
    station_complex, 
    week_start
    );
  
  
[0m17:21:34.032960 [debug] [Thread-5 (]: SQL status: OK in 0.219 seconds
[0m17:21:34.052963 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m17:21:34.053690 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m17:21:34.054785 [debug] [Thread-5 (]: SQL status: OK in 0.000 seconds
[0m17:21:34.105618 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m17:21:34.106457 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m17:21:34.107064 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m17:21:34.114377 [debug] [Thread-9 (]: SQL status: OK in 0.311 seconds
[0m17:21:34.115121 [debug] [Thread-5 (]: SQL status: OK in 0.007 seconds
[0m17:21:34.119762 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m17:21:34.129559 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m17:21:34.130549 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m17:21:34.131254 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m17:21:34.133794 [debug] [Thread-9 (]: SQL status: OK in 0.002 seconds
[0m17:21:34.135620 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m17:21:34.136082 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m17:21:34.136440 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m17:21:34.136819 [debug] [Thread-5 (]: SQL status: OK in 0.003 seconds
[0m17:21:34.139325 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m17:21:34.141181 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd65ea0>]}
[0m17:21:34.142100 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 0.54s]
[0m17:21:34.142782 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m17:21:34.143436 [debug] [Thread-9 (]: SQL status: OK in 0.006 seconds
[0m17:21:34.147462 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m17:21:34.148254 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m17:21:34.149793 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m17:21:34.151562 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m17:21:34.152322 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd6b010>]}
[0m17:21:34.153094 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 0.54s]
[0m17:21:34.153773 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m17:21:34.174249 [debug] [Thread-4 (]: SQL status: OK in 0.373 seconds
[0m17:21:34.190481 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m17:21:34.191615 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m17:21:34.193332 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m17:21:34.196303 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m17:21:34.201557 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m17:21:34.203210 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m17:21:34.211976 [debug] [Thread-8 (]: SQL status: OK in 0.385 seconds
[0m17:21:34.225185 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m17:21:34.225971 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m17:21:34.226488 [debug] [Thread-4 (]: SQL status: OK in 0.018 seconds
[0m17:21:34.242300 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m17:21:34.243167 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m17:21:34.243949 [debug] [Thread-8 (]: SQL status: OK in 0.017 seconds
[0m17:21:34.249926 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m17:21:34.250542 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m17:21:34.251095 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m17:21:34.251569 [debug] [Thread-4 (]: SQL status: OK in 0.007 seconds
[0m17:21:34.253252 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m17:21:34.253926 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd66f20>]}
[0m17:21:34.254678 [info ] [Thread-4 (]: 4 of 14 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 0.66s]
[0m17:21:34.255447 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m17:21:34.256421 [debug] [Thread-8 (]: SQL status: OK in 0.004 seconds
[0m17:21:34.279861 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m17:21:34.280421 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m17:21:34.283219 [debug] [Thread-8 (]: SQL status: OK in 0.002 seconds
[0m17:21:34.288077 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m17:21:34.288997 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd643d0>]}
[0m17:21:34.290239 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 0.68s]
[0m17:21:34.292498 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m17:21:34.564369 [debug] [Thread-7 (]: SQL status: OK in 0.758 seconds
[0m17:21:34.577293 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m17:21:34.578402 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m17:21:34.580926 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m17:21:34.586571 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m17:21:34.587642 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m17:21:34.588782 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m17:21:34.596337 [debug] [Thread-7 (]: SQL status: OK in 0.006 seconds
[0m17:21:34.612725 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m17:21:34.615263 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m17:21:34.617688 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m17:21:34.619658 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m17:21:34.620769 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd64250>]}
[0m17:21:34.621796 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 1.01s]
[0m17:21:34.622841 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m17:21:34.805418 [debug] [Thread-2 (]: SQL status: OK in 1.009 seconds
[0m17:21:34.813633 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m17:21:34.814617 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m17:21:34.816907 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m17:21:34.820005 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m17:21:34.826808 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m17:21:34.827640 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m17:21:34.835093 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m17:21:34.840359 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m17:21:34.841506 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m17:21:34.842613 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m17:21:34.845099 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m17:21:34.846428 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4905f964a0>]}
[0m17:21:34.847889 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 1.26s]
[0m17:21:34.849166 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m17:21:38.883645 [debug] [Thread-3 (]: SQL status: OK in 5.086 seconds
[0m17:21:38.888420 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m17:21:38.888911 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m17:21:38.895825 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m17:21:38.897629 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m17:21:38.898078 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m17:21:38.898434 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m17:21:38.904145 [debug] [Thread-3 (]: SQL status: OK in 0.005 seconds
[0m17:21:38.915114 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m17:21:38.915664 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m17:21:38.921207 [debug] [Thread-3 (]: SQL status: OK in 0.005 seconds
[0m17:21:38.923262 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m17:21:38.924504 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd65ff0>]}
[0m17:21:38.925289 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 5.34s]
[0m17:21:38.925987 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m17:21:39.264496 [debug] [Thread-1 (]: SQL status: OK in 5.469 seconds
[0m17:21:39.280585 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m17:21:39.289823 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m17:21:39.291431 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m17:21:39.294641 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m17:21:39.296162 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m17:21:39.296763 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m17:21:39.320697 [debug] [Thread-1 (]: SQL status: OK in 0.023 seconds
[0m17:21:39.327288 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m17:21:39.328293 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m17:21:39.329615 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m17:21:39.332103 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m17:21:39.333171 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd645e0>]}
[0m17:21:39.334267 [info ] [Thread-1 (]: 1 of 14 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 5.75s]
[0m17:21:39.346031 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m17:21:41.985556 [debug] [Thread-14 ]: SQL status: OK in 8.120 seconds
[0m17:21:41.993091 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m17:21:41.994136 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m17:21:41.995525 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m17:21:41.998851 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m17:21:41.999796 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m17:21:42.000650 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m17:21:42.024388 [debug] [Thread-14 ]: SQL status: OK in 0.023 seconds
[0m17:21:42.029393 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m17:21:42.031266 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m17:21:42.032495 [debug] [Thread-14 ]: SQL status: OK in 0.000 seconds
[0m17:21:42.035358 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m17:21:42.038985 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd69600>]}
[0m17:21:42.041931 [info ] [Thread-14 ]: 14 of 14 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 8.41s]
[0m17:21:42.043475 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m17:21:42.109069 [debug] [Thread-13 ]: SQL status: OK in 8.256 seconds
[0m17:21:42.119944 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m17:21:42.125796 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m17:21:42.127433 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m17:21:42.130748 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m17:21:42.131604 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m17:21:42.132231 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m17:21:42.165290 [debug] [Thread-13 ]: SQL status: OK in 0.032 seconds
[0m17:21:42.169493 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m17:21:42.170232 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m17:21:42.171279 [debug] [Thread-13 ]: SQL status: OK in 0.000 seconds
[0m17:21:42.173634 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: Close
[0m17:21:42.174879 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4816327c40>]}
[0m17:21:42.176188 [info ] [Thread-13 ]: 13 of 14 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 8.55s]
[0m17:21:42.177307 [debug] [Thread-13 ]: Finished running node model.mta.total_riders_per_station
[0m17:21:42.714073 [debug] [Thread-11 ]: SQL status: OK in 8.871 seconds
[0m17:21:42.717769 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m17:21:42.718290 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m17:21:42.719162 [debug] [Thread-11 ]: SQL status: OK in 0.000 seconds
[0m17:21:42.720827 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m17:21:42.721223 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m17:21:42.721554 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m17:21:42.726409 [debug] [Thread-11 ]: SQL status: OK in 0.004 seconds
[0m17:21:42.729732 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m17:21:42.730596 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m17:21:42.731790 [debug] [Thread-11 ]: SQL status: OK in 0.000 seconds
[0m17:21:42.734431 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: Close
[0m17:21:42.735858 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd65c60>]}
[0m17:21:42.737081 [info ] [Thread-11 ]: 11 of 14 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 9.12s]
[0m17:21:42.738195 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_increase
[0m17:21:43.393156 [debug] [Thread-10 ]: SQL status: OK in 9.576 seconds
[0m17:21:43.398228 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m17:21:43.398953 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m17:21:43.400727 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m17:21:43.404010 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m17:21:43.404953 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m17:21:43.405632 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m17:21:43.412389 [debug] [Thread-10 ]: SQL status: OK in 0.006 seconds
[0m17:21:43.423132 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m17:21:43.423875 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m17:21:43.425078 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m17:21:43.427520 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: Close
[0m17:21:43.428656 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd6a080>]}
[0m17:21:43.429767 [info ] [Thread-10 ]: 10 of 14 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 9.81s]
[0m17:21:43.430878 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_by_station
[0m17:21:47.269618 [debug] [Thread-6 (]: SQL status: OK in 13.468 seconds
[0m17:21:47.282898 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m17:21:47.285109 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m17:21:47.288663 [debug] [Thread-6 (]: SQL status: OK in 0.002 seconds
[0m17:21:47.291988 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m17:21:47.292769 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m17:21:47.293273 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m17:21:47.300297 [debug] [Thread-6 (]: SQL status: OK in 0.006 seconds
[0m17:21:47.303104 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m17:21:47.303542 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m17:21:47.304500 [debug] [Thread-6 (]: SQL status: OK in 0.000 seconds
[0m17:21:47.306059 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m17:21:47.306795 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd67a00>]}
[0m17:21:47.307455 [info ] [Thread-6 (]: 6 of 14 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 13.70s]
[0m17:21:47.308052 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m17:21:47.378248 [debug] [Thread-12 ]: SQL status: OK in 13.518 seconds
[0m17:21:47.383462 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m17:21:47.383934 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m17:21:47.384998 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m17:21:47.386691 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m17:21:47.387092 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m17:21:47.387405 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m17:21:47.395924 [debug] [Thread-12 ]: SQL status: OK in 0.008 seconds
[0m17:21:47.399661 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m17:21:47.400088 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m17:21:47.400872 [debug] [Thread-12 ]: SQL status: OK in 0.000 seconds
[0m17:21:47.402141 [debug] [Thread-12 ]: On model.mta.subway_station_stats: Close
[0m17:21:47.516248 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2b4737e-bb11-43cb-890c-7e98b066c8c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdd6b550>]}
[0m17:21:47.517199 [info ] [Thread-12 ]: 12 of 14 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 13.89s]
[0m17:21:47.517935 [debug] [Thread-12 ]: Finished running node model.mta.subway_station_stats
[0m17:21:47.530352 [debug] [MainThread]: Using duckdb connection "master"
[0m17:21:47.531115 [debug] [MainThread]: On master: BEGIN
[0m17:21:47.531576 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:21:47.572968 [debug] [MainThread]: SQL status: OK in 0.041 seconds
[0m17:21:47.573621 [debug] [MainThread]: On master: COMMIT
[0m17:21:47.573978 [debug] [MainThread]: Using duckdb connection "master"
[0m17:21:47.574250 [debug] [MainThread]: On master: COMMIT
[0m17:21:47.575222 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m17:21:47.575556 [debug] [MainThread]: On master: Close
[0m17:21:47.579926 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:21:47.580590 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m17:21:47.581053 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m17:21:47.581379 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m17:21:47.581678 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m17:21:47.581964 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m17:21:47.582194 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m17:21:47.582426 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m17:21:47.582775 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m17:21:47.583026 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m17:21:47.583300 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m17:21:47.583566 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m17:21:47.583845 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m17:21:47.584139 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m17:21:47.584867 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m17:21:47.586340 [info ] [MainThread]: 
[0m17:21:47.587316 [info ] [MainThread]: Finished running 14 table models in 0 hours 0 minutes and 14.28 seconds (14.28s).
[0m17:21:47.590065 [debug] [MainThread]: Command end result
[0m17:21:47.656203 [info ] [MainThread]: 
[0m17:21:47.657347 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:21:47.657933 [info ] [MainThread]: 
[0m17:21:47.658662 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m17:21:47.663931 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.210682, "process_user_time": 160.66356, "process_kernel_time": 15.337116, "process_mem_max_rss": "969640", "process_in_blocks": "2012944", "process_out_blocks": "17608"}
[0m17:21:47.666818 [debug] [MainThread]: Command `dbt run` succeeded at 17:21:47.666485 after 15.21 seconds
[0m17:21:47.668241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f490abd73a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4909a9fee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f48fdcce410>]}
[0m17:21:47.668930 [debug] [MainThread]: Flushing usage events
[0m18:32:25.552419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f654d523400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f654d0e83a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f654bd81ea0>]}


============================== 18:32:25.557385 | 256acef6-ac8b-44d2-89f0-7b85ce58e2c1 ==============================
[0m18:32:25.557385 [info ] [MainThread]: Running with dbt=1.8.7
[0m18:32:25.557777 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'version_check': 'True', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:32:25.754326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6540b7e0e0>]}
[0m18:32:25.805774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f654c9f6800>]}
[0m18:32:25.810717 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m18:32:25.823004 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m18:32:25.925510 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:32:25.926069 [debug] [MainThread]: Partial parsing: updated file: mta://models/weekly_riders_per_station.sql
[0m18:32:26.139105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65406bc130>]}
[0m18:32:26.217509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65408d6350>]}
[0m18:32:26.218097 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m18:32:26.218444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6540723ee0>]}
[0m18:32:26.219841 [info ] [MainThread]: 
[0m18:32:26.220333 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:32:26.224112 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m18:32:26.328169 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m18:32:26.328591 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m18:32:26.328917 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:32:26.340393 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m18:32:26.341497 [debug] [ThreadPool]: On list_mtastats: Close
[0m18:32:26.343257 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m18:32:26.343708 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m18:32:26.348140 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m18:32:26.348423 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m18:32:26.348690 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:32:26.357941 [debug] [ThreadPool]: SQL status: OK in 0.009 seconds
[0m18:32:26.359250 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m18:32:26.359713 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m18:32:26.361158 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m18:32:26.361404 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m18:32:26.361637 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m18:32:26.362115 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m18:32:26.362688 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m18:32:26.362901 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m18:32:26.363117 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m18:32:26.363536 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m18:32:26.363772 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m18:32:26.367439 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m18:32:26.371414 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m18:32:26.371707 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m18:32:26.371973 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:32:26.377387 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m18:32:26.377654 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m18:32:26.377912 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m18:32:26.394824 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m18:32:26.395998 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m18:32:26.397376 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m18:32:26.397633 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m18:32:26.400442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f654d194820>]}
[0m18:32:26.400796 [debug] [MainThread]: Using duckdb connection "master"
[0m18:32:26.401024 [debug] [MainThread]: On master: BEGIN
[0m18:32:26.401222 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:32:26.406105 [debug] [MainThread]: SQL status: OK in 0.005 seconds
[0m18:32:26.406370 [debug] [MainThread]: On master: COMMIT
[0m18:32:26.406629 [debug] [MainThread]: Using duckdb connection "master"
[0m18:32:26.406878 [debug] [MainThread]: On master: COMMIT
[0m18:32:26.407290 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m18:32:26.407509 [debug] [MainThread]: On master: Close
[0m18:32:26.409219 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m18:32:26.409531 [info ] [MainThread]: 
[0m18:32:26.419665 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m18:32:26.420328 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m18:32:26.421027 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m18:32:26.421350 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m18:32:26.421821 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m18:32:26.428171 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m18:32:26.432281 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m18:32:26.433016 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m18:32:26.433730 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m18:32:26.434731 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m18:32:26.435251 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m18:32:26.435737 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m18:32:26.436216 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m18:32:26.436731 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m18:32:26.437291 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m18:32:26.437923 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m18:32:26.438798 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m18:32:26.439341 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m18:32:26.439906 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m18:32:26.440768 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m18:32:26.441644 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m18:32:26.442998 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m18:32:26.443903 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m18:32:26.444796 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m18:32:26.446281 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m18:32:26.447589 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m18:32:26.448211 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m18:32:26.448799 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m18:32:26.449438 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m18:32:26.451104 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m18:32:26.451760 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m18:32:26.452281 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m18:32:26.452842 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m18:32:26.453501 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m18:32:26.454186 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m18:32:26.454875 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m18:32:26.455637 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m18:32:26.478964 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m18:32:26.492126 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m18:32:26.492950 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m18:32:26.493679 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m18:32:26.494284 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m18:32:26.494881 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m18:32:26.495370 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m18:32:26.495954 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m18:32:26.498138 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m18:32:26.498738 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m18:32:26.499266 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m18:32:26.499873 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m18:32:26.500583 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m18:32:26.501232 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m18:32:26.501781 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m18:32:26.506004 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m18:32:26.507399 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m18:32:26.508175 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m18:32:26.508678 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m18:32:26.511861 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m18:32:26.512319 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m18:32:26.512807 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m18:32:26.513453 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m18:32:26.513874 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m18:32:26.516115 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m18:32:26.516586 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m18:32:26.517360 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m18:32:26.520607 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m18:32:26.524118 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m18:32:26.524658 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m18:32:26.526558 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m18:32:26.527110 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m18:32:26.529283 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m18:32:26.530071 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m18:32:26.532060 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m18:32:26.535755 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m18:32:26.538067 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m18:32:26.538538 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m18:32:26.540778 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m18:32:26.542853 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m18:32:26.545939 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m18:32:26.547111 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m18:32:26.547568 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m18:32:26.549950 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m18:32:26.551078 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m18:32:26.551475 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:32:26.551792 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m18:32:26.552401 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m18:32:26.555413 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m18:32:26.556177 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m18:32:26.559967 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m18:32:26.560368 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m18:32:26.560921 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m18:32:26.563054 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m18:32:26.563604 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:26.564014 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m18:32:26.564611 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m18:32:26.567196 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m18:32:26.567735 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:26.575194 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m18:32:26.577574 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m18:32:26.580473 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m18:32:26.584446 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m18:32:26.584909 [debug] [Thread-1 (]: SQL status: OK in 0.033 seconds
[0m18:32:26.587343 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m18:32:26.588265 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:26.588906 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m18:32:26.591289 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m18:32:26.591717 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:26.594038 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m18:32:26.594598 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:26.595175 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m18:32:26.596750 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m18:32:26.597562 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:26.598249 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m18:32:26.598615 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:26.599484 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m18:32:26.600032 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m18:32:26.600847 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:26.601337 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m18:32:26.602040 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:26.602865 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m18:32:26.603335 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m18:32:26.604031 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:32:26.604578 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m18:32:26.604940 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m18:32:26.605422 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m18:32:26.605825 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m18:32:26.606294 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m18:32:26.606712 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m18:32:26.607110 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:32:26.607904 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m18:32:26.608422 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m18:32:26.608964 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m18:32:26.609398 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m18:32:26.610092 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m18:32:26.610873 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m18:32:26.611814 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m18:32:26.612273 [debug] [Thread-2 (]: SQL status: OK in 0.012 seconds
[0m18:32:26.612704 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m18:32:26.613255 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m18:32:26.616366 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m18:32:26.617167 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:32:26.618899 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m18:32:26.619376 [debug] [Thread-3 (]: SQL status: OK in 0.015 seconds
[0m18:32:26.620021 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m18:32:26.620445 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m18:32:26.621170 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:26.622335 [debug] [Thread-4 (]: SQL status: OK in 0.015 seconds
[0m18:32:26.622760 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m18:32:26.623821 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:26.624841 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m18:32:26.625664 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:26.626097 [debug] [Thread-7 (]: SQL status: OK in 0.018 seconds
[0m18:32:26.626760 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m18:32:26.630835 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m18:32:26.631562 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:26.633081 [debug] [Thread-5 (]: SQL status: OK in 0.024 seconds
[0m18:32:26.633987 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m18:32:26.634747 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:26.636750 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m18:32:26.637768 [debug] [Thread-11 ]: SQL status: OK in 0.026 seconds
[0m18:32:26.638180 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m18:32:26.638572 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m18:32:26.640136 [debug] [Thread-8 (]: SQL status: OK in 0.027 seconds
[0m18:32:26.640610 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:26.641087 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m18:32:26.642141 [debug] [Thread-9 (]: SQL status: OK in 0.026 seconds
[0m18:32:26.642458 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:26.642906 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m18:32:26.646991 [debug] [Thread-6 (]: SQL status: OK in 0.030 seconds
[0m18:32:26.647837 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:26.648778 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m18:32:26.650804 [debug] [Thread-10 ]: SQL status: OK in 0.032 seconds
[0m18:32:26.651303 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:26.651730 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m18:32:26.654593 [debug] [Thread-13 ]: SQL status: OK in 0.034 seconds
[0m18:32:26.655130 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m18:32:26.655478 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m18:32:26.659320 [debug] [Thread-14 ]: SQL status: OK in 0.039 seconds
[0m18:32:26.659841 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m18:32:26.660403 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start;
    );
  
  
[0m18:32:26.697777 [debug] [Thread-14 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start;
    );
  
  
[0m18:32:26.698745 [debug] [Thread-14 ]: DuckDB adapter: Rolling back transaction.
[0m18:32:26.699678 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: ROLLBACK
[0m18:32:26.700475 [debug] [Thread-12 ]: SQL status: OK in 0.078 seconds
[0m18:32:26.700887 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m18:32:26.701414 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m18:32:26.734207 [debug] [Thread-14 ]: Failed to rollback 'model.mta.weekly_riders_per_station'
[0m18:32:26.735238 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m18:32:26.736519 [debug] [Thread-14 ]: Runtime Error in model weekly_riders_per_station (models/weekly_riders_per_station.sql)
  Parser Error: syntax error at or near ";"
[0m18:32:26.739505 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f654c826830>]}
[0m18:32:26.740348 [error] [Thread-14 ]: 14 of 14 ERROR creating sql table model main.weekly_riders_per_station ......... [[31mERROR[0m in 0.24s]
[0m18:32:26.741173 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m18:32:26.771984 [debug] [Thread-4 (]: SQL status: OK in 0.139 seconds
[0m18:32:26.799409 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:26.801329 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m18:32:26.805325 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m18:32:26.847020 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m18:32:26.847755 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:26.848256 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m18:32:26.872385 [debug] [Thread-4 (]: SQL status: OK in 0.024 seconds
[0m18:32:26.891175 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:26.891960 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m18:32:26.898888 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m18:32:26.905296 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m18:32:26.906110 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6540b99030>]}
[0m18:32:26.906942 [info ] [Thread-4 (]: 4 of 14 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 0.46s]
[0m18:32:26.907692 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m18:32:26.935314 [debug] [Thread-8 (]: SQL status: OK in 0.292 seconds
[0m18:32:26.977467 [debug] [Thread-7 (]: SQL status: OK in 0.341 seconds
[0m18:32:26.989687 [debug] [Thread-5 (]: SQL status: OK in 0.352 seconds
[0m18:32:26.993471 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:27.001284 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m18:32:27.002741 [debug] [Thread-8 (]: SQL status: OK in 0.001 seconds
[0m18:32:27.014902 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:27.015684 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m18:32:27.034018 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:27.040925 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m18:32:27.059963 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:27.052481 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m18:32:27.084554 [debug] [Thread-5 (]: SQL status: OK in 0.003 seconds
[0m18:32:27.060669 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m18:32:27.091280 [debug] [Thread-9 (]: SQL status: OK in 0.448 seconds
[0m18:32:27.123807 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:27.105251 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m18:32:27.092363 [debug] [Thread-7 (]: SQL status: OK in 0.049 seconds
[0m18:32:27.125950 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m18:32:27.138856 [debug] [Thread-8 (]: SQL status: OK in 0.022 seconds
[0m18:32:27.142257 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:27.144127 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m18:32:27.158898 [debug] [Thread-9 (]: SQL status: OK in 0.008 seconds
[0m18:32:27.162449 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m18:32:27.159669 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m18:32:27.160239 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:27.163008 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:27.165761 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m18:32:27.165193 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m18:32:27.163786 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:27.166826 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m18:32:27.180801 [debug] [Thread-5 (]: SQL status: OK in 0.016 seconds
[0m18:32:27.194123 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:27.214293 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m18:32:27.225093 [debug] [Thread-9 (]: SQL status: OK in 0.059 seconds
[0m18:32:27.229261 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:27.229894 [debug] [Thread-7 (]: SQL status: OK in 0.063 seconds
[0m18:32:27.243634 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:27.230489 [debug] [Thread-5 (]: SQL status: OK in 0.015 seconds
[0m18:32:27.263977 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m18:32:27.231030 [debug] [Thread-8 (]: SQL status: OK in 0.063 seconds
[0m18:32:27.267400 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m18:32:27.268556 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f645ccf5870>]}
[0m18:32:27.231749 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m18:32:27.271605 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m18:32:27.260623 [debug] [Thread-2 (]: SQL status: OK in 0.633 seconds
[0m18:32:27.265095 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f645ccf8c10>]}
[0m18:32:27.275181 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m18:32:27.261516 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m18:32:27.269855 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 0.77s]
[0m18:32:27.292221 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:27.294183 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 0.81s]
[0m18:32:27.305923 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f645cd21d80>]}
[0m18:32:27.308121 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m18:32:27.309382 [debug] [Thread-7 (]: SQL status: OK in 0.002 seconds
[0m18:32:27.310801 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m18:32:27.312109 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m18:32:27.318824 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 0.81s]
[0m18:32:27.331054 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m18:32:27.334165 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m18:32:27.340732 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m18:32:27.343723 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m18:32:27.344836 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:27.346048 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f645cd39690>]}
[0m18:32:27.347983 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m18:32:27.349278 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 0.85s]
[0m18:32:27.352369 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m18:32:27.363618 [debug] [Thread-2 (]: SQL status: OK in 0.013 seconds
[0m18:32:27.371950 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:27.379256 [debug] [Thread-3 (]: SQL status: OK in 0.747 seconds
[0m18:32:27.381534 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m18:32:27.397724 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m18:32:27.406958 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m18:32:27.410274 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65407201c0>]}
[0m18:32:27.411455 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 0.97s]
[0m18:32:27.392535 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:27.415232 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m18:32:27.414517 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m18:32:27.416899 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m18:32:27.425064 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m18:32:27.425859 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:27.426512 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m18:32:27.433040 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m18:32:27.447900 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:27.448654 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m18:32:27.449701 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m18:32:27.455962 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m18:32:27.457124 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6540b9a8c0>]}
[0m18:32:27.458375 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 1.01s]
[0m18:32:27.460916 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m18:32:31.780919 [debug] [Thread-6 (]: SQL status: OK in 5.131 seconds
[0m18:32:31.791931 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:31.793057 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m18:32:31.797565 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m18:32:31.801013 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m18:32:31.801871 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:31.807348 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m18:32:31.816037 [debug] [Thread-6 (]: SQL status: OK in 0.007 seconds
[0m18:32:31.845542 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:31.846731 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m18:32:31.848273 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m18:32:31.852033 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m18:32:31.853385 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6540b9b760>]}
[0m18:32:31.854685 [info ] [Thread-6 (]: 6 of 14 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 5.38s]
[0m18:32:31.856295 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m18:32:32.439183 [debug] [Thread-10 ]: SQL status: OK in 5.787 seconds
[0m18:32:32.445548 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:32.448411 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m18:32:32.450256 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m18:32:32.453998 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m18:32:32.454970 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:32.455694 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m18:32:32.462146 [debug] [Thread-10 ]: SQL status: OK in 0.005 seconds
[0m18:32:32.509152 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:32.510295 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m18:32:32.511920 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m18:32:32.519015 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: Close
[0m18:32:32.521436 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6540b99b70>]}
[0m18:32:32.523630 [info ] [Thread-10 ]: 10 of 14 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 6.03s]
[0m18:32:32.525984 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_by_station
[0m18:32:32.725146 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.avg_riders_per_day. Details: Connection(type='duckdb', name='model.mta.avg_riders_per_day', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7f653d8d39a0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m18:32:32.727713 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.avg_riders_per_day
[0m18:32:32.729196 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.omny_adoption_increase. Details: Connection(type='duckdb', name='model.mta.omny_adoption_increase', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7f6540b9a410>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m18:32:32.729947 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.omny_adoption_increase
[0m18:32:32.730708 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.subway_station_stats. Details: Connection(type='duckdb', name='model.mta.subway_station_stats', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7f653d918760>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m18:32:32.731361 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.subway_station_stats
[0m18:32:32.732071 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.total_riders_per_station. Details: Connection(type='duckdb', name='model.mta.total_riders_per_station', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7f653c6ad390>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m18:32:32.732912 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.total_riders_per_station
[0m18:32:32.734749 [error] [MainThread]: CANCEL query model.mta.expense_type_per_year ................................... [[31mCANCEL[0m]
[0m18:32:32.735692 [error] [MainThread]: CANCEL query model.mta.avg_riders_per_day ...................................... [[31mCANCEL[0m]
[0m18:32:32.736437 [error] [MainThread]: CANCEL query model.mta.bond_payment_info ....................................... [[31mCANCEL[0m]
[0m18:32:32.738109 [error] [MainThread]: CANCEL query model.mta.busiest_specific_times .................................. [[31mCANCEL[0m]
[0m18:32:32.739949 [error] [MainThread]: CANCEL query model.mta.daily_ridership ......................................... [[31mCANCEL[0m]
[0m18:32:32.740888 [error] [MainThread]: CANCEL query model.mta.fare_class_boro ......................................... [[31mCANCEL[0m]
[0m18:32:32.741475 [error] [MainThread]: CANCEL query model.mta.forecast_accuracy_2023 .................................. [[31mCANCEL[0m]
[0m18:32:32.742142 [error] [MainThread]: CANCEL query model.mta.labor_expenses_per_agency ............................... [[31mCANCEL[0m]
[0m18:32:32.743009 [error] [MainThread]: CANCEL query model.mta.largest_expense_differences_2023 ........................ [[31mCANCEL[0m]
[0m18:32:32.744103 [error] [MainThread]: CANCEL query model.mta.omny_adoption_by_station ................................ [[31mCANCEL[0m]
[0m18:32:32.744814 [error] [MainThread]: CANCEL query model.mta.omny_adoption_increase .................................. [[31mCANCEL[0m]
[0m18:32:32.745385 [error] [MainThread]: CANCEL query model.mta.subway_station_stats .................................... [[31mCANCEL[0m]
[0m18:32:32.745959 [error] [MainThread]: CANCEL query model.mta.total_riders_per_station ................................ [[31mCANCEL[0m]
[0m18:32:32.746449 [error] [MainThread]: CANCEL query model.mta.weekly_riders_per_station ............................... [[31mCANCEL[0m]
[0m18:32:32.750415 [debug] [Thread-12 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m18:32:32.752531 [debug] [Thread-11 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m18:32:32.753495 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m18:32:32.754264 [debug] [Thread-12 ]: DuckDB adapter: Rolling back transaction.
[0m18:32:32.755106 [debug] [Thread-11 ]: DuckDB adapter: Rolling back transaction.
[0m18:32:32.755802 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m18:32:32.756490 [debug] [Thread-12 ]: On model.mta.subway_station_stats: ROLLBACK
[0m18:32:32.757074 [debug] [Thread-13 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m18:32:32.757734 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: ROLLBACK
[0m18:32:32.758294 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: ROLLBACK
[0m18:32:32.759052 [debug] [Thread-13 ]: DuckDB adapter: Rolling back transaction.
[0m18:32:32.760056 [debug] [Thread-12 ]: Failed to rollback 'model.mta.subway_station_stats'
[0m18:32:32.762231 [debug] [Thread-11 ]: Failed to rollback 'model.mta.omny_adoption_increase'
[0m18:32:32.763360 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: ROLLBACK
[0m18:32:32.764251 [debug] [Thread-1 (]: Failed to rollback 'model.mta.avg_riders_per_day'
[0m18:32:32.764781 [debug] [Thread-12 ]: On model.mta.subway_station_stats: Close
[0m18:32:32.765272 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: Close
[0m18:32:32.765960 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m18:32:32.769006 [debug] [Thread-12 ]: Runtime Error in model subway_station_stats (models/subway_station_stats.sql)
  INTERRUPT Error: Interrupted!
[0m18:32:32.770045 [debug] [Thread-13 ]: Failed to rollback 'model.mta.total_riders_per_station'
[0m18:32:32.771973 [debug] [Thread-11 ]: Runtime Error in model omny_adoption_increase (models/omny_adoption_increase.sql)
  INTERRUPT Error: Interrupted!
[0m18:32:32.772620 [debug] [Thread-1 (]: Runtime Error in model avg_riders_per_day (models/avg_riders_per_day.sql)
  INTERRUPT Error: Interrupted!
[0m18:32:32.773290 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f645cd854b0>]}
[0m18:32:32.773722 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: Close
[0m18:32:32.774185 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f653d8d0a60>]}
[0m18:32:32.774688 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6540b4a950>]}
[0m18:32:32.775418 [error] [Thread-12 ]: 12 of 14 ERROR creating sql table model main.subway_station_stats .............. [[31mERROR[0m in 6.27s]
[0m18:32:32.848553 [debug] [Thread-13 ]: Runtime Error in model total_riders_per_station (models/total_riders_per_station.sql)
  INTERRUPT Error: Interrupted!
[0m18:32:32.849459 [error] [Thread-11 ]: 11 of 14 ERROR creating sql table model main.omny_adoption_increase ............ [[31mERROR[0m in 6.28s]
[0m18:32:32.850937 [debug] [Thread-12 ]: Finished running node model.mta.subway_station_stats
[0m18:32:32.852011 [error] [Thread-1 (]: 1 of 14 ERROR creating sql table model main.avg_riders_per_day ................. [[31mERROR[0m in 6.35s]
[0m18:32:32.852824 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '256acef6-ac8b-44d2-89f0-7b85ce58e2c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f653d8d2f20>]}
[0m18:32:32.853733 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_increase
[0m18:32:32.855104 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m18:32:32.855781 [error] [Thread-13 ]: 13 of 14 ERROR creating sql table model main.total_riders_per_station .......... [[31mERROR[0m in 6.35s]
[0m18:32:32.857081 [debug] [Thread-13 ]: Finished running node model.mta.total_riders_per_station
[0m18:32:32.857642 [info ] [MainThread]: 
[0m18:32:32.858144 [info ] [MainThread]: [33mExited because of keyboard interrupt[0m
[0m18:32:32.858506 [info ] [MainThread]: 
[0m18:32:32.858875 [error] [MainThread]:   Runtime Error in model weekly_riders_per_station (models/weekly_riders_per_station.sql)
  Parser Error: syntax error at or near ";"
[0m18:32:32.859189 [info ] [MainThread]: 
[0m18:32:32.859516 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=1 SKIP=0 TOTAL=10
[0m18:32:32.859810 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:32:32.860041 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m18:32:32.860264 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m18:32:32.860499 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m18:32:32.860706 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m18:32:32.860919 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m18:32:32.861114 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m18:32:32.861325 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m18:32:32.861513 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m18:32:32.861702 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m18:32:32.861889 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m18:32:32.862079 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m18:32:32.862280 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m18:32:32.862467 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m18:32:32.862657 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m18:32:32.863030 [info ] [MainThread]: 
[0m18:32:32.863367 [info ] [MainThread]: Finished running 10 table models in 0 hours 0 minutes and 6.64 seconds (6.64s).
[0m18:32:32.863803 [error] [MainThread]: Encountered an error:

[0m18:32:32.871814 [error] [MainThread]: Traceback (most recent call last):
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/cli/requires.py", line 138, in wrapper
    result, success = func(*args, **kwargs)
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/cli/requires.py", line 101, in wrapper
    return func(*args, **kwargs)
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/cli/requires.py", line 218, in wrapper
    return func(*args, **kwargs)
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/cli/requires.py", line 247, in wrapper
    return func(*args, **kwargs)
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/cli/requires.py", line 294, in wrapper
    return func(*args, **kwargs)
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/cli/requires.py", line 332, in wrapper
    return func(*args, **kwargs)
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/cli/main.py", line 569, in run
    results = task.run()
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/task/runnable.py", line 526, in run
    result = self.execute_with_hooks(selected_uids)
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/task/runnable.py", line 487, in execute_with_hooks
    res = self.execute_nodes()
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/task/runnable.py", line 397, in execute_nodes
    self.run_queue(pool)
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/task/runnable.py", line 315, in run_queue
    self.job_queue.join()
  File "/home/christianocean/mta/.venv/lib/python3.10/site-packages/dbt/graph/queue.py", line 198, in join
    self.inner.join()
  File "/usr/lib/python3.10/queue.py", line 90, in join
    self.all_tasks_done.wait()
  File "/usr/lib/python3.10/threading.py", line 320, in wait
    waiter.acquire()
KeyboardInterrupt

[0m18:32:32.874315 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 7.3692656, "process_user_time": 83.958115, "process_kernel_time": 9.788928, "process_mem_max_rss": "467104", "process_in_blocks": "769816", "process_out_blocks": "7968", "command_success": false}
[0m18:32:32.875071 [debug] [MainThread]: Command `dbt run` failed at 18:32:32.874973 after 7.37 seconds
[0m18:32:32.875456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f654d523400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65408b1810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65406360e0>]}
[0m18:32:32.875808 [debug] [MainThread]: Flushing usage events
[0m18:32:40.452372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f78db3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f6138be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f6138b50>]}


============================== 18:32:40.454934 | 8468ffe2-a80c-4af0-9055-266f21963050 ==============================
[0m18:32:40.454934 [info ] [MainThread]: Running with dbt=1.8.7
[0m18:32:40.455499 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:32:40.642811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f2d0e620>]}
[0m18:32:40.697472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f2ceb520>]}
[0m18:32:40.700271 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m18:32:40.708419 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m18:32:40.782314 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:32:40.783265 [debug] [MainThread]: Partial parsing: updated file: mta://models/weekly_riders_per_station.sql
[0m18:32:41.005872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56ea9bc130>]}
[0m18:32:41.087767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eabd53f0>]}
[0m18:32:41.088344 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m18:32:41.088730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eabd5240>]}
[0m18:32:41.091258 [info ] [MainThread]: 
[0m18:32:41.092042 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m18:32:41.097687 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m18:32:41.165055 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m18:32:41.165529 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m18:32:41.165967 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:32:41.173652 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m18:32:41.174810 [debug] [ThreadPool]: On list_mtastats: Close
[0m18:32:41.176860 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m18:32:41.177229 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m18:32:41.181314 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m18:32:41.181607 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m18:32:41.181862 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:32:41.186978 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m18:32:41.188006 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m18:32:41.188359 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m18:32:41.188970 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m18:32:41.189242 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m18:32:41.189483 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m18:32:41.189897 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m18:32:41.190545 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m18:32:41.190833 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m18:32:41.191094 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m18:32:41.191674 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m18:32:41.191992 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m18:32:41.195611 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m18:32:41.200948 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m18:32:41.201504 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m18:32:41.201780 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:32:41.208429 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m18:32:41.208738 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m18:32:41.209002 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m18:32:41.226345 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m18:32:41.228327 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m18:32:41.228908 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m18:32:41.229180 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m18:32:41.232699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f754d720>]}
[0m18:32:41.233100 [debug] [MainThread]: Using duckdb connection "master"
[0m18:32:41.233365 [debug] [MainThread]: On master: BEGIN
[0m18:32:41.233591 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:32:41.239250 [debug] [MainThread]: SQL status: OK in 0.006 seconds
[0m18:32:41.239542 [debug] [MainThread]: On master: COMMIT
[0m18:32:41.239783 [debug] [MainThread]: Using duckdb connection "master"
[0m18:32:41.239984 [debug] [MainThread]: On master: COMMIT
[0m18:32:41.240361 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m18:32:41.240605 [debug] [MainThread]: On master: Close
[0m18:32:41.242397 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m18:32:41.242711 [info ] [MainThread]: 
[0m18:32:41.250580 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m18:32:41.251213 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m18:32:41.252161 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m18:32:41.253405 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m18:32:41.251788 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m18:32:41.253050 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m18:32:41.254547 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m18:32:41.255205 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m18:32:41.255887 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m18:32:41.256434 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m18:32:41.257031 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m18:32:41.257772 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m18:32:41.258276 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m18:32:41.258760 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m18:32:41.259281 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m18:32:41.259741 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m18:32:41.260502 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m18:32:41.261184 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m18:32:41.261837 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m18:32:41.262239 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m18:32:41.262741 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m18:32:41.263219 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m18:32:41.263971 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m18:32:41.264566 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m18:32:41.265249 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m18:32:41.265782 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m18:32:41.266379 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m18:32:41.267058 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m18:32:41.267742 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m18:32:41.268358 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m18:32:41.269036 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m18:32:41.269576 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m18:32:41.270017 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m18:32:41.270523 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m18:32:41.271155 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m18:32:41.271756 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m18:32:41.272309 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m18:32:41.272873 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m18:32:41.273537 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m18:32:41.283770 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m18:32:41.284610 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m18:32:41.285328 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m18:32:41.287607 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m18:32:41.288220 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m18:32:41.288715 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m18:32:41.289307 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m18:32:41.293170 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m18:32:41.294208 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m18:32:41.295086 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m18:32:41.295559 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m18:32:41.296219 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m18:32:41.299712 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m18:32:41.300304 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m18:32:41.301195 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m18:32:41.301805 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m18:32:41.302578 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m18:32:41.303152 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m18:32:41.307479 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m18:32:41.308618 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m18:32:41.309150 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m18:32:41.310051 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m18:32:41.310603 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m18:32:41.311083 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m18:32:41.314932 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m18:32:41.315441 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m18:32:41.318872 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m18:32:41.322394 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m18:32:41.323137 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m18:32:41.325862 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m18:32:41.327962 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m18:32:41.351178 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m18:32:41.354233 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m18:32:41.357085 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m18:32:41.357536 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m18:32:41.359565 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m18:32:41.361792 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m18:32:41.363578 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m18:32:41.365877 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m18:32:41.366416 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m18:32:41.367199 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m18:32:41.367670 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m18:32:41.369875 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m18:32:41.370869 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m18:32:41.371329 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m18:32:41.372508 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m18:32:41.373028 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:41.373364 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m18:32:41.376426 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m18:32:41.377065 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m18:32:41.377906 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m18:32:41.378475 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m18:32:41.380775 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m18:32:41.383115 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m18:32:41.383577 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:41.386767 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m18:32:41.390208 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m18:32:41.391017 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:41.393443 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m18:32:41.393971 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m18:32:41.394629 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m18:32:41.397824 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m18:32:41.401816 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m18:32:41.402587 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:41.405541 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m18:32:41.407935 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m18:32:41.408799 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:41.409290 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m18:32:41.410059 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:41.410895 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m18:32:41.411543 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:41.411880 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:41.412468 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:41.412877 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:32:41.413299 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m18:32:41.414196 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m18:32:41.414698 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m18:32:41.415236 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m18:32:41.416185 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m18:32:41.416811 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m18:32:41.417263 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:32:41.417663 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m18:32:41.418268 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m18:32:41.418658 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:32:41.419089 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m18:32:41.419500 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m18:32:41.419883 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m18:32:41.425534 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m18:32:41.427094 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m18:32:41.427432 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m18:32:41.428161 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m18:32:41.428608 [debug] [Thread-2 (]: SQL status: OK in 0.015 seconds
[0m18:32:41.428992 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:32:41.429411 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m18:32:41.430084 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m18:32:41.430643 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m18:32:41.431316 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m18:32:41.431727 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m18:32:41.432561 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m18:32:41.434158 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m18:32:41.435083 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m18:32:41.435575 [debug] [Thread-3 (]: SQL status: OK in 0.018 seconds
[0m18:32:41.436486 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m18:32:41.437465 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:41.438366 [debug] [Thread-4 (]: SQL status: OK in 0.020 seconds
[0m18:32:41.438809 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m18:32:41.439366 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m18:32:41.441074 [debug] [Thread-5 (]: SQL status: OK in 0.014 seconds
[0m18:32:41.441703 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m18:32:41.442177 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:41.443545 [debug] [Thread-6 (]: SQL status: OK in 0.015 seconds
[0m18:32:41.444063 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m18:32:41.445002 [debug] [Thread-7 (]: SQL status: OK in 0.014 seconds
[0m18:32:41.445655 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:41.446298 [debug] [Thread-9 (]: SQL status: OK in 0.015 seconds
[0m18:32:41.447085 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:41.448059 [debug] [Thread-8 (]: SQL status: OK in 0.016 seconds
[0m18:32:41.448590 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m18:32:41.449095 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:41.449988 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:41.450571 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m18:32:41.451719 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:41.452371 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m18:32:41.452956 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:41.453863 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m18:32:41.454271 [debug] [Thread-10 ]: SQL status: OK in 0.022 seconds
[0m18:32:41.455077 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m18:32:41.457014 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m18:32:41.458157 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m18:32:41.458780 [debug] [Thread-11 ]: SQL status: OK in 0.025 seconds
[0m18:32:41.459531 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:41.462112 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m18:32:41.462933 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m18:32:41.464134 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m18:32:41.466047 [debug] [Thread-12 ]: SQL status: OK in 0.030 seconds
[0m18:32:41.467051 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m18:32:41.467555 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
weekday_weekend AS (
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m18:32:41.472915 [debug] [Thread-13 ]: SQL status: OK in 0.034 seconds
[0m18:32:41.473437 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m18:32:41.473895 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m18:32:41.475442 [debug] [Thread-14 ]: SQL status: OK in 0.036 seconds
[0m18:32:41.475846 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m18:32:41.476197 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start
    );
  
  
[0m18:32:41.671351 [debug] [Thread-5 (]: SQL status: OK in 0.213 seconds
[0m18:32:41.696165 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:41.697021 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m18:32:41.698065 [debug] [Thread-5 (]: SQL status: OK in 0.000 seconds
[0m18:32:41.794016 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m18:32:41.795697 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:41.796430 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m18:32:41.820728 [debug] [Thread-9 (]: SQL status: OK in 0.358 seconds
[0m18:32:41.826445 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:41.828789 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m18:32:41.830661 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m18:32:41.833492 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m18:32:41.835566 [debug] [Thread-5 (]: SQL status: OK in 0.031 seconds
[0m18:32:41.836838 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:41.844940 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m18:32:41.851946 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m18:32:41.857928 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m18:32:41.862386 [debug] [Thread-5 (]: SQL status: OK in 0.002 seconds
[0m18:32:41.870189 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m18:32:41.870948 [debug] [Thread-9 (]: SQL status: OK in 0.025 seconds
[0m18:32:41.871895 [debug] [Thread-8 (]: SQL status: OK in 0.410 seconds
[0m18:32:41.874280 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f79674f0>]}
[0m18:32:41.880660 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m18:32:41.909957 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m18:32:41.905703 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 0.60s]
[0m18:32:41.931868 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:41.942724 [debug] [Thread-9 (]: SQL status: OK in 0.032 seconds
[0m18:32:41.949253 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m18:32:41.950079 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m18:32:41.957512 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m18:32:41.966840 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56e3bf7550>]}
[0m18:32:41.968612 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 0.68s]
[0m18:32:41.969691 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m18:32:41.970818 [debug] [Thread-8 (]: SQL status: OK in 0.008 seconds
[0m18:32:41.985151 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m18:32:41.986794 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:41.987408 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m18:32:42.005454 [debug] [Thread-8 (]: SQL status: OK in 0.017 seconds
[0m18:32:42.034646 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m18:32:42.038142 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m18:32:42.039597 [debug] [Thread-8 (]: SQL status: OK in 0.001 seconds
[0m18:32:42.043868 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m18:32:42.045446 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eaa22a70>]}
[0m18:32:42.048272 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 0.76s]
[0m18:32:42.049275 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m18:32:42.054649 [debug] [Thread-7 (]: SQL status: OK in 0.594 seconds
[0m18:32:42.086844 [debug] [Thread-4 (]: SQL status: OK in 0.628 seconds
[0m18:32:42.101527 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:42.122925 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m18:32:42.153039 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:42.155179 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m18:32:42.157099 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m18:32:42.160410 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m18:32:42.161313 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:42.162114 [debug] [Thread-7 (]: SQL status: OK in 0.006 seconds
[0m18:32:42.172709 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m18:32:42.180816 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m18:32:42.192715 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:42.194930 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m18:32:42.224070 [debug] [Thread-4 (]: SQL status: OK in 0.039 seconds
[0m18:32:42.230200 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m18:32:42.231227 [debug] [Thread-7 (]: SQL status: OK in 0.030 seconds
[0m18:32:42.232852 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m18:32:42.240164 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m18:32:42.243234 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m18:32:42.244786 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m18:32:42.252430 [debug] [Thread-4 (]: SQL status: OK in 0.011 seconds
[0m18:32:42.277020 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m18:32:42.278234 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eaa22470>]}
[0m18:32:42.279663 [info ] [Thread-4 (]: 4 of 14 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 1.01s]
[0m18:32:42.280869 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m18:32:42.256422 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m18:32:42.282660 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56ead011b0>]}
[0m18:32:42.285159 [debug] [Thread-2 (]: SQL status: OK in 0.835 seconds
[0m18:32:42.283900 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 1.01s]
[0m18:32:42.296521 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m18:32:42.314409 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:42.325064 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m18:32:42.332700 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m18:32:42.338788 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m18:32:42.340135 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:42.340902 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m18:32:42.355874 [debug] [Thread-2 (]: SQL status: OK in 0.014 seconds
[0m18:32:42.361085 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m18:32:42.375104 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m18:32:42.376670 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m18:32:42.379385 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m18:32:42.380594 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f2d7bd00>]}
[0m18:32:42.381851 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 1.12s]
[0m18:32:42.404653 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m18:32:43.573655 [debug] [Thread-1 (]: SQL status: OK in 2.126 seconds
[0m18:32:43.580113 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m18:32:43.581721 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m18:32:43.583074 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m18:32:43.586088 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m18:32:43.587428 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m18:32:43.588434 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m18:32:43.596207 [debug] [Thread-1 (]: SQL status: OK in 0.007 seconds
[0m18:32:43.599254 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m18:32:43.600423 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m18:32:43.601796 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m18:32:43.604818 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m18:32:43.606417 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eaa21000>]}
[0m18:32:43.607781 [info ] [Thread-1 (]: 1 of 14 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 2.35s]
[0m18:32:43.609032 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m18:32:45.874611 [debug] [Thread-3 (]: SQL status: OK in 4.421 seconds
[0m18:32:45.880831 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:45.889121 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m18:32:45.891806 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m18:32:45.896030 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m18:32:45.898117 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:45.898940 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m18:32:45.906380 [debug] [Thread-3 (]: SQL status: OK in 0.007 seconds
[0m18:32:45.911482 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m18:32:45.912499 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m18:32:45.915982 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m18:32:45.918212 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m18:32:45.918929 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eaa221d0>]}
[0m18:32:45.919640 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 4.66s]
[0m18:32:45.920279 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m18:32:50.626976 [debug] [Thread-14 ]: SQL status: OK in 9.150 seconds
[0m18:32:50.648887 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m18:32:50.649828 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m18:32:50.651299 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m18:32:50.656498 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m18:32:50.675219 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m18:32:50.676064 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m18:32:50.756001 [debug] [Thread-14 ]: SQL status: OK in 0.079 seconds
[0m18:32:50.760553 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m18:32:50.763029 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m18:32:50.764245 [debug] [Thread-14 ]: SQL status: OK in 0.000 seconds
[0m18:32:50.766953 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m18:32:50.775750 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eaa2f010>]}
[0m18:32:50.777133 [info ] [Thread-14 ]: 14 of 14 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 9.48s]
[0m18:32:50.778406 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m18:32:51.361361 [debug] [Thread-10 ]: SQL status: OK in 9.895 seconds
[0m18:32:51.380392 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:51.381407 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m18:32:51.384142 [debug] [Thread-10 ]: SQL status: OK in 0.002 seconds
[0m18:32:51.387379 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m18:32:51.388308 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:51.389012 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m18:32:51.402783 [debug] [Thread-10 ]: SQL status: OK in 0.013 seconds
[0m18:32:51.408291 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m18:32:51.409291 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m18:32:51.413117 [debug] [Thread-10 ]: SQL status: OK in 0.003 seconds
[0m18:32:51.416045 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: Close
[0m18:32:51.417130 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55f5e24d90>]}
[0m18:32:51.418328 [info ] [Thread-10 ]: 10 of 14 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 10.13s]
[0m18:32:51.419450 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_by_station
[0m18:32:51.613103 [debug] [Thread-12 ]: SQL status: OK in 10.142 seconds
[0m18:32:51.643587 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m18:32:51.644564 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m18:32:51.645933 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m18:32:51.650098 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m18:32:51.651006 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m18:32:51.651723 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m18:32:51.695796 [debug] [Thread-12 ]: SQL status: OK in 0.043 seconds
[0m18:32:51.700370 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m18:32:51.701316 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m18:32:51.702499 [debug] [Thread-12 ]: SQL status: OK in 0.000 seconds
[0m18:32:51.704958 [debug] [Thread-12 ]: On model.mta.subway_station_stats: Close
[0m18:32:51.705979 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eaa218a0>]}
[0m18:32:51.707086 [info ] [Thread-12 ]: 12 of 14 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 10.41s]
[0m18:32:51.714880 [debug] [Thread-12 ]: Finished running node model.mta.subway_station_stats
[0m18:32:51.749963 [debug] [Thread-11 ]: SQL status: OK in 10.283 seconds
[0m18:32:51.755799 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m18:32:51.757756 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m18:32:51.759050 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m18:32:51.762069 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m18:32:51.762819 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m18:32:51.763481 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m18:32:51.793316 [debug] [Thread-11 ]: SQL status: OK in 0.029 seconds
[0m18:32:51.798157 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m18:32:51.799038 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m18:32:51.800237 [debug] [Thread-11 ]: SQL status: OK in 0.000 seconds
[0m18:32:51.804236 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: Close
[0m18:32:51.816283 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eaa21720>]}
[0m18:32:51.817683 [info ] [Thread-11 ]: 11 of 14 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 10.53s]
[0m18:32:51.819141 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_increase
[0m18:32:52.581283 [debug] [Thread-13 ]: SQL status: OK in 11.107 seconds
[0m18:32:52.586677 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m18:32:52.587647 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m18:32:52.589015 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m18:32:52.592188 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m18:32:52.593125 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m18:32:52.593868 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m18:32:52.607726 [debug] [Thread-13 ]: SQL status: OK in 0.013 seconds
[0m18:32:52.611816 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m18:32:52.612566 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m18:32:52.613669 [debug] [Thread-13 ]: SQL status: OK in 0.000 seconds
[0m18:32:52.616107 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: Close
[0m18:32:52.617014 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56eaa2e4d0>]}
[0m18:32:52.618037 [info ] [Thread-13 ]: 13 of 14 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 11.32s]
[0m18:32:52.619845 [debug] [Thread-13 ]: Finished running node model.mta.total_riders_per_station
[0m18:32:52.989369 [debug] [Thread-6 (]: SQL status: OK in 11.530 seconds
[0m18:32:52.994270 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:52.995092 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m18:32:52.996250 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m18:32:52.997829 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m18:32:52.998227 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:52.998567 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m18:32:53.002099 [debug] [Thread-6 (]: SQL status: OK in 0.003 seconds
[0m18:32:53.004187 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m18:32:53.004623 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m18:32:53.005238 [debug] [Thread-6 (]: SQL status: OK in 0.000 seconds
[0m18:32:53.006405 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m18:32:53.080724 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8468ffe2-a80c-4af0-9055-266f21963050', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55f5ddd690>]}
[0m18:32:53.081522 [info ] [Thread-6 (]: 6 of 14 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 11.81s]
[0m18:32:53.082104 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m18:32:53.086766 [debug] [MainThread]: Using duckdb connection "master"
[0m18:32:53.087243 [debug] [MainThread]: On master: BEGIN
[0m18:32:53.087624 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:32:53.094947 [debug] [MainThread]: SQL status: OK in 0.007 seconds
[0m18:32:53.095477 [debug] [MainThread]: On master: COMMIT
[0m18:32:53.095800 [debug] [MainThread]: Using duckdb connection "master"
[0m18:32:53.096065 [debug] [MainThread]: On master: COMMIT
[0m18:32:53.096549 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m18:32:53.096829 [debug] [MainThread]: On master: Close
[0m18:32:53.100495 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:32:53.102564 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m18:32:53.103463 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m18:32:53.103753 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m18:32:53.103980 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m18:32:53.104213 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m18:32:53.104513 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m18:32:53.104784 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m18:32:53.105031 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m18:32:53.105258 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m18:32:53.105488 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m18:32:53.105720 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m18:32:53.105942 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m18:32:53.106197 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m18:32:53.106401 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m18:32:53.107005 [info ] [MainThread]: 
[0m18:32:53.107533 [info ] [MainThread]: Finished running 14 table models in 0 hours 0 minutes and 12.01 seconds (12.01s).
[0m18:32:53.109125 [debug] [MainThread]: Command end result
[0m18:32:53.138633 [info ] [MainThread]: 
[0m18:32:53.139378 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:32:53.139907 [info ] [MainThread]: 
[0m18:32:53.140253 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m18:32:53.141129 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.7366905, "process_user_time": 160.08522, "process_kernel_time": 8.056629, "process_mem_max_rss": "740248", "process_in_blocks": "143320", "process_out_blocks": "21016"}
[0m18:32:53.141693 [debug] [MainThread]: Command `dbt run` succeeded at 18:32:53.141595 after 12.74 seconds
[0m18:32:53.142071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f78db3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56f675be50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56ea97f8b0>]}
[0m18:32:53.142422 [debug] [MainThread]: Flushing usage events
[0m19:37:12.746454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ec7e67370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ec79d6bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ec66c6b00>]}


============================== 19:37:12.751478 | b2af4051-6ea8-440e-a3cc-c05eae6477fd ==============================
[0m19:37:12.751478 [info ] [MainThread]: Running with dbt=1.8.7
[0m19:37:12.752040 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m19:37:12.764895 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'LAKE_PATH'
[0m19:37:12.767336 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.07302397, "process_user_time": 1.043635, "process_kernel_time": 0.501747, "process_mem_max_rss": "91228", "process_in_blocks": "5624", "process_out_blocks": "8", "command_success": false}
[0m19:37:12.768159 [debug] [MainThread]: Command `dbt run` failed at 19:37:12.767960 after 0.07 seconds
[0m19:37:12.768780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ec7e67370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ec6562dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ec65634f0>]}
[0m19:37:12.769610 [debug] [MainThread]: Flushing usage events
[0m19:38:17.886890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc295573a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc27c0be80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc27c0a230>]}


============================== 19:38:17.889122 | b544f260-6f4b-4d3a-93e4-94e364045204 ==============================
[0m19:38:17.889122 [info ] [MainThread]: Running with dbt=1.8.7
[0m19:38:17.889564 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:38:18.126411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc27bb6b60>]}
[0m19:38:18.195598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc27b81990>]}
[0m19:38:18.200581 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m19:38:18.213123 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m19:38:18.321019 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m19:38:18.321781 [debug] [MainThread]: Partial parsing: added file: mta://models/fare_class_per_station.sql
[0m19:38:18.322266 [debug] [MainThread]: Partial parsing: updated file: mta://models/subway_station_stats.sql
[0m19:38:18.554613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc1c6b04f0>]}
[0m19:38:18.628282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc288adf90>]}
[0m19:38:18.628855 [info ] [MainThread]: Found 15 models, 9 sources, 416 macros
[0m19:38:18.629300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc1c89b4f0>]}
[0m19:38:18.632632 [info ] [MainThread]: 
[0m19:38:18.633632 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:38:18.641082 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m19:38:18.765541 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m19:38:18.766054 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m19:38:18.766381 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:18.787437 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m19:38:18.788814 [debug] [ThreadPool]: On list_mtastats: Close
[0m19:38:18.793373 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m19:38:18.794323 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m19:38:18.801224 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:38:18.801649 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m19:38:18.801946 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:38:18.813405 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m19:38:18.815191 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:38:18.815737 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m19:38:18.816758 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m19:38:18.817185 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:38:18.817560 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m19:38:18.818661 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m19:38:18.819395 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m19:38:18.819636 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:38:18.819866 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m19:38:18.820308 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:38:18.820623 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m19:38:18.826398 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m19:38:18.832245 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m19:38:18.832629 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m19:38:18.832914 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:38:18.840857 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m19:38:18.841499 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m19:38:18.841871 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m19:38:18.864848 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m19:38:18.866882 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m19:38:18.868864 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m19:38:18.869177 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m19:38:18.873558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc29842c50>]}
[0m19:38:18.876555 [debug] [MainThread]: Using duckdb connection "master"
[0m19:38:18.877817 [debug] [MainThread]: On master: BEGIN
[0m19:38:18.878262 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:38:18.886672 [debug] [MainThread]: SQL status: OK in 0.008 seconds
[0m19:38:18.887171 [debug] [MainThread]: On master: COMMIT
[0m19:38:18.887637 [debug] [MainThread]: Using duckdb connection "master"
[0m19:38:18.887968 [debug] [MainThread]: On master: COMMIT
[0m19:38:18.888766 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m19:38:18.889094 [debug] [MainThread]: On master: Close
[0m19:38:18.891572 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m19:38:18.892512 [info ] [MainThread]: 
[0m19:38:18.901289 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m19:38:18.901977 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m19:38:18.902515 [info ] [Thread-1 (]: 1 of 15 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m19:38:18.903150 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m19:38:18.903633 [info ] [Thread-2 (]: 2 of 15 START sql table model main.bond_payment_info ........................... [RUN]
[0m19:38:18.904209 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m19:38:18.904866 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m19:38:18.905360 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m19:38:18.906462 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m19:38:18.906053 [info ] [Thread-3 (]: 3 of 15 START sql table model main.busiest_specific_times ...................... [RUN]
[0m19:38:18.907566 [debug] [Thread-7 (]: Began running node model.mta.fare_class_per_station
[0m19:38:18.908342 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m19:38:18.909618 [debug] [Thread-8 (]: Began running node model.mta.forecast_accuracy_2023
[0m19:38:18.910473 [debug] [Thread-9 (]: Began running node model.mta.labor_expenses_per_agency
[0m19:38:18.911579 [debug] [Thread-10 ]: Began running node model.mta.largest_expense_differences_2023
[0m19:38:18.912272 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_by_station
[0m19:38:18.911134 [info ] [Thread-4 (]: 4 of 15 START sql table model main.daily_ridership ............................. [RUN]
[0m19:38:18.913076 [debug] [Thread-12 ]: Began running node model.mta.omny_adoption_increase
[0m19:38:18.913528 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m19:38:18.914110 [debug] [Thread-13 ]: Began running node model.mta.subway_station_stats
[0m19:38:18.914656 [debug] [Thread-14 ]: Began running node model.mta.total_riders_per_station
[0m19:38:18.915290 [debug] [Thread-15 ]: Began running node model.mta.weekly_riders_per_station
[0m19:38:18.915790 [info ] [Thread-5 (]: 5 of 15 START sql table model main.expense_type_per_year ....................... [RUN]
[0m19:38:18.916594 [info ] [Thread-6 (]: 6 of 15 START sql table model main.fare_class_boro ............................. [RUN]
[0m19:38:18.917318 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m19:38:18.917911 [info ] [Thread-7 (]: 7 of 15 START sql table model main.fare_class_per_station ...................... [RUN]
[0m19:38:18.918501 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m19:38:18.919140 [info ] [Thread-8 (]: 8 of 15 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m19:38:18.919919 [info ] [Thread-9 (]: 9 of 15 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m19:38:18.920559 [info ] [Thread-10 ]: 10 of 15 START sql table model main.largest_expense_differences_2023 ........... [RUN]
[0m19:38:18.921286 [info ] [Thread-11 ]: 11 of 15 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m19:38:18.922130 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m19:38:18.922711 [info ] [Thread-12 ]: 12 of 15 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m19:38:18.933210 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m19:38:18.934277 [info ] [Thread-13 ]: 13 of 15 START sql table model main.subway_station_stats ....................... [RUN]
[0m19:38:18.935112 [info ] [Thread-14 ]: 14 of 15 START sql table model main.total_riders_per_station ................... [RUN]
[0m19:38:18.935774 [info ] [Thread-15 ]: 15 of 15 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m19:38:18.936398 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m19:38:18.936999 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m19:38:18.937513 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m19:38:18.938108 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.fare_class_per_station'
[0m19:38:18.944393 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m19:38:18.945324 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m19:38:18.946107 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m19:38:18.946899 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m19:38:18.947727 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m19:38:18.948271 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m19:38:18.948933 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m19:38:18.949786 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m19:38:18.950693 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m19:38:18.951301 [debug] [Thread-15 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m19:38:18.952124 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m19:38:18.952596 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m19:38:18.953127 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m19:38:18.956294 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m19:38:18.957116 [debug] [Thread-7 (]: Began compiling node model.mta.fare_class_per_station
[0m19:38:18.958189 [debug] [Thread-8 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m19:38:18.958762 [debug] [Thread-9 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m19:38:18.959903 [debug] [Thread-10 ]: Began compiling node model.mta.largest_expense_differences_2023
[0m19:38:18.960608 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m19:38:18.961148 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_by_station
[0m19:38:18.965840 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m19:38:18.966594 [debug] [Thread-12 ]: Began compiling node model.mta.omny_adoption_increase
[0m19:38:18.967188 [debug] [Thread-13 ]: Began compiling node model.mta.subway_station_stats
[0m19:38:18.967662 [debug] [Thread-14 ]: Began compiling node model.mta.total_riders_per_station
[0m19:38:18.968129 [debug] [Thread-15 ]: Began compiling node model.mta.weekly_riders_per_station
[0m19:38:18.971778 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m19:38:18.975004 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m19:38:18.996531 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.fare_class_per_station"
[0m19:38:19.000106 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m19:38:19.008178 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m19:38:19.017898 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m19:38:19.013720 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m19:38:19.021186 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m19:38:19.031145 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m19:38:19.031815 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m19:38:19.036039 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m19:38:19.039595 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m19:38:19.043780 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m19:38:19.044612 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m19:38:19.048484 [debug] [Thread-15 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m19:38:19.053042 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m19:38:19.053969 [debug] [Thread-7 (]: Began executing node model.mta.fare_class_per_station
[0m19:38:19.054469 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m19:38:19.055432 [debug] [Thread-8 (]: Began executing node model.mta.forecast_accuracy_2023
[0m19:38:19.056290 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m19:38:19.057094 [debug] [Thread-9 (]: Began executing node model.mta.labor_expenses_per_agency
[0m19:38:19.058605 [debug] [Thread-10 ]: Began executing node model.mta.largest_expense_differences_2023
[0m19:38:19.060879 [debug] [Thread-12 ]: Began executing node model.mta.omny_adoption_increase
[0m19:38:19.061657 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:19.062311 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:38:19.063701 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_by_station
[0m19:38:19.064229 [debug] [Thread-14 ]: Began executing node model.mta.total_riders_per_station
[0m19:38:19.067813 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m19:38:19.068346 [debug] [Thread-13 ]: Began executing node model.mta.subway_station_stats
[0m19:38:19.071484 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.fare_class_per_station"
[0m19:38:19.076202 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m19:38:19.080292 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m19:38:19.081074 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:19.081606 [debug] [Thread-15 ]: Began executing node model.mta.weekly_riders_per_station
[0m19:38:19.084510 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m19:38:19.087131 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m19:38:19.090906 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m19:38:19.095259 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m19:38:19.096090 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m19:38:19.096779 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m19:38:19.100151 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m19:38:19.102493 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m19:38:19.105146 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m19:38:19.106559 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:19.107856 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:38:19.108825 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m19:38:19.109843 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:38:19.110785 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:19.116056 [debug] [Thread-15 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m19:38:19.117571 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:19.118733 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:19.119354 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:19.119914 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:38:19.120394 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:38:19.120874 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:38:19.122370 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:38:19.122974 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:38:19.123585 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:38:19.124053 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m19:38:19.124649 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m19:38:19.125345 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:38:19.125879 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: BEGIN
[0m19:38:19.126944 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m19:38:19.127748 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m19:38:19.128340 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: BEGIN
[0m19:38:19.128949 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m19:38:19.129626 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:38:19.130053 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: BEGIN
[0m19:38:19.137139 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: BEGIN
[0m19:38:19.137878 [debug] [Thread-13 ]: On model.mta.subway_station_stats: BEGIN
[0m19:38:19.138665 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m19:38:19.139693 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:38:19.140454 [debug] [Thread-2 (]: SQL status: OK in 0.020 seconds
[0m19:38:19.141147 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:38:19.143525 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m19:38:19.144900 [debug] [Thread-1 (]: SQL status: OK in 0.024 seconds
[0m19:38:19.145524 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m19:38:19.146068 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m19:38:19.146726 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m19:38:19.147290 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m19:38:19.147977 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m19:38:19.148793 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m19:38:19.149393 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m19:38:19.149755 [debug] [Thread-3 (]: SQL status: OK in 0.024 seconds
[0m19:38:19.150231 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m19:38:19.150746 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m19:38:19.151542 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:19.152577 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:38:19.154017 [debug] [Thread-15 ]: Opening a new connection, currently in state init
[0m19:38:19.155695 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:19.156944 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m19:38:19.158145 [debug] [Thread-4 (]: SQL status: OK in 0.018 seconds
[0m19:38:19.159119 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m19:38:19.160578 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m19:38:19.166198 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:19.167818 [debug] [Thread-6 (]: SQL status: OK in 0.027 seconds
[0m19:38:19.169164 [debug] [Thread-7 (]: SQL status: OK in 0.026 seconds
[0m19:38:19.169779 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m19:38:19.170424 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:38:19.171157 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:38:19.172154 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:38:19.173172 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_per_station__dbt_tmp"
  
    as (
      WITH total_ridership_per_station AS (
    -- Calculate total ridership for each station_complex
    SELECT 
        station_complex, 
        SUM(ridership) AS total_ridership_station
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category and station_complex
    SELECT 
        station_complex, 
        fare_class_category, 
        SUM(ridership) AS total_ridership_fare_class
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        fare_class_category IN (
            'MetroCard - Unlimited 30-Day', 
            'MetroCard - Fair Fare', 
            'OMNY - Full Fare', 
            'OMNY - Fair Fare', 
            'MetroCard - Full Fare', 
            'MetroCard - Other', 
            'MetroCard - Unlimited 7-Day', 
            'OMNY - Seniors & Disability', 
            'OMNY - Students', 
            'OMNY - Other', 
            'MetroCard - Seniors & Disability', 
            'MetroCard - Students'
        )
    GROUP BY 
        station_complex, fare_class_category
),
pivoted_ridership AS (
    -- Pivot the fare class category percentages into columns as decimals
    SELECT
        r.station_complex,
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 30-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 30-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 7-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 7-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Students",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Students"
    FROM 
        ridership_by_fare_class r
    JOIN 
        total_ridership_per_station t
    ON 
        r.station_complex = t.station_complex
    GROUP BY 
        r.station_complex, t.total_ridership_station
)
SELECT * 
FROM pivoted_ridership
ORDER BY station_complex;
    );
  
  
[0m19:38:19.174323 [debug] [Thread-8 (]: SQL status: OK in 0.029 seconds
[0m19:38:19.186542 [debug] [Thread-7 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_per_station__dbt_tmp"
  
    as (
      WITH total_ridership_per_station AS (
    -- Calculate total ridership for each station_complex
    SELECT 
        station_complex, 
        SUM(ridership) AS total_ridership_station
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category and station_complex
    SELECT 
        station_complex, 
        fare_class_category, 
        SUM(ridership) AS total_ridership_fare_class
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        fare_class_category IN (
            'MetroCard - Unlimited 30-Day', 
            'MetroCard - Fair Fare', 
            'OMNY - Full Fare', 
            'OMNY - Fair Fare', 
            'MetroCard - Full Fare', 
            'MetroCard - Other', 
            'MetroCard - Unlimited 7-Day', 
            'OMNY - Seniors & Disability', 
            'OMNY - Students', 
            'OMNY - Other', 
            'MetroCard - Seniors & Disability', 
            'MetroCard - Students'
        )
    GROUP BY 
        station_complex, fare_class_category
),
pivoted_ridership AS (
    -- Pivot the fare class category percentages into columns as decimals
    SELECT
        r.station_complex,
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 30-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 30-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 7-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 7-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Students",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Students"
    FROM 
        ridership_by_fare_class r
    JOIN 
        total_ridership_per_station t
    ON 
        r.station_complex = t.station_complex
    GROUP BY 
        r.station_complex, t.total_ridership_station
)
SELECT * 
FROM pivoted_ridership
ORDER BY station_complex;
    );
  
  
[0m19:38:19.187743 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:19.188631 [debug] [Thread-7 (]: DuckDB adapter: Rolling back transaction.
[0m19:38:19.189782 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m19:38:19.192757 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: ROLLBACK
[0m19:38:19.193325 [debug] [Thread-5 (]: SQL status: OK in 0.047 seconds
[0m19:38:19.198421 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:19.200848 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m19:38:19.211290 [debug] [Thread-10 ]: SQL status: OK in 0.061 seconds
[0m19:38:19.216083 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:19.219096 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m19:38:19.240952 [debug] [Thread-9 (]: SQL status: OK in 0.093 seconds
[0m19:38:19.242512 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:19.244632 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m19:38:19.254447 [debug] [Thread-12 ]: SQL status: OK in 0.104 seconds
[0m19:38:19.258760 [debug] [Thread-7 (]: Failed to rollback 'model.mta.fare_class_per_station'
[0m19:38:19.261849 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:38:19.263717 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m19:38:19.262812 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: Close
[0m19:38:19.266082 [debug] [Thread-14 ]: SQL status: OK in 0.117 seconds
[0m19:38:19.270996 [debug] [Thread-13 ]: SQL status: OK in 0.121 seconds
[0m19:38:19.271722 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:38:19.272933 [debug] [Thread-7 (]: Runtime Error in model fare_class_per_station (models/fare_class_per_station.sql)
  Parser Error: syntax error at or near ";"
[0m19:38:19.274185 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:38:19.275306 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:38:19.284931 [debug] [Thread-13 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    -- Calculate the weekend transfer percentage change compared to the weekday
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m19:38:19.284257 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc28892890>]}
[0m19:38:19.298040 [error] [Thread-7 (]: 7 of 15 ERROR creating sql table model main.fare_class_per_station ............. [[31mERROR[0m in 0.34s]
[0m19:38:19.299202 [debug] [Thread-7 (]: Finished running node model.mta.fare_class_per_station
[0m19:38:19.300136 [debug] [Thread-11 ]: SQL status: OK in 0.149 seconds
[0m19:38:19.304651 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:38:19.305411 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m19:38:19.307955 [debug] [Thread-15 ]: SQL status: OK in 0.154 seconds
[0m19:38:19.309002 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:38:19.309470 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start
    );
  
  
[0m19:38:19.321119 [debug] [Thread-13 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    -- Calculate the weekend transfer percentage change compared to the weekday
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m19:38:19.322173 [debug] [Thread-13 ]: DuckDB adapter: Rolling back transaction.
[0m19:38:19.322949 [debug] [Thread-13 ]: On model.mta.subway_station_stats: ROLLBACK
[0m19:38:19.324716 [debug] [Thread-13 ]: Failed to rollback 'model.mta.subway_station_stats'
[0m19:38:19.325329 [debug] [Thread-13 ]: On model.mta.subway_station_stats: Close
[0m19:38:19.327348 [debug] [Thread-13 ]: Runtime Error in model subway_station_stats (models/subway_station_stats.sql)
  Binder Error: Referenced column "transit_timestamp" not found in FROM clause!
  Candidate bindings: "mta_operations_statement.timestamp"
  LINE 22:         YEAR(transit_timestamp) = 2024
                        ^
[0m19:38:19.328930 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc185b6770>]}
[0m19:38:19.330842 [error] [Thread-13 ]: 13 of 15 ERROR creating sql table model main.subway_station_stats .............. [[31mERROR[0m in 0.38s]
[0m19:38:19.332157 [debug] [Thread-13 ]: Finished running node model.mta.subway_station_stats
[0m19:38:19.578790 [debug] [Thread-4 (]: SQL status: OK in 0.407 seconds
[0m19:38:19.594983 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:19.596252 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m19:38:19.598640 [debug] [Thread-5 (]: SQL status: OK in 0.396 seconds
[0m19:38:19.608890 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:19.609873 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m19:38:19.613268 [debug] [Thread-5 (]: SQL status: OK in 0.003 seconds
[0m19:38:19.619116 [debug] [Thread-4 (]: SQL status: OK in 0.022 seconds
[0m19:38:19.707498 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m19:38:19.709035 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m19:38:19.710384 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:19.711357 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:19.712019 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m19:38:19.712540 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m19:38:19.723893 [debug] [Thread-4 (]: SQL status: OK in 0.010 seconds
[0m19:38:19.732786 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:19.733875 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m19:38:19.735832 [debug] [Thread-10 ]: SQL status: OK in 0.516 seconds
[0m19:38:19.736712 [debug] [Thread-5 (]: SQL status: OK in 0.023 seconds
[0m19:38:19.754972 [debug] [Thread-9 (]: SQL status: OK in 0.509 seconds
[0m19:38:19.766953 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:19.769555 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:19.771685 [debug] [Thread-4 (]: SQL status: OK in 0.036 seconds
[0m19:38:19.783639 [debug] [Thread-8 (]: SQL status: OK in 0.588 seconds
[0m19:38:19.800290 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:19.845490 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m19:38:19.808635 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m19:38:19.814108 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m19:38:19.807386 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m19:38:19.855055 [debug] [Thread-5 (]: SQL status: OK in 0.001 seconds
[0m19:38:19.864731 [debug] [Thread-9 (]: SQL status: OK in 0.016 seconds
[0m19:38:19.869675 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc1c765e10>]}
[0m19:38:19.876212 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:19.897966 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m19:38:19.884025 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m19:38:19.888081 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m19:38:19.915917 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:19.909908 [debug] [Thread-8 (]: SQL status: OK in 0.010 seconds
[0m19:38:19.912609 [info ] [Thread-4 (]: 4 of 15 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 0.95s]
[0m19:38:19.881394 [debug] [Thread-10 ]: SQL status: OK in 0.006 seconds
[0m19:38:19.921376 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m19:38:19.926708 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m19:38:19.928274 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m19:38:19.943605 [debug] [Thread-2 (]: SQL status: OK in 0.782 seconds
[0m19:38:19.944906 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc1c764a30>]}
[0m19:38:19.985723 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:19.957382 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:19.966225 [debug] [Thread-9 (]: SQL status: OK in 0.014 seconds
[0m19:38:19.950938 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: COMMIT
[0m19:38:20.022261 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:19.996699 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m19:38:19.998835 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m19:38:19.993383 [info ] [Thread-5 (]: 5 of 15 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 1.01s]
[0m19:38:20.056618 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m19:38:20.031079 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: COMMIT
[0m19:38:20.055049 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:20.044568 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m19:38:20.069069 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m19:38:20.073166 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m19:38:20.079445 [debug] [Thread-8 (]: SQL status: OK in 0.032 seconds
[0m19:38:20.081431 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:20.096108 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m19:38:20.097812 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:20.098715 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m19:38:20.115452 [debug] [Thread-8 (]: SQL status: OK in 0.001 seconds
[0m19:38:20.119461 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: Close
[0m19:38:20.101285 [debug] [Thread-9 (]: SQL status: OK in 0.025 seconds
[0m19:38:20.127277 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: Close
[0m19:38:20.123955 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc1c767580>]}
[0m19:38:20.102192 [debug] [Thread-10 ]: SQL status: OK in 0.040 seconds
[0m19:38:20.133071 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc248b3580>]}
[0m19:38:20.142966 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:20.145654 [debug] [Thread-2 (]: SQL status: OK in 0.048 seconds
[0m19:38:20.153091 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:20.167850 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m19:38:20.144306 [info ] [Thread-9 (]: 9 of 15 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 1.19s]
[0m19:38:20.170453 [debug] [Thread-9 (]: Finished running node model.mta.labor_expenses_per_agency
[0m19:38:20.147349 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m19:38:20.169360 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m19:38:20.136634 [info ] [Thread-8 (]: 8 of 15 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 1.18s]
[0m19:38:20.173840 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m19:38:20.175118 [debug] [Thread-8 (]: Finished running node model.mta.forecast_accuracy_2023
[0m19:38:20.176004 [debug] [Thread-10 ]: SQL status: OK in 0.005 seconds
[0m19:38:20.178940 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: Close
[0m19:38:20.181523 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc248b0370>]}
[0m19:38:20.182820 [info ] [Thread-10 ]: 10 of 15 OK created sql table model main.largest_expense_differences_2023 ...... [[32mOK[0m in 1.23s]
[0m19:38:20.180099 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcb88903a90>]}
[0m19:38:20.186927 [info ] [Thread-2 (]: 2 of 15 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 1.27s]
[0m19:38:20.183847 [debug] [Thread-10 ]: Finished running node model.mta.largest_expense_differences_2023
[0m19:38:20.191029 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m19:38:20.196746 [debug] [Thread-3 (]: SQL status: OK in 1.027 seconds
[0m19:38:20.209385 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:20.210262 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m19:38:20.211840 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m19:38:20.216089 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m19:38:20.219495 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:20.222172 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m19:38:20.232105 [debug] [Thread-3 (]: SQL status: OK in 0.007 seconds
[0m19:38:20.249646 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:20.250693 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m19:38:20.252983 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m19:38:20.256430 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m19:38:20.259564 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc1c7652a0>]}
[0m19:38:20.261061 [info ] [Thread-3 (]: 3 of 15 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 1.34s]
[0m19:38:20.262441 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m19:38:22.300283 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.avg_riders_per_day. Details: Connection(type='duckdb', name='model.mta.avg_riders_per_day', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fcc199ba0e0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m19:38:22.301150 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.avg_riders_per_day
[0m19:38:22.301840 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.fare_class_boro. Details: Connection(type='duckdb', name='model.mta.fare_class_boro', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fcc1c7645e0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m19:38:22.302456 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.fare_class_boro
[0m19:38:22.303066 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.omny_adoption_by_station. Details: Connection(type='duckdb', name='model.mta.omny_adoption_by_station', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fcc185cbc70>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m19:38:22.303656 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.omny_adoption_by_station
[0m19:38:22.304906 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.omny_adoption_increase. Details: Connection(type='duckdb', name='model.mta.omny_adoption_increase', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fcc1c7787f0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m19:38:22.305667 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.omny_adoption_increase
[0m19:38:22.306426 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.total_riders_per_station. Details: Connection(type='duckdb', name='model.mta.total_riders_per_station', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fcc1c77ae60>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m19:38:22.317162 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.total_riders_per_station
[0m19:38:22.318204 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.weekly_riders_per_station. Details: Connection(type='duckdb', name='model.mta.weekly_riders_per_station', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7fcc27b83be0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m19:38:22.318828 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.weekly_riders_per_station
[0m19:38:22.319416 [error] [MainThread]: CANCEL query model.mta.expense_type_per_year ................................... [[31mCANCEL[0m]
[0m19:38:22.321650 [debug] [Thread-12 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m19:38:22.322597 [debug] [Thread-6 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:38:22.323923 [debug] [Thread-11 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m19:38:22.323228 [error] [MainThread]: CANCEL query model.mta.avg_riders_per_day ...................................... [[31mCANCEL[0m]
[0m19:38:22.325029 [debug] [Thread-12 ]: DuckDB adapter: Rolling back transaction.
[0m19:38:22.325929 [debug] [Thread-6 (]: DuckDB adapter: Rolling back transaction.
[0m19:38:22.326513 [debug] [Thread-11 ]: DuckDB adapter: Rolling back transaction.
[0m19:38:22.327173 [debug] [Thread-14 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:38:22.328539 [debug] [Thread-15 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start
    );
  
  
[0m19:38:22.327694 [error] [MainThread]: CANCEL query model.mta.bond_payment_info ....................................... [[31mCANCEL[0m]
[0m19:38:22.329498 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: ROLLBACK
[0m19:38:22.330083 [debug] [Thread-6 (]: On model.mta.fare_class_boro: ROLLBACK
[0m19:38:22.330634 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: ROLLBACK
[0m19:38:22.331071 [debug] [Thread-14 ]: DuckDB adapter: Rolling back transaction.
[0m19:38:22.331510 [debug] [Thread-15 ]: DuckDB adapter: Rolling back transaction.
[0m19:38:22.332024 [error] [MainThread]: CANCEL query model.mta.busiest_specific_times .................................. [[31mCANCEL[0m]
[0m19:38:22.333548 [debug] [Thread-6 (]: Failed to rollback 'model.mta.fare_class_boro'
[0m19:38:22.334395 [debug] [Thread-12 ]: Failed to rollback 'model.mta.omny_adoption_increase'
[0m19:38:22.335584 [debug] [Thread-11 ]: Failed to rollback 'model.mta.omny_adoption_by_station'
[0m19:38:22.336439 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: ROLLBACK
[0m19:38:22.337122 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: ROLLBACK
[0m19:38:22.337643 [error] [MainThread]: CANCEL query model.mta.daily_ridership ......................................... [[31mCANCEL[0m]
[0m19:38:22.338381 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m19:38:22.338888 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: Close
[0m19:38:22.339323 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: Close
[0m19:38:22.340192 [debug] [Thread-14 ]: Failed to rollback 'model.mta.total_riders_per_station'
[0m19:38:22.341089 [error] [MainThread]: CANCEL query model.mta.fare_class_boro ......................................... [[31mCANCEL[0m]
[0m19:38:22.344414 [debug] [Thread-6 (]: Runtime Error in model fare_class_boro (models/fare_class_boro.sql)
  INTERRUPT Error: Interrupted!
[0m19:38:22.345491 [debug] [Thread-15 ]: Failed to rollback 'model.mta.weekly_riders_per_station'
[0m19:38:22.347537 [debug] [Thread-12 ]: Runtime Error in model omny_adoption_increase (models/omny_adoption_increase.sql)
  INTERRUPT Error: Interrupted!
[0m19:38:22.348830 [debug] [Thread-11 ]: Runtime Error in model omny_adoption_by_station (models/omny_adoption_by_station.sql)
  INTERRUPT Error: Interrupted!
[0m19:38:22.349353 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: Close
[0m19:38:22.349904 [error] [MainThread]: CANCEL query model.mta.fare_class_per_station .................................. [[31mCANCEL[0m]
[0m19:38:22.350646 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc1c809b10>]}
[0m19:38:22.351345 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: Close
[0m19:38:22.351891 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc1c80bdf0>]}
[0m19:38:22.352487 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc185900d0>]}
[0m19:38:22.355697 [debug] [Thread-14 ]: Runtime Error in model total_riders_per_station (models/total_riders_per_station.sql)
  INTERRUPT Error: Interrupted!
[0m19:38:22.354977 [error] [MainThread]: CANCEL query model.mta.forecast_accuracy_2023 .................................. [[31mCANCEL[0m]
[0m19:38:22.356737 [error] [Thread-6 (]: 6 of 15 ERROR creating sql table model main.fare_class_boro .................... [[31mERROR[0m in 3.41s]
[0m19:38:22.359101 [error] [Thread-12 ]: 12 of 15 ERROR creating sql table model main.omny_adoption_increase ............ [[31mERROR[0m in 3.40s]
[0m19:38:22.360075 [error] [Thread-11 ]: 11 of 15 ERROR creating sql table model main.omny_adoption_by_station .......... [[31mERROR[0m in 3.40s]
[0m19:38:22.360998 [debug] [Thread-15 ]: Runtime Error in model weekly_riders_per_station (models/weekly_riders_per_station.sql)
  INTERRUPT Error: Interrupted!
[0m19:38:22.361579 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc1859f4f0>]}
[0m19:38:22.362277 [error] [MainThread]: CANCEL query model.mta.labor_expenses_per_agency ............................... [[31mCANCEL[0m]
[0m19:38:22.363149 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m19:38:22.363960 [debug] [Thread-12 ]: Finished running node model.mta.omny_adoption_increase
[0m19:38:22.364684 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_by_station
[0m19:38:22.365232 [debug] [Thread-15 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b544f260-6f4b-4d3a-93e4-94e364045204', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc185c9cc0>]}
[0m19:38:22.365972 [error] [Thread-14 ]: 14 of 15 ERROR creating sql table model main.total_riders_per_station .......... [[31mERROR[0m in 3.41s]
[0m19:38:22.366581 [error] [MainThread]: CANCEL query model.mta.largest_expense_differences_2023 ........................ [[31mCANCEL[0m]
[0m19:38:22.368299 [error] [Thread-15 ]: 15 of 15 ERROR creating sql table model main.weekly_riders_per_station ......... [[31mERROR[0m in 3.41s]
[0m19:38:22.369413 [debug] [Thread-14 ]: Finished running node model.mta.total_riders_per_station
[0m19:38:22.369810 [error] [MainThread]: CANCEL query model.mta.omny_adoption_by_station ................................ [[31mCANCEL[0m]
[0m19:38:22.370488 [debug] [Thread-15 ]: Finished running node model.mta.weekly_riders_per_station
[0m19:38:22.371092 [error] [MainThread]: CANCEL query model.mta.omny_adoption_increase .................................. [[31mCANCEL[0m]
[0m19:38:22.371979 [error] [MainThread]: CANCEL query model.mta.subway_station_stats .................................... [[31mCANCEL[0m]
[0m19:38:22.372343 [error] [MainThread]: CANCEL query model.mta.total_riders_per_station ................................ [[31mCANCEL[0m]
[0m19:38:22.372640 [error] [MainThread]: CANCEL query model.mta.weekly_riders_per_station ............................... [[31mCANCEL[0m]
[0m19:38:22.383369 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m19:38:22.383980 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m19:38:22.384579 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: ROLLBACK
[0m19:38:22.385268 [debug] [Thread-1 (]: Failed to rollback 'model.mta.avg_riders_per_day'
[0m19:38:22.385591 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m19:38:52.428683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4d031f460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4ce9d1e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4ce9d0c70>]}


============================== 19:38:52.430416 | 504f42ae-86c8-4d90-8903-b31d00dc6a2e ==============================
[0m19:38:52.430416 [info ] [MainThread]: Running with dbt=1.8.7
[0m19:38:52.430875 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:38:52.598424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4d03dce20>]}
[0m19:38:52.646808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4ce90ac20>]}
[0m19:38:52.649525 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m19:38:52.658456 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m19:38:52.733647 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:38:52.734047 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:38:52.758967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c356c130>]}
[0m19:38:52.824930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c352e4a0>]}
[0m19:38:52.825400 [info ] [MainThread]: Found 15 models, 9 sources, 416 macros
[0m19:38:52.825772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c352e1d0>]}
[0m19:38:52.827181 [info ] [MainThread]: 
[0m19:38:52.827762 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:38:52.832058 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m19:38:52.858820 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m19:38:52.859596 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m19:38:52.859951 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:52.868349 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m19:38:52.869518 [debug] [ThreadPool]: On list_mtastats: Close
[0m19:38:52.872128 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m19:38:52.872800 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m19:38:52.878941 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:38:52.879577 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m19:38:52.879843 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:38:52.885454 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m19:38:52.886582 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:38:52.886841 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m19:38:52.887319 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:38:52.887555 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:38:52.887792 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m19:38:52.888390 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:38:52.888949 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m19:38:52.889170 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:38:52.889387 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m19:38:52.889765 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:38:52.889984 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m19:38:52.893306 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m19:38:52.896809 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m19:38:52.897093 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m19:38:52.897361 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:38:52.902754 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m19:38:52.903052 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m19:38:52.903305 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m19:38:52.919501 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m19:38:52.920816 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m19:38:52.921285 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m19:38:52.921531 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m19:38:52.924491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4cea10b20>]}
[0m19:38:52.925010 [debug] [MainThread]: Using duckdb connection "master"
[0m19:38:52.925242 [debug] [MainThread]: On master: BEGIN
[0m19:38:52.925467 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:38:52.929955 [debug] [MainThread]: SQL status: OK in 0.004 seconds
[0m19:38:52.930214 [debug] [MainThread]: On master: COMMIT
[0m19:38:52.930486 [debug] [MainThread]: Using duckdb connection "master"
[0m19:38:52.930685 [debug] [MainThread]: On master: COMMIT
[0m19:38:52.931089 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m19:38:52.931375 [debug] [MainThread]: On master: Close
[0m19:38:52.932687 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m19:38:52.932997 [info ] [MainThread]: 
[0m19:38:52.940729 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m19:38:52.941735 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m19:38:52.943177 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m19:38:52.941348 [info ] [Thread-1 (]: 1 of 15 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m19:38:52.944167 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m19:38:52.944731 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m19:38:52.945625 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m19:38:52.943817 [info ] [Thread-2 (]: 2 of 15 START sql table model main.bond_payment_info ........................... [RUN]
[0m19:38:52.946378 [debug] [Thread-7 (]: Began running node model.mta.fare_class_per_station
[0m19:38:52.947009 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m19:38:52.945236 [info ] [Thread-3 (]: 3 of 15 START sql table model main.busiest_specific_times ...................... [RUN]
[0m19:38:52.947835 [debug] [Thread-8 (]: Began running node model.mta.forecast_accuracy_2023
[0m19:38:52.948422 [info ] [Thread-4 (]: 4 of 15 START sql table model main.daily_ridership ............................. [RUN]
[0m19:38:52.949052 [debug] [Thread-9 (]: Began running node model.mta.labor_expenses_per_agency
[0m19:38:52.949556 [info ] [Thread-5 (]: 5 of 15 START sql table model main.expense_type_per_year ....................... [RUN]
[0m19:38:52.950143 [debug] [Thread-10 ]: Began running node model.mta.largest_expense_differences_2023
[0m19:38:52.951300 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_by_station
[0m19:38:52.952124 [debug] [Thread-12 ]: Began running node model.mta.omny_adoption_increase
[0m19:38:52.950722 [info ] [Thread-6 (]: 6 of 15 START sql table model main.fare_class_boro ............................. [RUN]
[0m19:38:52.953286 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m19:38:52.953914 [debug] [Thread-13 ]: Began running node model.mta.subway_station_stats
[0m19:38:52.955586 [debug] [Thread-14 ]: Began running node model.mta.total_riders_per_station
[0m19:38:52.956252 [debug] [Thread-15 ]: Began running node model.mta.weekly_riders_per_station
[0m19:38:52.957025 [info ] [Thread-7 (]: 7 of 15 START sql table model main.fare_class_per_station ...................... [RUN]
[0m19:38:52.957806 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m19:38:52.958656 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m19:38:52.959953 [info ] [Thread-8 (]: 8 of 15 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m19:38:52.961409 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m19:38:52.962098 [info ] [Thread-9 (]: 9 of 15 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m19:38:52.962938 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m19:38:52.963657 [info ] [Thread-10 ]: 10 of 15 START sql table model main.largest_expense_differences_2023 ........... [RUN]
[0m19:38:52.964331 [info ] [Thread-11 ]: 11 of 15 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m19:38:52.965103 [info ] [Thread-12 ]: 12 of 15 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m19:38:52.965834 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m19:38:52.966346 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m19:38:52.967084 [info ] [Thread-13 ]: 13 of 15 START sql table model main.subway_station_stats ....................... [RUN]
[0m19:38:52.967857 [info ] [Thread-14 ]: 14 of 15 START sql table model main.total_riders_per_station ................... [RUN]
[0m19:38:52.968545 [info ] [Thread-15 ]: 15 of 15 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m19:38:52.969322 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.fare_class_per_station'
[0m19:38:52.976500 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m19:38:52.977318 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m19:38:52.978253 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m19:38:52.978892 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m19:38:52.979570 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m19:38:52.980322 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m19:38:52.981294 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m19:38:52.981958 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m19:38:52.982595 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m19:38:52.983141 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m19:38:52.986355 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m19:38:52.987330 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m19:38:52.988018 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m19:38:52.988666 [debug] [Thread-15 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m19:38:52.989316 [debug] [Thread-7 (]: Began compiling node model.mta.fare_class_per_station
[0m19:38:52.992178 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m19:38:52.992918 [debug] [Thread-8 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m19:38:52.993344 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m19:38:52.999928 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m19:38:53.001325 [debug] [Thread-9 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m19:38:53.050136 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m19:38:53.050688 [debug] [Thread-10 ]: Began compiling node model.mta.largest_expense_differences_2023
[0m19:38:53.051180 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_by_station
[0m19:38:53.051636 [debug] [Thread-12 ]: Began compiling node model.mta.omny_adoption_increase
[0m19:38:53.053880 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m19:38:53.054613 [debug] [Thread-13 ]: Began compiling node model.mta.subway_station_stats
[0m19:38:53.055170 [debug] [Thread-14 ]: Began compiling node model.mta.total_riders_per_station
[0m19:38:53.055521 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m19:38:53.056022 [debug] [Thread-15 ]: Began compiling node model.mta.weekly_riders_per_station
[0m19:38:53.058558 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.fare_class_per_station"
[0m19:38:53.061893 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m19:38:53.068184 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m19:38:53.086452 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m19:38:53.088265 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m19:38:53.091399 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m19:38:53.091835 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m19:38:53.093719 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m19:38:53.094113 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m19:38:53.095955 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m19:38:53.098299 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m19:38:53.100087 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m19:38:53.100473 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m19:38:53.102981 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m19:38:53.104893 [debug] [Thread-15 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m19:38:53.105713 [debug] [Thread-7 (]: Began executing node model.mta.fare_class_per_station
[0m19:38:53.107885 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m19:38:53.108285 [debug] [Thread-8 (]: Began executing node model.mta.forecast_accuracy_2023
[0m19:38:53.108857 [debug] [Thread-9 (]: Began executing node model.mta.labor_expenses_per_agency
[0m19:38:53.109699 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:38:53.112540 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m19:38:53.113025 [debug] [Thread-10 ]: Began executing node model.mta.largest_expense_differences_2023
[0m19:38:53.113594 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_by_station
[0m19:38:53.115697 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m19:38:53.116282 [debug] [Thread-12 ]: Began executing node model.mta.omny_adoption_increase
[0m19:38:53.118838 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m19:38:53.119291 [debug] [Thread-14 ]: Began executing node model.mta.total_riders_per_station
[0m19:38:53.119755 [debug] [Thread-13 ]: Began executing node model.mta.subway_station_stats
[0m19:38:53.122431 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.fare_class_per_station"
[0m19:38:53.122854 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:53.123261 [debug] [Thread-15 ]: Began executing node model.mta.weekly_riders_per_station
[0m19:38:53.125667 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m19:38:53.126175 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:53.128271 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m19:38:53.128726 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m19:38:53.129255 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:53.131736 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m19:38:53.133905 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m19:38:53.136261 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m19:38:53.136775 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:53.137367 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:38:53.139429 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m19:38:53.141500 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m19:38:53.141993 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:38:53.142357 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m19:38:53.144816 [debug] [Thread-15 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m19:38:53.145567 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:53.145897 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m19:38:53.146419 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:53.146816 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:38:53.147217 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m19:38:53.147749 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:53.148310 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:38:53.149556 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m19:38:53.150073 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:38:53.150577 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m19:38:53.151353 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: BEGIN
[0m19:38:53.151826 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:38:53.152221 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:38:53.152591 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:38:53.153285 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m19:38:53.153811 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:38:53.154219 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:38:53.154683 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m19:38:53.159756 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m19:38:53.160300 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: BEGIN
[0m19:38:53.160794 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m19:38:53.161411 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:38:53.161925 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: BEGIN
[0m19:38:53.162255 [debug] [Thread-1 (]: SQL status: OK in 0.015 seconds
[0m19:38:53.162669 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:38:53.163065 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m19:38:53.163463 [debug] [Thread-13 ]: On model.mta.subway_station_stats: BEGIN
[0m19:38:53.163832 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: BEGIN
[0m19:38:53.164501 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m19:38:53.164988 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m19:38:53.165625 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m19:38:53.166226 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m19:38:53.166651 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m19:38:53.167379 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m19:38:53.168338 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:38:53.169362 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m19:38:53.169895 [debug] [Thread-2 (]: SQL status: OK in 0.017 seconds
[0m19:38:53.170359 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m19:38:53.170988 [debug] [Thread-15 ]: Opening a new connection, currently in state init
[0m19:38:53.172247 [debug] [Thread-3 (]: SQL status: OK in 0.018 seconds
[0m19:38:53.173109 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m19:38:53.174317 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:53.175473 [debug] [Thread-5 (]: SQL status: OK in 0.016 seconds
[0m19:38:53.175819 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:53.176611 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m19:38:53.177139 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:53.177849 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m19:38:53.179153 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m19:38:53.180839 [debug] [Thread-4 (]: SQL status: OK in 0.019 seconds
[0m19:38:53.181938 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:53.182846 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m19:38:53.183853 [debug] [Thread-6 (]: SQL status: OK in 0.021 seconds
[0m19:38:53.184455 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:38:53.184882 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:38:53.186330 [debug] [Thread-7 (]: SQL status: OK in 0.023 seconds
[0m19:38:53.186793 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:38:53.187359 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_per_station__dbt_tmp"
  
    as (
      WITH total_ridership_per_station AS (
    -- Calculate total ridership for each station_complex
    SELECT 
        station_complex, 
        SUM(ridership) AS total_ridership_station
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category and station_complex
    SELECT 
        station_complex, 
        fare_class_category, 
        SUM(ridership) AS total_ridership_fare_class
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        fare_class_category IN (
            'MetroCard - Unlimited 30-Day', 
            'MetroCard - Fair Fare', 
            'OMNY - Full Fare', 
            'OMNY - Fair Fare', 
            'MetroCard - Full Fare', 
            'MetroCard - Other', 
            'MetroCard - Unlimited 7-Day', 
            'OMNY - Seniors & Disability', 
            'OMNY - Students', 
            'OMNY - Other', 
            'MetroCard - Seniors & Disability', 
            'MetroCard - Students'
        )
    GROUP BY 
        station_complex, fare_class_category
),
pivoted_ridership AS (
    -- Pivot the fare class category percentages into columns as decimals
    SELECT
        r.station_complex,
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 30-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 30-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 7-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 7-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Students",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Students"
    FROM 
        ridership_by_fare_class r
    JOIN 
        total_ridership_per_station t
    ON 
        r.station_complex = t.station_complex
    GROUP BY 
        r.station_complex, t.total_ridership_station
)
SELECT * 
FROM pivoted_ridership
ORDER BY station_complex;
    );
  
  
[0m19:38:53.192509 [debug] [Thread-7 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_per_station__dbt_tmp"
  
    as (
      WITH total_ridership_per_station AS (
    -- Calculate total ridership for each station_complex
    SELECT 
        station_complex, 
        SUM(ridership) AS total_ridership_station
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category and station_complex
    SELECT 
        station_complex, 
        fare_class_category, 
        SUM(ridership) AS total_ridership_fare_class
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        fare_class_category IN (
            'MetroCard - Unlimited 30-Day', 
            'MetroCard - Fair Fare', 
            'OMNY - Full Fare', 
            'OMNY - Fair Fare', 
            'MetroCard - Full Fare', 
            'MetroCard - Other', 
            'MetroCard - Unlimited 7-Day', 
            'OMNY - Seniors & Disability', 
            'OMNY - Students', 
            'OMNY - Other', 
            'MetroCard - Seniors & Disability', 
            'MetroCard - Students'
        )
    GROUP BY 
        station_complex, fare_class_category
),
pivoted_ridership AS (
    -- Pivot the fare class category percentages into columns as decimals
    SELECT
        r.station_complex,
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 30-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 30-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 7-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 7-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Students",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Students"
    FROM 
        ridership_by_fare_class r
    JOIN 
        total_ridership_per_station t
    ON 
        r.station_complex = t.station_complex
    GROUP BY 
        r.station_complex, t.total_ridership_station
)
SELECT * 
FROM pivoted_ridership
ORDER BY station_complex;
    );
  
  
[0m19:38:53.193199 [debug] [Thread-7 (]: DuckDB adapter: Rolling back transaction.
[0m19:38:53.193919 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: ROLLBACK
[0m19:38:53.210100 [debug] [Thread-8 (]: SQL status: OK in 0.046 seconds
[0m19:38:53.211341 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:53.212139 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m19:38:53.224899 [debug] [Thread-7 (]: Failed to rollback 'model.mta.fare_class_per_station'
[0m19:38:53.225830 [debug] [Thread-9 (]: SQL status: OK in 0.060 seconds
[0m19:38:53.226283 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: Close
[0m19:38:53.226710 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:53.227546 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m19:38:53.228567 [debug] [Thread-7 (]: Runtime Error in model fare_class_per_station (models/fare_class_per_station.sql)
  Parser Error: syntax error at or near ";"
[0m19:38:53.231417 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4cf65e830>]}
[0m19:38:53.232124 [error] [Thread-7 (]: 7 of 15 ERROR creating sql table model main.fare_class_per_station ............. [[31mERROR[0m in 0.26s]
[0m19:38:53.232907 [debug] [Thread-7 (]: Finished running node model.mta.fare_class_per_station
[0m19:38:53.236877 [debug] [Thread-10 ]: SQL status: OK in 0.071 seconds
[0m19:38:53.237312 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:53.237805 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m19:38:53.241389 [debug] [Thread-11 ]: SQL status: OK in 0.075 seconds
[0m19:38:53.241905 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:38:53.242362 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m19:38:53.245186 [debug] [Thread-12 ]: SQL status: OK in 0.078 seconds
[0m19:38:53.245578 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:38:53.245921 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m19:38:53.252848 [debug] [Thread-13 ]: SQL status: OK in 0.083 seconds
[0m19:38:53.253952 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:38:53.254830 [debug] [Thread-13 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    -- Calculate the weekend transfer percentage change compared to the weekday
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m19:38:53.263979 [debug] [Thread-14 ]: SQL status: OK in 0.093 seconds
[0m19:38:53.264640 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:38:53.265001 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:38:53.269579 [debug] [Thread-15 ]: SQL status: OK in 0.099 seconds
[0m19:38:53.270169 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:38:53.270661 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start
    );
  
  
[0m19:38:53.328059 [debug] [Thread-5 (]: SQL status: OK in 0.147 seconds
[0m19:38:53.337453 [debug] [Thread-13 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    -- Calculate the weekend transfer percentage change compared to the weekday
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m19:38:53.341965 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:53.342910 [debug] [Thread-13 ]: DuckDB adapter: Rolling back transaction.
[0m19:38:53.344764 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m19:38:53.345650 [debug] [Thread-13 ]: On model.mta.subway_station_stats: ROLLBACK
[0m19:38:53.347491 [debug] [Thread-13 ]: Failed to rollback 'model.mta.subway_station_stats'
[0m19:38:53.348145 [debug] [Thread-5 (]: SQL status: OK in 0.002 seconds
[0m19:38:53.349442 [debug] [Thread-13 ]: On model.mta.subway_station_stats: Close
[0m19:38:53.359888 [debug] [Thread-13 ]: Runtime Error in model subway_station_stats (models/subway_station_stats.sql)
  Binder Error: Referenced column "transit_timestamp" not found in FROM clause!
  Candidate bindings: "mta_operations_statement.timestamp"
  LINE 22:         YEAR(transit_timestamp) = 2024
                        ^
[0m19:38:53.423811 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m19:38:53.425723 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:53.424667 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c27c1000>]}
[0m19:38:53.426366 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m19:38:53.433348 [error] [Thread-13 ]: 13 of 15 ERROR creating sql table model main.subway_station_stats .............. [[31mERROR[0m in 0.44s]
[0m19:38:53.434335 [debug] [Thread-13 ]: Finished running node model.mta.subway_station_stats
[0m19:38:53.442730 [debug] [Thread-5 (]: SQL status: OK in 0.010 seconds
[0m19:38:53.457145 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:38:53.458300 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m19:38:53.459502 [debug] [Thread-5 (]: SQL status: OK in 0.000 seconds
[0m19:38:53.465512 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m19:38:53.466769 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c299add0>]}
[0m19:38:53.467509 [info ] [Thread-5 (]: 5 of 15 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 0.50s]
[0m19:38:53.468306 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m19:38:53.556421 [debug] [Thread-9 (]: SQL status: OK in 0.328 seconds
[0m19:38:53.577783 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:53.578870 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m19:38:53.581429 [debug] [Thread-9 (]: SQL status: OK in 0.002 seconds
[0m19:38:53.585540 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m19:38:53.586971 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:53.587637 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m19:38:53.613703 [debug] [Thread-9 (]: SQL status: OK in 0.025 seconds
[0m19:38:53.655480 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:38:53.656214 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m19:38:53.660816 [debug] [Thread-9 (]: SQL status: OK in 0.004 seconds
[0m19:38:53.667018 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: Close
[0m19:38:53.669680 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c2885c60>]}
[0m19:38:53.670613 [info ] [Thread-9 (]: 9 of 15 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 0.69s]
[0m19:38:53.671786 [debug] [Thread-9 (]: Finished running node model.mta.labor_expenses_per_agency
[0m19:38:53.731520 [debug] [Thread-10 ]: SQL status: OK in 0.493 seconds
[0m19:38:53.747030 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:53.747806 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m19:38:53.753005 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m19:38:53.787852 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: COMMIT
[0m19:38:53.800945 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:53.802896 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: COMMIT
[0m19:38:53.809725 [debug] [Thread-2 (]: SQL status: OK in 0.631 seconds
[0m19:38:53.823685 [debug] [Thread-4 (]: SQL status: OK in 0.640 seconds
[0m19:38:53.831067 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:53.836025 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m19:38:53.838535 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m19:38:53.841697 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m19:38:53.842483 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:53.843138 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m19:38:53.844734 [debug] [Thread-10 ]: SQL status: OK in 0.040 seconds
[0m19:38:53.863080 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:38:53.896034 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m19:38:53.897112 [debug] [Thread-10 ]: SQL status: OK in 0.000 seconds
[0m19:38:53.899287 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: Close
[0m19:38:53.900365 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe461dbe110>]}
[0m19:38:53.901903 [info ] [Thread-10 ]: 10 of 15 OK created sql table model main.largest_expense_differences_2023 ...... [[32mOK[0m in 0.92s]
[0m19:38:53.903008 [debug] [Thread-10 ]: Finished running node model.mta.largest_expense_differences_2023
[0m19:38:53.864865 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:53.903890 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m19:38:53.875831 [debug] [Thread-4 (]: SQL status: OK in 0.032 seconds
[0m19:38:53.905398 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m19:38:53.895087 [debug] [Thread-8 (]: SQL status: OK in 0.680 seconds
[0m19:38:53.947468 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:38:53.975890 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m19:38:53.964699 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:53.977341 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m19:38:53.978982 [debug] [Thread-8 (]: SQL status: OK in 0.001 seconds
[0m19:38:53.982941 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m19:38:53.984962 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:53.984021 [debug] [Thread-4 (]: SQL status: OK in 0.007 seconds
[0m19:38:53.952648 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m19:38:53.989649 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:53.990354 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m19:38:53.986285 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m19:38:54.002490 [debug] [Thread-2 (]: SQL status: OK in 0.011 seconds
[0m19:38:53.988783 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m19:38:54.006992 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:38:54.009648 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c299b790>]}
[0m19:38:54.031819 [info ] [Thread-4 (]: 4 of 15 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 1.05s]
[0m19:38:54.030799 [debug] [Thread-8 (]: SQL status: OK in 0.039 seconds
[0m19:38:54.029666 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m19:38:54.037184 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:38:54.038332 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m19:38:54.045469 [debug] [Thread-8 (]: SQL status: OK in 0.000 seconds
[0m19:38:54.032760 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m19:38:54.051793 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: Close
[0m19:38:54.062274 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c28858d0>]}
[0m19:38:54.060145 [debug] [Thread-2 (]: SQL status: OK in 0.021 seconds
[0m19:38:54.070548 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m19:38:54.072728 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c299beb0>]}
[0m19:38:54.096424 [info ] [Thread-2 (]: 2 of 15 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 1.12s]
[0m19:38:54.066353 [info ] [Thread-8 (]: 8 of 15 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 1.08s]
[0m19:38:54.097660 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m19:38:54.107747 [debug] [Thread-8 (]: Finished running node model.mta.forecast_accuracy_2023
[0m19:38:55.297391 [debug] [Thread-1 (]: SQL status: OK in 2.121 seconds
[0m19:38:55.340960 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:38:55.355292 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m19:38:55.357002 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m19:38:55.362010 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m19:38:55.376546 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:38:55.377581 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m19:38:55.413070 [debug] [Thread-1 (]: SQL status: OK in 0.018 seconds
[0m19:38:55.418071 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:38:55.425687 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m19:38:55.428538 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m19:38:55.431447 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m19:38:55.432597 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c299b040>]}
[0m19:38:55.433956 [info ] [Thread-1 (]: 1 of 15 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 2.49s]
[0m19:38:55.435080 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m19:38:57.574995 [debug] [Thread-3 (]: SQL status: OK in 4.395 seconds
[0m19:38:57.582699 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:57.583888 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m19:38:57.585704 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m19:38:57.588718 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m19:38:57.589634 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:57.605036 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m19:38:57.616590 [debug] [Thread-3 (]: SQL status: OK in 0.010 seconds
[0m19:38:57.623471 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:38:57.624308 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m19:38:57.625821 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m19:38:57.628238 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m19:38:57.645246 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c299ba60>]}
[0m19:38:57.646642 [info ] [Thread-3 (]: 3 of 15 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 4.69s]
[0m19:38:57.647900 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m19:38:58.665054 [debug] [Thread-15 ]: SQL status: OK in 5.394 seconds
[0m19:38:58.674047 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:38:58.675147 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m19:38:58.687144 [debug] [Thread-15 ]: SQL status: OK in 0.002 seconds
[0m19:38:58.715741 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m19:38:58.716632 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:38:58.717287 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m19:38:58.776708 [debug] [Thread-15 ]: SQL status: OK in 0.059 seconds
[0m19:38:58.782551 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:38:58.783508 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m19:38:58.786130 [debug] [Thread-15 ]: SQL status: OK in 0.001 seconds
[0m19:38:58.789789 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: Close
[0m19:38:58.805422 [debug] [Thread-15 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c28862c0>]}
[0m19:38:58.806670 [info ] [Thread-15 ]: 15 of 15 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 5.82s]
[0m19:38:58.807619 [debug] [Thread-15 ]: Finished running node model.mta.weekly_riders_per_station
[0m19:38:58.974833 [debug] [Thread-11 ]: SQL status: OK in 5.731 seconds
[0m19:38:58.981153 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:38:58.985209 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m19:38:58.986680 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m19:38:58.990031 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m19:38:58.993632 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:38:58.995716 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m19:38:59.004093 [debug] [Thread-11 ]: SQL status: OK in 0.007 seconds
[0m19:38:59.008494 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:38:59.009284 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m19:38:59.010341 [debug] [Thread-11 ]: SQL status: OK in 0.000 seconds
[0m19:38:59.012440 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: Close
[0m19:38:59.013425 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c2885fc0>]}
[0m19:38:59.014499 [info ] [Thread-11 ]: 11 of 15 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 6.03s]
[0m19:38:59.015731 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_by_station
[0m19:38:59.053187 [debug] [Thread-12 ]: SQL status: OK in 5.807 seconds
[0m19:38:59.079685 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:38:59.080603 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m19:38:59.082022 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m19:38:59.084907 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: COMMIT
[0m19:38:59.086184 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:38:59.086991 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: COMMIT
[0m19:38:59.108448 [debug] [Thread-12 ]: SQL status: OK in 0.021 seconds
[0m19:38:59.113126 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:38:59.114148 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m19:38:59.115424 [debug] [Thread-12 ]: SQL status: OK in 0.000 seconds
[0m19:38:59.117554 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: Close
[0m19:38:59.118426 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c2885f30>]}
[0m19:38:59.119361 [info ] [Thread-12 ]: 12 of 15 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 6.14s]
[0m19:38:59.120439 [debug] [Thread-12 ]: Finished running node model.mta.omny_adoption_increase
[0m19:39:00.960925 [debug] [Thread-6 (]: SQL status: OK in 7.775 seconds
[0m19:39:00.993963 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:39:00.995826 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m19:39:00.997471 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m19:39:01.000882 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m19:39:01.001967 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:39:01.004486 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m19:39:01.021471 [debug] [Thread-6 (]: SQL status: OK in 0.015 seconds
[0m19:39:01.027869 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:39:01.028937 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m19:39:01.030336 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m19:39:01.032849 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m19:39:01.057593 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe40fcebd30>]}
[0m19:39:01.059618 [info ] [Thread-6 (]: 6 of 15 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 8.09s]
[0m19:39:01.062917 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m19:39:01.116478 [debug] [Thread-14 ]: SQL status: OK in 7.851 seconds
[0m19:39:01.120053 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:39:01.120961 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m19:39:01.123117 [debug] [Thread-14 ]: SQL status: OK in 0.002 seconds
[0m19:39:01.124963 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: COMMIT
[0m19:39:01.125368 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:39:01.125680 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: COMMIT
[0m19:39:01.129820 [debug] [Thread-14 ]: SQL status: OK in 0.004 seconds
[0m19:39:01.132294 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:39:01.132676 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m19:39:01.133457 [debug] [Thread-14 ]: SQL status: OK in 0.000 seconds
[0m19:39:01.134745 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: Close
[0m19:39:01.233653 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '504f42ae-86c8-4d90-8903-b31d00dc6a2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe461d92cb0>]}
[0m19:39:01.234773 [info ] [Thread-14 ]: 14 of 15 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 8.25s]
[0m19:39:01.235734 [debug] [Thread-14 ]: Finished running node model.mta.total_riders_per_station
[0m19:39:01.242784 [debug] [MainThread]: Using duckdb connection "master"
[0m19:39:01.243553 [debug] [MainThread]: On master: BEGIN
[0m19:39:01.244106 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:39:01.257318 [debug] [MainThread]: SQL status: OK in 0.013 seconds
[0m19:39:01.258139 [debug] [MainThread]: On master: COMMIT
[0m19:39:01.258594 [debug] [MainThread]: Using duckdb connection "master"
[0m19:39:01.258968 [debug] [MainThread]: On master: COMMIT
[0m19:39:01.260927 [debug] [MainThread]: SQL status: OK in 0.002 seconds
[0m19:39:01.261470 [debug] [MainThread]: On master: Close
[0m19:39:01.264251 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:39:01.264746 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m19:39:01.264994 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m19:39:01.265229 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m19:39:01.265429 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m19:39:01.265625 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m19:39:01.265825 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m19:39:01.266006 [debug] [MainThread]: Connection 'model.mta.fare_class_per_station' was properly closed.
[0m19:39:01.266227 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m19:39:01.266452 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m19:39:01.266666 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m19:39:01.266866 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m19:39:01.267063 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m19:39:01.267267 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m19:39:01.267470 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m19:39:01.267667 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m19:39:01.268146 [info ] [MainThread]: 
[0m19:39:01.268644 [info ] [MainThread]: Finished running 15 table models in 0 hours 0 minutes and 8.44 seconds (8.44s).
[0m19:39:01.271640 [debug] [MainThread]: Command end result
[0m19:39:01.311059 [info ] [MainThread]: 
[0m19:39:01.311902 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m19:39:01.312328 [info ] [MainThread]: 
[0m19:39:01.312866 [error] [MainThread]:   Runtime Error in model fare_class_per_station (models/fare_class_per_station.sql)
  Parser Error: syntax error at or near ";"
[0m19:39:01.313252 [info ] [MainThread]: 
[0m19:39:01.313720 [error] [MainThread]:   Runtime Error in model subway_station_stats (models/subway_station_stats.sql)
  Binder Error: Referenced column "transit_timestamp" not found in FROM clause!
  Candidate bindings: "mta_operations_statement.timestamp"
  LINE 22:         YEAR(transit_timestamp) = 2024
                        ^
[0m19:39:01.314127 [info ] [MainThread]: 
[0m19:39:01.314674 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=2 SKIP=0 TOTAL=15
[0m19:39:01.315673 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 8.928014, "process_user_time": 115.3991, "process_kernel_time": 7.169322, "process_mem_max_rss": "502328", "process_in_blocks": "19088", "process_out_blocks": "19632", "command_success": false}
[0m19:39:01.316336 [debug] [MainThread]: Command `dbt run` failed at 19:39:01.316210 after 8.93 seconds
[0m19:39:01.316728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4d031f460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4c3550100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4ce9d2e30>]}
[0m19:39:01.317124 [debug] [MainThread]: Flushing usage events
[0m19:40:07.335807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c851f3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c6d7fe80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c6d7e230>]}


============================== 19:40:07.337612 | 87da0255-c022-474e-8c61-92665da0f09d ==============================
[0m19:40:07.337612 [info ] [MainThread]: Running with dbt=1.8.7
[0m19:40:07.338120 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m19:40:07.508179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bbb7eb60>]}
[0m19:40:07.569691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bbb49990>]}
[0m19:40:07.572281 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m19:40:07.579068 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m19:40:07.658757 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:40:07.659409 [debug] [MainThread]: Partial parsing: updated file: mta://models/fare_class_per_station.sql
[0m19:40:07.861640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bb6c4520>]}
[0m19:40:07.928822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bb8b9d50>]}
[0m19:40:07.929317 [info ] [MainThread]: Found 15 models, 9 sources, 416 macros
[0m19:40:07.929677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bb8bb280>]}
[0m19:40:07.931355 [info ] [MainThread]: 
[0m19:40:07.931929 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:40:07.940387 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m19:40:08.013158 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m19:40:08.013555 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m19:40:08.013855 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:40:08.025602 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m19:40:08.026676 [debug] [ThreadPool]: On list_mtastats: Close
[0m19:40:08.029185 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m19:40:08.029596 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m19:40:08.034672 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:40:08.035001 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m19:40:08.035262 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:40:08.040137 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m19:40:08.040997 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:40:08.041322 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m19:40:08.041782 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:40:08.042014 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:40:08.042264 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m19:40:08.042648 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:40:08.043186 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m19:40:08.043388 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:40:08.043645 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m19:40:08.044016 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:40:08.044333 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m19:40:08.047681 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m19:40:08.051164 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m19:40:08.051454 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m19:40:08.051725 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:40:08.056849 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m19:40:08.057159 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m19:40:08.057414 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m19:40:08.073979 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m19:40:08.075837 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m19:40:08.076304 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m19:40:08.076547 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m19:40:08.079703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c880ac50>]}
[0m19:40:08.080188 [debug] [MainThread]: Using duckdb connection "master"
[0m19:40:08.080505 [debug] [MainThread]: On master: BEGIN
[0m19:40:08.080804 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:40:08.088113 [debug] [MainThread]: SQL status: OK in 0.007 seconds
[0m19:40:08.088422 [debug] [MainThread]: On master: COMMIT
[0m19:40:08.088665 [debug] [MainThread]: Using duckdb connection "master"
[0m19:40:08.088864 [debug] [MainThread]: On master: COMMIT
[0m19:40:08.089227 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m19:40:08.089459 [debug] [MainThread]: On master: Close
[0m19:40:08.090947 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m19:40:08.091413 [info ] [MainThread]: 
[0m19:40:08.101410 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m19:40:08.102172 [info ] [Thread-1 (]: 1 of 15 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m19:40:08.102879 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m19:40:08.103198 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m19:40:08.103646 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m19:40:08.109738 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m19:40:08.115336 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m19:40:08.116448 [info ] [Thread-2 (]: 2 of 15 START sql table model main.bond_payment_info ........................... [RUN]
[0m19:40:08.117181 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m19:40:08.117659 [info ] [Thread-3 (]: 3 of 15 START sql table model main.busiest_specific_times ...................... [RUN]
[0m19:40:08.118243 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m19:40:08.119100 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m19:40:08.119711 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m19:40:08.120155 [debug] [Thread-7 (]: Began running node model.mta.fare_class_per_station
[0m19:40:08.121442 [debug] [Thread-8 (]: Began running node model.mta.forecast_accuracy_2023
[0m19:40:08.120952 [info ] [Thread-4 (]: 4 of 15 START sql table model main.daily_ridership ............................. [RUN]
[0m19:40:08.122221 [debug] [Thread-9 (]: Began running node model.mta.labor_expenses_per_agency
[0m19:40:08.123704 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m19:40:08.125159 [debug] [Thread-10 ]: Began running node model.mta.largest_expense_differences_2023
[0m19:40:08.126503 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m19:40:08.127316 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_by_station
[0m19:40:08.128017 [debug] [Thread-12 ]: Began running node model.mta.omny_adoption_increase
[0m19:40:08.128787 [info ] [Thread-5 (]: 5 of 15 START sql table model main.expense_type_per_year ....................... [RUN]
[0m19:40:08.129642 [debug] [Thread-13 ]: Began running node model.mta.subway_station_stats
[0m19:40:08.130530 [info ] [Thread-6 (]: 6 of 15 START sql table model main.fare_class_boro ............................. [RUN]
[0m19:40:08.131251 [debug] [Thread-14 ]: Began running node model.mta.total_riders_per_station
[0m19:40:08.131693 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m19:40:08.132201 [debug] [Thread-15 ]: Began running node model.mta.weekly_riders_per_station
[0m19:40:08.132837 [info ] [Thread-7 (]: 7 of 15 START sql table model main.fare_class_per_station ...................... [RUN]
[0m19:40:08.133481 [info ] [Thread-8 (]: 8 of 15 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m19:40:08.134216 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m19:40:08.134826 [info ] [Thread-9 (]: 9 of 15 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m19:40:08.135430 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m19:40:08.136252 [info ] [Thread-10 ]: 10 of 15 START sql table model main.largest_expense_differences_2023 ........... [RUN]
[0m19:40:08.143552 [info ] [Thread-11 ]: 11 of 15 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m19:40:08.150215 [info ] [Thread-12 ]: 12 of 15 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m19:40:08.172862 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m19:40:08.178029 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m19:40:08.178843 [info ] [Thread-13 ]: 13 of 15 START sql table model main.subway_station_stats ....................... [RUN]
[0m19:40:08.179611 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m19:40:08.180201 [info ] [Thread-14 ]: 14 of 15 START sql table model main.total_riders_per_station ................... [RUN]
[0m19:40:08.185424 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m19:40:08.186774 [info ] [Thread-15 ]: 15 of 15 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m19:40:08.188900 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.fare_class_per_station'
[0m19:40:08.189824 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m19:40:08.190483 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m19:40:08.191065 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m19:40:08.193584 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m19:40:08.194261 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m19:40:08.194831 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m19:40:08.195448 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m19:40:08.196063 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m19:40:08.196930 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m19:40:08.197592 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m19:40:08.198162 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:40:08.198790 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m19:40:08.199479 [debug] [Thread-15 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m19:40:08.200012 [debug] [Thread-7 (]: Began compiling node model.mta.fare_class_per_station
[0m19:40:08.200625 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m19:40:08.201314 [debug] [Thread-8 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m19:40:08.205304 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m19:40:08.206176 [debug] [Thread-9 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m19:40:08.207455 [debug] [Thread-10 ]: Began compiling node model.mta.largest_expense_differences_2023
[0m19:40:08.209305 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_by_station
[0m19:40:08.210169 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m19:40:08.210728 [debug] [Thread-12 ]: Began compiling node model.mta.omny_adoption_increase
[0m19:40:08.213811 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m19:40:08.214399 [debug] [Thread-13 ]: Began compiling node model.mta.subway_station_stats
[0m19:40:08.217691 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m19:40:08.218654 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m19:40:08.219800 [debug] [Thread-14 ]: Began compiling node model.mta.total_riders_per_station
[0m19:40:08.221232 [debug] [Thread-15 ]: Began compiling node model.mta.weekly_riders_per_station
[0m19:40:08.224192 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.fare_class_per_station"
[0m19:40:08.227462 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m19:40:08.230167 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m19:40:08.235199 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m19:40:08.239423 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m19:40:08.239996 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m19:40:08.242346 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m19:40:08.244899 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m19:40:08.246716 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m19:40:08.250647 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m19:40:08.251277 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m19:40:08.252025 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:40:08.254188 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m19:40:08.257942 [debug] [Thread-15 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m19:40:08.258527 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m19:40:08.260300 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:40:08.260984 [debug] [Thread-7 (]: Began executing node model.mta.fare_class_per_station
[0m19:40:08.261445 [debug] [Thread-8 (]: Began executing node model.mta.forecast_accuracy_2023
[0m19:40:08.264373 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m19:40:08.266174 [debug] [Thread-9 (]: Began executing node model.mta.labor_expenses_per_agency
[0m19:40:08.266675 [debug] [Thread-10 ]: Began executing node model.mta.largest_expense_differences_2023
[0m19:40:08.267392 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:40:08.268285 [debug] [Thread-12 ]: Began executing node model.mta.omny_adoption_increase
[0m19:40:08.271090 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m19:40:08.271885 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_by_station
[0m19:40:08.277264 [debug] [Thread-13 ]: Began executing node model.mta.subway_station_stats
[0m19:40:08.278017 [debug] [Thread-14 ]: Began executing node model.mta.total_riders_per_station
[0m19:40:08.281662 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m19:40:08.282439 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m19:40:08.284884 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.fare_class_per_station"
[0m19:40:08.285458 [debug] [Thread-15 ]: Began executing node model.mta.weekly_riders_per_station
[0m19:40:08.288448 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m19:40:08.291344 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m19:40:08.292307 [debug] [Thread-1 (]: SQL status: OK in 0.040 seconds
[0m19:40:08.292688 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:40:08.295669 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m19:40:08.296236 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m19:40:08.298774 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m19:40:08.301114 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m19:40:08.303912 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m19:40:08.304339 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:40:08.306612 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m19:40:08.307263 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:40:08.307651 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:40:08.311068 [debug] [Thread-15 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m19:40:08.311656 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:40:08.312573 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:40:08.312939 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:40:08.313421 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:40:08.313867 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m19:40:08.314624 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:40:08.315072 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:40:08.317011 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:40:08.317543 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m19:40:08.317977 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:40:08.318362 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:40:08.319447 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m19:40:08.321109 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:40:08.321482 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:40:08.321938 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: BEGIN
[0m19:40:08.322668 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m19:40:08.323191 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m19:40:08.323803 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m19:40:08.324248 [debug] [Thread-2 (]: SQL status: OK in 0.017 seconds
[0m19:40:08.324827 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:40:08.325655 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: BEGIN
[0m19:40:08.326170 [debug] [Thread-13 ]: On model.mta.subway_station_stats: BEGIN
[0m19:40:08.326637 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m19:40:08.327373 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: BEGIN
[0m19:40:08.327962 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m19:40:08.328520 [debug] [Thread-3 (]: SQL status: OK in 0.014 seconds
[0m19:40:08.328973 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:40:08.329390 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m19:40:08.329779 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: BEGIN
[0m19:40:08.330181 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m19:40:08.330561 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m19:40:08.331218 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m19:40:08.332031 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:40:08.332903 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m19:40:08.333639 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m19:40:08.334387 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m19:40:08.335027 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m19:40:08.335729 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:40:08.336913 [debug] [Thread-4 (]: SQL status: OK in 0.012 seconds
[0m19:40:08.337418 [debug] [Thread-15 ]: Opening a new connection, currently in state init
[0m19:40:08.337981 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m19:40:08.338879 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m19:40:08.340163 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m19:40:08.340630 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:40:08.343557 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m19:40:08.344837 [debug] [Thread-5 (]: SQL status: OK in 0.018 seconds
[0m19:40:08.345379 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:40:08.345885 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m19:40:08.347027 [debug] [Thread-6 (]: SQL status: OK in 0.018 seconds
[0m19:40:08.347537 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:40:08.348071 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:40:08.350156 [debug] [Thread-7 (]: SQL status: OK in 0.020 seconds
[0m19:40:08.350724 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:40:08.351272 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_per_station__dbt_tmp"
  
    as (
      WITH total_ridership_per_station AS (
    -- Calculate total ridership for each station_complex
    SELECT 
        station_complex, 
        SUM(ridership) AS total_ridership_station
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category and station_complex
    SELECT 
        station_complex, 
        fare_class_category, 
        SUM(ridership) AS total_ridership_fare_class
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        fare_class_category IN (
            'MetroCard - Unlimited 30-Day', 
            'MetroCard - Fair Fare', 
            'OMNY - Full Fare', 
            'OMNY - Fair Fare', 
            'MetroCard - Full Fare', 
            'MetroCard - Other', 
            'MetroCard - Unlimited 7-Day', 
            'OMNY - Seniors & Disability', 
            'OMNY - Students', 
            'OMNY - Other', 
            'MetroCard - Seniors & Disability', 
            'MetroCard - Students'
        )
    GROUP BY 
        station_complex, fare_class_category
),
pivoted_ridership AS (
    -- Pivot the fare class category percentages into columns as decimals
    SELECT
        r.station_complex,
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 30-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 30-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 7-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 7-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Students",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Students"
    FROM 
        ridership_by_fare_class r
    JOIN 
        total_ridership_per_station t
    ON 
        r.station_complex = t.station_complex
    GROUP BY 
        r.station_complex, t.total_ridership_station
)
SELECT * 
FROM pivoted_ridership
ORDER BY station_complex
    );
  
  
[0m19:40:08.355380 [debug] [Thread-8 (]: SQL status: OK in 0.025 seconds
[0m19:40:08.356091 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:40:08.357864 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m19:40:08.371065 [debug] [Thread-9 (]: SQL status: OK in 0.040 seconds
[0m19:40:08.371778 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:40:08.372339 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m19:40:08.375598 [debug] [Thread-10 ]: SQL status: OK in 0.043 seconds
[0m19:40:08.376127 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:40:08.376576 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m19:40:08.378460 [debug] [Thread-13 ]: SQL status: OK in 0.045 seconds
[0m19:40:08.378867 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:40:08.379439 [debug] [Thread-13 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    -- Calculate the weekend transfer percentage change compared to the weekday
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m19:40:08.387357 [debug] [Thread-12 ]: SQL status: OK in 0.053 seconds
[0m19:40:08.388017 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:40:08.388565 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m19:40:08.390877 [debug] [Thread-11 ]: SQL status: OK in 0.056 seconds
[0m19:40:08.391357 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:40:08.391777 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m19:40:08.394842 [debug] [Thread-15 ]: SQL status: OK in 0.057 seconds
[0m19:40:08.395303 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:40:08.395788 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start
    );
  
  
[0m19:40:08.400232 [debug] [Thread-14 ]: SQL status: OK in 0.062 seconds
[0m19:40:08.400703 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:40:08.401086 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:40:08.441483 [debug] [Thread-13 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024
    FROM 
        "mtastats"."main"."mta_operations_statement"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    -- Calculate the weekend transfer percentage change compared to the weekday
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m19:40:08.443201 [debug] [Thread-13 ]: DuckDB adapter: Rolling back transaction.
[0m19:40:08.444117 [debug] [Thread-13 ]: On model.mta.subway_station_stats: ROLLBACK
[0m19:40:08.481503 [debug] [Thread-13 ]: Failed to rollback 'model.mta.subway_station_stats'
[0m19:40:08.482268 [debug] [Thread-13 ]: On model.mta.subway_station_stats: Close
[0m19:40:08.483460 [debug] [Thread-13 ]: Runtime Error in model subway_station_stats (models/subway_station_stats.sql)
  Binder Error: Referenced column "transit_timestamp" not found in FROM clause!
  Candidate bindings: "mta_operations_statement.timestamp"
  LINE 22:         YEAR(transit_timestamp) = 2024
                        ^
[0m19:40:08.489242 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c7826890>]}
[0m19:40:08.491112 [error] [Thread-13 ]: 13 of 15 ERROR creating sql table model main.subway_station_stats .............. [[31mERROR[0m in 0.29s]
[0m19:40:08.492947 [debug] [Thread-13 ]: Finished running node model.mta.subway_station_stats
[0m19:40:08.501211 [debug] [Thread-5 (]: SQL status: OK in 0.155 seconds
[0m19:40:08.528188 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:40:08.528886 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m19:40:08.530448 [debug] [Thread-5 (]: SQL status: OK in 0.001 seconds
[0m19:40:08.582200 [debug] [Thread-9 (]: SQL status: OK in 0.209 seconds
[0m19:40:08.586871 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:40:08.587429 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m19:40:08.588135 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m19:40:08.589381 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m19:40:08.589981 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:40:08.594191 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m19:40:08.594889 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m19:40:08.595449 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:40:08.596658 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m19:40:08.609100 [debug] [Thread-5 (]: SQL status: OK in 0.013 seconds
[0m19:40:08.622155 [debug] [Thread-9 (]: SQL status: OK in 0.025 seconds
[0m19:40:08.628627 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:40:08.633332 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:40:08.634637 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m19:40:08.635799 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m19:40:08.637352 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m19:40:08.641514 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: Close
[0m19:40:08.642330 [debug] [Thread-5 (]: SQL status: OK in 0.006 seconds
[0m19:40:08.643315 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bba669b0>]}
[0m19:40:08.648356 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m19:40:08.650529 [info ] [Thread-9 (]: 9 of 15 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 0.45s]
[0m19:40:08.651776 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bba65420>]}
[0m19:40:08.652632 [debug] [Thread-9 (]: Finished running node model.mta.labor_expenses_per_agency
[0m19:40:08.653573 [info ] [Thread-5 (]: 5 of 15 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 0.48s]
[0m19:40:08.662535 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m19:40:08.666866 [debug] [Thread-4 (]: SQL status: OK in 0.322 seconds
[0m19:40:08.683147 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:40:08.684234 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m19:40:08.686524 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m19:40:08.705119 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m19:40:08.705981 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:40:08.706865 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m19:40:08.716314 [debug] [Thread-4 (]: SQL status: OK in 0.009 seconds
[0m19:40:08.734968 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:40:08.735782 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m19:40:08.790237 [debug] [Thread-4 (]: SQL status: OK in 0.009 seconds
[0m19:40:08.798191 [debug] [Thread-10 ]: SQL status: OK in 0.421 seconds
[0m19:40:08.800665 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m19:40:08.812930 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:40:08.818877 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m19:40:08.815575 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff6d3d65ab0>]}
[0m19:40:08.830518 [debug] [Thread-10 ]: SQL status: OK in 0.011 seconds
[0m19:40:08.832941 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: COMMIT
[0m19:40:08.833536 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:40:08.834061 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: COMMIT
[0m19:40:08.829303 [info ] [Thread-4 (]: 4 of 15 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 0.68s]
[0m19:40:08.835689 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m19:40:08.858852 [debug] [Thread-10 ]: SQL status: OK in 0.024 seconds
[0m19:40:08.879514 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:40:08.884545 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m19:40:08.893119 [debug] [Thread-10 ]: SQL status: OK in 0.002 seconds
[0m19:40:08.895527 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: Close
[0m19:40:08.899630 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff79c7300a0>]}
[0m19:40:08.902106 [info ] [Thread-10 ]: 10 of 15 OK created sql table model main.largest_expense_differences_2023 ...... [[32mOK[0m in 0.70s]
[0m19:40:08.908966 [debug] [Thread-10 ]: Finished running node model.mta.largest_expense_differences_2023
[0m19:40:08.958635 [debug] [Thread-8 (]: SQL status: OK in 0.597 seconds
[0m19:40:08.964277 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:40:08.967157 [debug] [Thread-2 (]: SQL status: OK in 0.625 seconds
[0m19:40:08.969049 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m19:40:08.973617 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:40:08.995440 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m19:40:08.997346 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m19:40:09.000610 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m19:40:09.001618 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:40:09.002263 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m19:40:09.025330 [debug] [Thread-8 (]: SQL status: OK in 0.035 seconds
[0m19:40:09.028375 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m19:40:09.030314 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:40:09.031360 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m19:40:09.029316 [debug] [Thread-2 (]: SQL status: OK in 0.026 seconds
[0m19:40:09.039956 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:40:09.040972 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m19:40:09.055952 [debug] [Thread-8 (]: SQL status: OK in 0.024 seconds
[0m19:40:09.060427 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:40:09.061958 [debug] [Thread-2 (]: SQL status: OK in 0.020 seconds
[0m19:40:09.076942 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m19:40:09.078591 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bbbaa410>]}
[0m19:40:09.063051 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m19:40:09.096365 [debug] [Thread-8 (]: SQL status: OK in 0.001 seconds
[0m19:40:09.099671 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: Close
[0m19:40:09.079997 [info ] [Thread-2 (]: 2 of 15 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 0.96s]
[0m19:40:09.105358 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m19:40:09.101236 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bba67520>]}
[0m19:40:09.107580 [info ] [Thread-8 (]: 8 of 15 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 0.91s]
[0m19:40:09.112310 [debug] [Thread-8 (]: Finished running node model.mta.forecast_accuracy_2023
[0m19:40:10.371160 [debug] [Thread-1 (]: SQL status: OK in 2.039 seconds
[0m19:40:10.378699 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:40:10.385420 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m19:40:10.386840 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m19:40:10.405952 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m19:40:10.414207 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:40:10.415197 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m19:40:10.429198 [debug] [Thread-1 (]: SQL status: OK in 0.013 seconds
[0m19:40:10.446991 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:40:10.455393 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m19:40:10.456734 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m19:40:10.459982 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m19:40:10.463816 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bbbab790>]}
[0m19:40:10.466165 [info ] [Thread-1 (]: 1 of 15 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 2.36s]
[0m19:40:10.480596 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m19:40:12.611339 [debug] [Thread-3 (]: SQL status: OK in 4.268 seconds
[0m19:40:12.619234 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:40:12.632265 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m19:40:12.635028 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m19:40:12.637938 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m19:40:12.638769 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:40:12.639418 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m19:40:12.664290 [debug] [Thread-3 (]: SQL status: OK in 0.024 seconds
[0m19:40:12.668912 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:40:12.669706 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m19:40:12.670831 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m19:40:12.672974 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m19:40:12.673888 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bbbaae00>]}
[0m19:40:12.675190 [info ] [Thread-3 (]: 3 of 15 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 4.55s]
[0m19:40:12.676433 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m19:40:14.155891 [debug] [Thread-11 ]: SQL status: OK in 5.763 seconds
[0m19:40:14.169197 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:40:14.170271 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m19:40:14.171689 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m19:40:14.174595 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m19:40:14.190288 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:40:14.191342 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m19:40:14.224913 [debug] [Thread-11 ]: SQL status: OK in 0.033 seconds
[0m19:40:14.229764 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:40:14.230621 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m19:40:14.232035 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m19:40:14.235223 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: Close
[0m19:40:14.237307 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bba67760>]}
[0m19:40:14.265629 [info ] [Thread-11 ]: 11 of 15 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 6.04s]
[0m19:40:14.266986 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_by_station
[0m19:40:14.535147 [debug] [Thread-12 ]: SQL status: OK in 6.146 seconds
[0m19:40:14.541632 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:40:14.542462 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m19:40:14.543832 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m19:40:14.546765 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: COMMIT
[0m19:40:14.555020 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:40:14.555965 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: COMMIT
[0m19:40:14.571286 [debug] [Thread-12 ]: SQL status: OK in 0.014 seconds
[0m19:40:14.576490 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:40:14.577496 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m19:40:14.578921 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m19:40:14.581549 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: Close
[0m19:40:14.595415 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bba67130>]}
[0m19:40:14.596826 [info ] [Thread-12 ]: 12 of 15 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 6.40s]
[0m19:40:14.598183 [debug] [Thread-12 ]: Finished running node model.mta.omny_adoption_increase
[0m19:40:14.828640 [debug] [Thread-7 (]: SQL status: OK in 6.476 seconds
[0m19:40:14.833838 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:40:14.834618 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */
alter table "mtastats"."main"."fare_class_per_station__dbt_tmp" rename to "fare_class_per_station"
[0m19:40:14.835883 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m19:40:14.838492 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: COMMIT
[0m19:40:14.839202 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:40:14.839805 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: COMMIT
[0m19:40:14.850141 [debug] [Thread-7 (]: SQL status: OK in 0.010 seconds
[0m19:40:14.857552 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:40:14.858614 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */
drop table if exists "mtastats"."main"."fare_class_per_station__dbt_backup" cascade
[0m19:40:14.859911 [debug] [Thread-7 (]: SQL status: OK in 0.000 seconds
[0m19:40:14.862333 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: Close
[0m19:40:14.863504 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bba66650>]}
[0m19:40:14.866310 [info ] [Thread-7 (]: 7 of 15 OK created sql table model main.fare_class_per_station ................. [[32mOK[0m in 6.67s]
[0m19:40:14.868708 [debug] [Thread-7 (]: Finished running node model.mta.fare_class_per_station
[0m19:40:15.348618 [debug] [Thread-15 ]: SQL status: OK in 6.952 seconds
[0m19:40:15.363555 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:40:15.364768 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m19:40:15.366308 [debug] [Thread-15 ]: SQL status: OK in 0.001 seconds
[0m19:40:15.369059 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m19:40:15.369725 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:40:15.394781 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m19:40:15.426752 [debug] [Thread-15 ]: SQL status: OK in 0.031 seconds
[0m19:40:15.431253 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:40:15.432201 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m19:40:15.433294 [debug] [Thread-15 ]: SQL status: OK in 0.000 seconds
[0m19:40:15.435490 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: Close
[0m19:40:15.436473 [debug] [Thread-15 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7b898d7e0>]}
[0m19:40:15.437471 [info ] [Thread-15 ]: 15 of 15 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 7.24s]
[0m19:40:15.438480 [debug] [Thread-15 ]: Finished running node model.mta.weekly_riders_per_station
[0m19:40:16.588593 [debug] [Thread-14 ]: SQL status: OK in 8.187 seconds
[0m19:40:16.615126 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:40:16.615950 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m19:40:16.617224 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m19:40:16.619909 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: COMMIT
[0m19:40:16.620818 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:40:16.621553 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: COMMIT
[0m19:40:16.644704 [debug] [Thread-14 ]: SQL status: OK in 0.022 seconds
[0m19:40:16.649230 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:40:16.650083 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m19:40:16.650976 [debug] [Thread-14 ]: SQL status: OK in 0.000 seconds
[0m19:40:16.652429 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: Close
[0m19:40:16.653389 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff6d3d44790>]}
[0m19:40:16.654579 [info ] [Thread-14 ]: 14 of 15 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 8.45s]
[0m19:40:16.655573 [debug] [Thread-14 ]: Finished running node model.mta.total_riders_per_station
[0m19:40:16.659505 [debug] [Thread-6 (]: SQL status: OK in 8.311 seconds
[0m19:40:16.662290 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:40:16.662670 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m19:40:16.663484 [debug] [Thread-6 (]: SQL status: OK in 0.000 seconds
[0m19:40:16.664984 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m19:40:16.665356 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:40:16.665685 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m19:40:16.669339 [debug] [Thread-6 (]: SQL status: OK in 0.003 seconds
[0m19:40:16.671731 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:40:16.672103 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m19:40:16.672755 [debug] [Thread-6 (]: SQL status: OK in 0.000 seconds
[0m19:40:16.673885 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m19:40:16.773611 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87da0255-c022-474e-8c61-92665da0f09d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bba66d70>]}
[0m19:40:16.775173 [info ] [Thread-6 (]: 6 of 15 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 8.59s]
[0m19:40:16.776331 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m19:40:16.781293 [debug] [MainThread]: Using duckdb connection "master"
[0m19:40:16.782061 [debug] [MainThread]: On master: BEGIN
[0m19:40:16.782513 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:40:16.796283 [debug] [MainThread]: SQL status: OK in 0.014 seconds
[0m19:40:16.797158 [debug] [MainThread]: On master: COMMIT
[0m19:40:16.797794 [debug] [MainThread]: Using duckdb connection "master"
[0m19:40:16.798288 [debug] [MainThread]: On master: COMMIT
[0m19:40:16.799367 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m19:40:16.800001 [debug] [MainThread]: On master: Close
[0m19:40:16.804486 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:40:16.805274 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m19:40:16.805783 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m19:40:16.806257 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m19:40:16.806728 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m19:40:16.807234 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m19:40:16.807701 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m19:40:16.808204 [debug] [MainThread]: Connection 'model.mta.fare_class_per_station' was properly closed.
[0m19:40:16.808751 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m19:40:16.809290 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m19:40:16.809891 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m19:40:16.810260 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m19:40:16.810660 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m19:40:16.810969 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m19:40:16.811235 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m19:40:16.811503 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m19:40:16.812235 [info ] [MainThread]: 
[0m19:40:16.813106 [info ] [MainThread]: Finished running 15 table models in 0 hours 0 minutes and 8.88 seconds (8.88s).
[0m19:40:16.818752 [debug] [MainThread]: Command end result
[0m19:40:16.870319 [info ] [MainThread]: 
[0m19:40:16.871594 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:40:16.872259 [info ] [MainThread]: 
[0m19:40:16.872967 [error] [MainThread]:   Runtime Error in model subway_station_stats (models/subway_station_stats.sql)
  Binder Error: Referenced column "transit_timestamp" not found in FROM clause!
  Candidate bindings: "mta_operations_statement.timestamp"
  LINE 22:         YEAR(transit_timestamp) = 2024
                        ^
[0m19:40:16.873595 [info ] [MainThread]: 
[0m19:40:16.874175 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=1 SKIP=0 TOTAL=15
[0m19:40:16.875645 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 9.57965, "process_user_time": 125.725464, "process_kernel_time": 3.571233, "process_mem_max_rss": "506280", "process_out_blocks": "21152", "command_success": false, "process_in_blocks": "0"}
[0m19:40:16.876230 [debug] [MainThread]: Command `dbt run` failed at 19:40:16.876124 after 9.58 seconds
[0m19:40:16.876661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c851f3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff79c730070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7bbb49990>]}
[0m19:40:16.877087 [debug] [MainThread]: Flushing usage events
[0m19:41:13.292992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f28a3400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f24643a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f1101ea0>]}


============================== 19:41:13.294980 | 5f842428-40b4-4f11-99a2-a155c4ccb8ab ==============================
[0m19:41:13.294980 [info ] [MainThread]: Running with dbt=1.8.7
[0m19:41:13.295452 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:41:13.459174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5edd1e0e0>]}
[0m19:41:13.510513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f1b7a800>]}
[0m19:41:13.513276 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m19:41:13.523975 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m19:41:13.604436 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:41:13.605060 [debug] [MainThread]: Partial parsing: updated file: mta://models/subway_station_stats.sql
[0m19:41:13.812030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5e59d8580>]}
[0m19:41:13.870770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5e5bde200>]}
[0m19:41:13.871232 [info ] [MainThread]: Found 15 models, 9 sources, 416 macros
[0m19:41:13.871575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5e5b51a20>]}
[0m19:41:13.873117 [info ] [MainThread]: 
[0m19:41:13.873623 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m19:41:13.877943 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m19:41:13.948894 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m19:41:13.949294 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m19:41:13.949594 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:41:13.962003 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m19:41:13.962932 [debug] [ThreadPool]: On list_mtastats: Close
[0m19:41:13.965305 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m19:41:13.965702 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m19:41:13.970095 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:41:13.970362 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m19:41:13.970599 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:41:13.975687 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m19:41:13.976530 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:41:13.976767 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m19:41:13.977187 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:41:13.977498 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:41:13.977735 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m19:41:13.978203 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:41:13.978904 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m19:41:13.979111 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m19:41:13.979326 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m19:41:13.979708 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m19:41:13.979942 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m19:41:13.982875 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m19:41:13.986348 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m19:41:13.986630 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m19:41:13.986903 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:41:13.992424 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m19:41:13.992711 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m19:41:13.993050 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m19:41:14.008449 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m19:41:14.010062 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m19:41:14.010736 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m19:41:14.011148 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m19:41:14.014526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f2b8ecb0>]}
[0m19:41:14.014958 [debug] [MainThread]: Using duckdb connection "master"
[0m19:41:14.015216 [debug] [MainThread]: On master: BEGIN
[0m19:41:14.015435 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:41:14.021078 [debug] [MainThread]: SQL status: OK in 0.006 seconds
[0m19:41:14.021361 [debug] [MainThread]: On master: COMMIT
[0m19:41:14.021602 [debug] [MainThread]: Using duckdb connection "master"
[0m19:41:14.021815 [debug] [MainThread]: On master: COMMIT
[0m19:41:14.022164 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m19:41:14.022405 [debug] [MainThread]: On master: Close
[0m19:41:14.023954 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m19:41:14.024422 [info ] [MainThread]: 
[0m19:41:14.034001 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m19:41:14.034733 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m19:41:14.035300 [info ] [Thread-1 (]: 1 of 15 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m19:41:14.035937 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m19:41:14.036392 [info ] [Thread-2 (]: 2 of 15 START sql table model main.bond_payment_info ........................... [RUN]
[0m19:41:14.036989 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m19:41:14.037647 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m19:41:14.038204 [info ] [Thread-3 (]: 3 of 15 START sql table model main.busiest_specific_times ...................... [RUN]
[0m19:41:14.038697 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m19:41:14.039207 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m19:41:14.039832 [debug] [Thread-7 (]: Began running node model.mta.fare_class_per_station
[0m19:41:14.040487 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m19:41:14.041117 [debug] [Thread-8 (]: Began running node model.mta.forecast_accuracy_2023
[0m19:41:14.042459 [debug] [Thread-9 (]: Began running node model.mta.labor_expenses_per_agency
[0m19:41:14.043229 [debug] [Thread-10 ]: Began running node model.mta.largest_expense_differences_2023
[0m19:41:14.044657 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_by_station
[0m19:41:14.045321 [debug] [Thread-12 ]: Began running node model.mta.omny_adoption_increase
[0m19:41:14.044131 [info ] [Thread-4 (]: 4 of 15 START sql table model main.daily_ridership ............................. [RUN]
[0m19:41:14.045930 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m19:41:14.046597 [debug] [Thread-13 ]: Began running node model.mta.subway_station_stats
[0m19:41:14.047615 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m19:41:14.048269 [debug] [Thread-14 ]: Began running node model.mta.total_riders_per_station
[0m19:41:14.049638 [debug] [Thread-15 ]: Began running node model.mta.weekly_riders_per_station
[0m19:41:14.048941 [info ] [Thread-5 (]: 5 of 15 START sql table model main.expense_type_per_year ....................... [RUN]
[0m19:41:14.051317 [info ] [Thread-6 (]: 6 of 15 START sql table model main.fare_class_boro ............................. [RUN]
[0m19:41:14.051970 [info ] [Thread-7 (]: 7 of 15 START sql table model main.fare_class_per_station ...................... [RUN]
[0m19:41:14.052496 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m19:41:14.053101 [info ] [Thread-8 (]: 8 of 15 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m19:41:14.053679 [info ] [Thread-9 (]: 9 of 15 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m19:41:14.054444 [info ] [Thread-10 ]: 10 of 15 START sql table model main.largest_expense_differences_2023 ........... [RUN]
[0m19:41:14.055154 [info ] [Thread-11 ]: 11 of 15 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m19:41:14.055829 [info ] [Thread-12 ]: 12 of 15 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m19:41:14.056433 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m19:41:14.064572 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m19:41:14.065267 [info ] [Thread-13 ]: 13 of 15 START sql table model main.subway_station_stats ....................... [RUN]
[0m19:41:14.065884 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m19:41:14.066478 [info ] [Thread-14 ]: 14 of 15 START sql table model main.total_riders_per_station ................... [RUN]
[0m19:41:14.067238 [info ] [Thread-15 ]: 15 of 15 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m19:41:14.068209 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m19:41:14.069526 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m19:41:14.071047 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.fare_class_per_station'
[0m19:41:14.074198 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m19:41:14.074952 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m19:41:14.075666 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m19:41:14.076286 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m19:41:14.076935 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m19:41:14.077480 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m19:41:14.077954 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m19:41:14.078740 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m19:41:14.079318 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m19:41:14.082673 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m19:41:14.083323 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m19:41:14.084121 [debug] [Thread-15 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m19:41:14.085093 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m19:41:14.085872 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m19:41:14.087156 [debug] [Thread-7 (]: Began compiling node model.mta.fare_class_per_station
[0m19:41:14.088367 [debug] [Thread-8 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m19:41:14.089095 [debug] [Thread-9 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m19:41:14.089630 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m19:41:14.090131 [debug] [Thread-10 ]: Began compiling node model.mta.largest_expense_differences_2023
[0m19:41:14.090619 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_by_station
[0m19:41:14.091070 [debug] [Thread-12 ]: Began compiling node model.mta.omny_adoption_increase
[0m19:41:14.096203 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m19:41:14.097327 [debug] [Thread-13 ]: Began compiling node model.mta.subway_station_stats
[0m19:41:14.104189 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m19:41:14.104741 [debug] [Thread-14 ]: Began compiling node model.mta.total_riders_per_station
[0m19:41:14.124086 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m19:41:14.124785 [debug] [Thread-15 ]: Began compiling node model.mta.weekly_riders_per_station
[0m19:41:14.126971 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m19:41:14.129864 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m19:41:14.132294 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.fare_class_per_station"
[0m19:41:14.134680 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m19:41:14.136485 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m19:41:14.139429 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m19:41:14.141267 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m19:41:14.143071 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m19:41:14.145195 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m19:41:14.145951 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m19:41:14.148074 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m19:41:14.150284 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m19:41:14.152045 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m19:41:14.154000 [debug] [Thread-15 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m19:41:14.154571 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:41:14.155237 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m19:41:14.156080 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m19:41:14.156445 [debug] [Thread-7 (]: Began executing node model.mta.fare_class_per_station
[0m19:41:14.157025 [debug] [Thread-8 (]: Began executing node model.mta.forecast_accuracy_2023
[0m19:41:14.158087 [debug] [Thread-9 (]: Began executing node model.mta.labor_expenses_per_agency
[0m19:41:14.158531 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:41:14.159226 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_by_station
[0m19:41:14.159986 [debug] [Thread-10 ]: Began executing node model.mta.largest_expense_differences_2023
[0m19:41:14.160331 [debug] [Thread-12 ]: Began executing node model.mta.omny_adoption_increase
[0m19:41:14.163388 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m19:41:14.164429 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:41:14.164904 [debug] [Thread-13 ]: Began executing node model.mta.subway_station_stats
[0m19:41:14.165348 [debug] [Thread-14 ]: Began executing node model.mta.total_riders_per_station
[0m19:41:14.165980 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m19:41:14.166426 [debug] [Thread-15 ]: Began executing node model.mta.weekly_riders_per_station
[0m19:41:14.168558 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m19:41:14.170678 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m19:41:14.172771 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.fare_class_per_station"
[0m19:41:14.175009 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m19:41:14.177148 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m19:41:14.177760 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m19:41:14.180228 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m19:41:14.182353 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m19:41:14.184565 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m19:41:14.185101 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:41:14.185523 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m19:41:14.187554 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m19:41:14.189698 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m19:41:14.190169 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:41:14.192214 [debug] [Thread-15 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m19:41:14.192974 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:41:14.193445 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:41:14.194773 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:41:14.195241 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m19:41:14.195687 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:41:14.197050 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:41:14.197697 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:41:14.198640 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:41:14.199250 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:41:14.199674 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m19:41:14.200178 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m19:41:14.201321 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:41:14.207884 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:41:14.209217 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m19:41:14.210048 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m19:41:14.210641 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:41:14.211297 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: BEGIN
[0m19:41:14.211684 [debug] [Thread-1 (]: SQL status: OK in 0.022 seconds
[0m19:41:14.212348 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m19:41:14.212860 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m19:41:14.213478 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m19:41:14.213991 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: BEGIN
[0m19:41:14.214510 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: BEGIN
[0m19:41:14.214990 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:41:14.216861 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: BEGIN
[0m19:41:14.217345 [debug] [Thread-13 ]: On model.mta.subway_station_stats: BEGIN
[0m19:41:14.217809 [debug] [Thread-2 (]: SQL status: OK in 0.023 seconds
[0m19:41:14.218264 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:41:14.218771 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m19:41:14.219321 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m19:41:14.219987 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m19:41:14.220427 [debug] [Thread-3 (]: SQL status: OK in 0.020 seconds
[0m19:41:14.220900 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:41:14.221381 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m19:41:14.221826 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m19:41:14.222289 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m19:41:14.222757 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m19:41:14.223172 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m19:41:14.223787 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m19:41:14.224194 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m19:41:14.224909 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:41:14.225494 [debug] [Thread-15 ]: Opening a new connection, currently in state init
[0m19:41:14.226040 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:41:14.226743 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m19:41:14.227912 [debug] [Thread-4 (]: SQL status: OK in 0.013 seconds
[0m19:41:14.229105 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m19:41:14.229925 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m19:41:14.230782 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:41:14.232376 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(subways_total_ridership) AS ridership,
    'subway' AS transport_type,
    AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(buses_total_ridership) AS ridership,
    'buses' AS transport_type,
    AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(lirr_total_ridership) AS ridership,
    'lirr' AS transport_type,
    AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(metro_north_total_ridership) AS ridership,
    'metro_north' AS transport_type,
    AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(access_a_ride_total_trips) AS ridership,
    'access_a_ride' AS transport_type,
    AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(bridges_tunnels_total_traffic) AS ridership,
    'bridges_tunnels' AS transport_type,
    AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type

UNION ALL

SELECT 
    DATE_TRUNC('week', date) AS week_start,
    SUM(staten_island_railway_total_ridership) AS ridership,
    'staten_island_railway' AS transport_type,
    AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
FROM "mtastats"."main"."mta_daily_ridership"
GROUP BY week_start, transport_type
    );
  
  
[0m19:41:14.233069 [debug] [Thread-6 (]: SQL status: OK in 0.015 seconds
[0m19:41:14.233580 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:41:14.234331 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:41:14.235546 [debug] [Thread-5 (]: SQL status: OK in 0.017 seconds
[0m19:41:14.236023 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:41:14.236458 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m19:41:14.238094 [debug] [Thread-7 (]: SQL status: OK in 0.018 seconds
[0m19:41:14.238660 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:41:14.239250 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_per_station__dbt_tmp"
  
    as (
      WITH total_ridership_per_station AS (
    -- Calculate total ridership for each station_complex
    SELECT 
        station_complex, 
        SUM(ridership) AS total_ridership_station
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category and station_complex
    SELECT 
        station_complex, 
        fare_class_category, 
        SUM(ridership) AS total_ridership_fare_class
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        fare_class_category IN (
            'MetroCard - Unlimited 30-Day', 
            'MetroCard - Fair Fare', 
            'OMNY - Full Fare', 
            'OMNY - Fair Fare', 
            'MetroCard - Full Fare', 
            'MetroCard - Other', 
            'MetroCard - Unlimited 7-Day', 
            'OMNY - Seniors & Disability', 
            'OMNY - Students', 
            'OMNY - Other', 
            'MetroCard - Seniors & Disability', 
            'MetroCard - Students'
        )
    GROUP BY 
        station_complex, fare_class_category
),
pivoted_ridership AS (
    -- Pivot the fare class category percentages into columns as decimals
    SELECT
        r.station_complex,
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 30-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 30-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Fair Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Fair Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Full Fare' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Full Fare",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Unlimited 7-Day' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Unlimited 7-Day",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Students",
        ROUND(SUM(CASE WHEN fare_class_category = 'OMNY - Other' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "OMNY - Other",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Seniors & Disability' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Seniors & Disability",
        ROUND(SUM(CASE WHEN fare_class_category = 'MetroCard - Students' THEN total_ridership_fare_class ELSE 0 END) / t.total_ridership_station, 2) AS "MetroCard - Students"
    FROM 
        ridership_by_fare_class r
    JOIN 
        total_ridership_per_station t
    ON 
        r.station_complex = t.station_complex
    GROUP BY 
        r.station_complex, t.total_ridership_station
)
SELECT * 
FROM pivoted_ridership
ORDER BY station_complex
    );
  
  
[0m19:41:14.245277 [debug] [Thread-8 (]: SQL status: OK in 0.024 seconds
[0m19:41:14.245734 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:41:14.246271 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m19:41:14.251289 [debug] [Thread-9 (]: SQL status: OK in 0.029 seconds
[0m19:41:14.251700 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:41:14.252137 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m19:41:14.255198 [debug] [Thread-11 ]: SQL status: OK in 0.033 seconds
[0m19:41:14.255598 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:41:14.256030 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m19:41:14.259128 [debug] [Thread-10 ]: SQL status: OK in 0.036 seconds
[0m19:41:14.259697 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:41:14.260225 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m19:41:14.265690 [debug] [Thread-12 ]: SQL status: OK in 0.042 seconds
[0m19:41:14.266342 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:41:14.266824 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m19:41:14.270307 [debug] [Thread-14 ]: SQL status: OK in 0.047 seconds
[0m19:41:14.270693 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:41:14.271074 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m19:41:14.273959 [debug] [Thread-13 ]: SQL status: OK in 0.050 seconds
[0m19:41:14.274532 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:41:14.275027 [debug] [Thread-13 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    -- Calculate the weekend transfer percentage change compared to the weekday
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m19:41:14.281904 [debug] [Thread-15 ]: SQL status: OK in 0.056 seconds
[0m19:41:14.282415 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:41:14.282869 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start
    );
  
  
[0m19:41:14.332461 [debug] [Thread-5 (]: SQL status: OK in 0.095 seconds
[0m19:41:14.344157 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:41:14.345279 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m19:41:14.348155 [debug] [Thread-5 (]: SQL status: OK in 0.001 seconds
[0m19:41:14.422714 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m19:41:14.423742 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:41:14.424372 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m19:41:14.434888 [debug] [Thread-5 (]: SQL status: OK in 0.010 seconds
[0m19:41:14.452024 [debug] [Thread-9 (]: SQL status: OK in 0.199 seconds
[0m19:41:14.468823 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:41:14.471183 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m19:41:14.472088 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m19:41:14.473007 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m19:41:14.475022 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m19:41:14.475997 [debug] [Thread-5 (]: SQL status: OK in 0.000 seconds
[0m19:41:14.489191 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m19:41:14.493391 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m19:41:14.494218 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:41:14.498694 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m19:41:14.502363 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5edccefb0>]}
[0m19:41:14.504409 [info ] [Thread-5 (]: 5 of 15 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 0.43s]
[0m19:41:14.508875 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m19:41:14.514431 [debug] [Thread-9 (]: SQL status: OK in 0.015 seconds
[0m19:41:14.522472 [debug] [Thread-9 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m19:41:14.525222 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m19:41:14.527379 [debug] [Thread-9 (]: SQL status: OK in 0.001 seconds
[0m19:41:14.532711 [debug] [Thread-9 (]: On model.mta.labor_expenses_per_agency: Close
[0m19:41:14.545448 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5edcce830>]}
[0m19:41:14.546755 [info ] [Thread-9 (]: 9 of 15 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 0.47s]
[0m19:41:14.547813 [debug] [Thread-9 (]: Finished running node model.mta.labor_expenses_per_agency
[0m19:41:14.745232 [debug] [Thread-4 (]: SQL status: OK in 0.512 seconds
[0m19:41:14.756249 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:41:14.757238 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m19:41:14.763384 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m19:41:14.770711 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m19:41:14.782300 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:41:14.783091 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m19:41:14.834614 [debug] [Thread-4 (]: SQL status: OK in 0.051 seconds
[0m19:41:14.856289 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m19:41:14.860562 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m19:41:14.867776 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m19:41:14.886445 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m19:41:14.888405 [debug] [Thread-8 (]: SQL status: OK in 0.639 seconds
[0m19:41:14.890014 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f12026b0>]}
[0m19:41:14.925823 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:41:14.953443 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m19:41:14.971674 [debug] [Thread-8 (]: SQL status: OK in 0.001 seconds
[0m19:41:14.933713 [debug] [Thread-10 ]: SQL status: OK in 0.673 seconds
[0m19:41:14.952851 [info ] [Thread-4 (]: 4 of 15 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 0.83s]
[0m19:41:15.036413 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m19:41:15.037740 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:41:14.951405 [debug] [Thread-2 (]: SQL status: OK in 0.720 seconds
[0m19:41:15.050429 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:41:15.051186 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m19:41:15.052501 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m19:41:14.985608 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m19:41:15.085336 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:41:15.086411 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m19:41:15.045311 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m19:41:15.089500 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m19:41:15.092588 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: COMMIT
[0m19:41:15.093662 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:41:15.094854 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: COMMIT
[0m19:41:15.055274 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m19:41:15.096664 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:41:15.097332 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m19:41:15.095828 [debug] [Thread-8 (]: SQL status: OK in 0.009 seconds
[0m19:41:15.102721 [debug] [Thread-8 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m19:41:15.103570 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m19:41:15.116657 [debug] [Thread-2 (]: SQL status: OK in 0.018 seconds
[0m19:41:15.121616 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m19:41:15.122749 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m19:41:15.124001 [debug] [Thread-8 (]: SQL status: OK in 0.020 seconds
[0m19:41:15.127121 [debug] [Thread-8 (]: On model.mta.forecast_accuracy_2023: Close
[0m19:41:15.136924 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5edcce950>]}
[0m19:41:15.151382 [debug] [Thread-10 ]: SQL status: OK in 0.056 seconds
[0m19:41:15.162139 [debug] [Thread-10 ]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m19:41:15.163127 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m19:41:15.205435 [debug] [Thread-10 ]: SQL status: OK in 0.000 seconds
[0m19:41:15.157665 [debug] [Thread-2 (]: SQL status: OK in 0.034 seconds
[0m19:41:15.215342 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m19:41:15.234633 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5e5d66c50>]}
[0m19:41:15.210575 [debug] [Thread-10 ]: On model.mta.largest_expense_differences_2023: Close
[0m19:41:15.237008 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5edccdf90>]}
[0m19:41:15.276113 [info ] [Thread-10 ]: 10 of 15 OK created sql table model main.largest_expense_differences_2023 ...... [[32mOK[0m in 1.16s]
[0m19:41:15.235897 [info ] [Thread-2 (]: 2 of 15 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 1.19s]
[0m19:41:15.280204 [debug] [Thread-10 ]: Finished running node model.mta.largest_expense_differences_2023
[0m19:41:15.156635 [info ] [Thread-8 (]: 8 of 15 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 1.06s]
[0m19:41:15.281523 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m19:41:15.306386 [debug] [Thread-8 (]: Finished running node model.mta.forecast_accuracy_2023
[0m19:41:16.241048 [debug] [Thread-1 (]: SQL status: OK in 2.010 seconds
[0m19:41:16.249770 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:41:16.285067 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m19:41:16.286518 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m19:41:16.289316 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m19:41:16.298968 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:41:16.301537 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m19:41:16.324443 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m19:41:16.345564 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m19:41:16.346490 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m19:41:16.347830 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m19:41:16.350232 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m19:41:16.351204 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5e5d67b80>]}
[0m19:41:16.352146 [info ] [Thread-1 (]: 1 of 15 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 2.31s]
[0m19:41:16.353384 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m19:41:19.206073 [debug] [Thread-3 (]: SQL status: OK in 4.974 seconds
[0m19:41:19.214577 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:41:19.216338 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m19:41:19.217964 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m19:41:19.220743 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m19:41:19.221431 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:41:19.222024 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m19:41:19.249474 [debug] [Thread-3 (]: SQL status: OK in 0.027 seconds
[0m19:41:19.269955 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m19:41:19.295733 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m19:41:19.301526 [debug] [Thread-3 (]: SQL status: OK in 0.005 seconds
[0m19:41:19.304181 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m19:41:19.310665 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd4f1198340>]}
[0m19:41:19.311871 [info ] [Thread-3 (]: 3 of 15 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 5.26s]
[0m19:41:19.312857 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m19:41:20.245071 [debug] [Thread-15 ]: SQL status: OK in 5.962 seconds
[0m19:41:20.250938 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:41:20.251908 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m19:41:20.253230 [debug] [Thread-15 ]: SQL status: OK in 0.001 seconds
[0m19:41:20.276660 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m19:41:20.290257 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:41:20.296628 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m19:41:20.335843 [debug] [Thread-15 ]: SQL status: OK in 0.038 seconds
[0m19:41:20.341628 [debug] [Thread-15 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m19:41:20.342559 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m19:41:20.343822 [debug] [Thread-15 ]: SQL status: OK in 0.000 seconds
[0m19:41:20.346199 [debug] [Thread-15 ]: On model.mta.weekly_riders_per_station: Close
[0m19:41:20.347400 [debug] [Thread-15 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5edcf23e0>]}
[0m19:41:20.350740 [debug] [Thread-12 ]: SQL status: OK in 6.083 seconds
[0m19:41:20.349660 [info ] [Thread-15 ]: 15 of 15 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 6.26s]
[0m19:41:20.358281 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:41:20.361064 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m19:41:20.362802 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m19:41:20.366288 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: COMMIT
[0m19:41:20.367523 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:41:20.368494 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: COMMIT
[0m19:41:20.359722 [debug] [Thread-15 ]: Finished running node model.mta.weekly_riders_per_station
[0m19:41:20.382372 [debug] [Thread-12 ]: SQL status: OK in 0.013 seconds
[0m19:41:20.393115 [debug] [Thread-12 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m19:41:20.399417 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m19:41:20.405396 [debug] [Thread-12 ]: SQL status: OK in 0.002 seconds
[0m19:41:20.408300 [debug] [Thread-12 ]: On model.mta.omny_adoption_increase: Close
[0m19:41:20.409591 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f1202f80>]}
[0m19:41:20.411153 [info ] [Thread-12 ]: 12 of 15 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 6.33s]
[0m19:41:20.412618 [debug] [Thread-12 ]: Finished running node model.mta.omny_adoption_increase
[0m19:41:21.369876 [debug] [Thread-7 (]: SQL status: OK in 7.129 seconds
[0m19:41:21.375673 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:41:21.376793 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */
alter table "mtastats"."main"."fare_class_per_station__dbt_tmp" rename to "fare_class_per_station"
[0m19:41:21.395491 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m19:41:21.399718 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: COMMIT
[0m19:41:21.400742 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:41:21.401549 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: COMMIT
[0m19:41:21.428410 [debug] [Thread-7 (]: SQL status: OK in 0.026 seconds
[0m19:41:21.433972 [debug] [Thread-7 (]: Using duckdb connection "model.mta.fare_class_per_station"
[0m19:41:21.435450 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_per_station"} */
drop table if exists "mtastats"."main"."fare_class_per_station__dbt_backup" cascade
[0m19:41:21.437825 [debug] [Thread-7 (]: SQL status: OK in 0.000 seconds
[0m19:41:21.440283 [debug] [Thread-7 (]: On model.mta.fare_class_per_station: Close
[0m19:41:21.441402 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5edccfa90>]}
[0m19:41:21.451996 [info ] [Thread-7 (]: 7 of 15 OK created sql table model main.fare_class_per_station ................. [[32mOK[0m in 7.37s]
[0m19:41:21.453494 [debug] [Thread-7 (]: Finished running node model.mta.fare_class_per_station
[0m19:41:21.803098 [debug] [Thread-14 ]: SQL status: OK in 7.531 seconds
[0m19:41:21.816887 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:41:21.818022 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m19:41:21.819513 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m19:41:21.823330 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: COMMIT
[0m19:41:21.854960 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:41:21.855744 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: COMMIT
[0m19:41:21.866872 [debug] [Thread-14 ]: SQL status: OK in 0.010 seconds
[0m19:41:21.871882 [debug] [Thread-14 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m19:41:21.872722 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m19:41:21.873908 [debug] [Thread-14 ]: SQL status: OK in 0.000 seconds
[0m19:41:21.876570 [debug] [Thread-14 ]: On model.mta.total_riders_per_station: Close
[0m19:41:21.877869 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd4f1180d00>]}
[0m19:41:21.879040 [info ] [Thread-14 ]: 14 of 15 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 7.79s]
[0m19:41:21.880116 [debug] [Thread-14 ]: Finished running node model.mta.total_riders_per_station
[0m19:41:25.336732 [debug] [Thread-11 ]: SQL status: OK in 11.080 seconds
[0m19:41:25.342920 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:41:25.344399 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m19:41:25.345984 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m19:41:25.348889 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m19:41:25.349832 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:41:25.350623 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m19:41:25.357655 [debug] [Thread-11 ]: SQL status: OK in 0.006 seconds
[0m19:41:25.362397 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m19:41:25.363155 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m19:41:25.365264 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m19:41:25.367742 [debug] [Thread-11 ]: On model.mta.omny_adoption_by_station: Close
[0m19:41:25.368745 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f1bfa650>]}
[0m19:41:25.369815 [info ] [Thread-11 ]: 11 of 15 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 11.29s]
[0m19:41:25.370884 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_by_station
[0m19:41:27.166812 [debug] [Thread-6 (]: SQL status: OK in 12.932 seconds
[0m19:41:27.171646 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:41:27.172294 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m19:41:27.173794 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m19:41:27.178134 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m19:41:27.179584 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:41:27.180876 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m19:41:27.186596 [debug] [Thread-6 (]: SQL status: OK in 0.004 seconds
[0m19:41:27.190404 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m19:41:27.191065 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m19:41:27.192415 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m19:41:27.194437 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m19:41:27.195339 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5edcf1870>]}
[0m19:41:27.196211 [info ] [Thread-6 (]: 6 of 15 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 13.13s]
[0m19:41:27.197094 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m19:41:27.292761 [debug] [Thread-13 ]: SQL status: OK in 13.016 seconds
[0m19:41:27.296933 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:41:27.297550 [debug] [Thread-13 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m19:41:27.298960 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m19:41:27.301431 [debug] [Thread-13 ]: On model.mta.subway_station_stats: COMMIT
[0m19:41:27.302013 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:41:27.302503 [debug] [Thread-13 ]: On model.mta.subway_station_stats: COMMIT
[0m19:41:27.307643 [debug] [Thread-13 ]: SQL status: OK in 0.005 seconds
[0m19:41:27.310499 [debug] [Thread-13 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m19:41:27.310986 [debug] [Thread-13 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m19:41:27.311782 [debug] [Thread-13 ]: SQL status: OK in 0.000 seconds
[0m19:41:27.313337 [debug] [Thread-13 ]: On model.mta.subway_station_stats: Close
[0m19:41:27.428007 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f842428-40b4-4f11-99a2-a155c4ccb8ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5edccce20>]}
[0m19:41:27.428926 [info ] [Thread-13 ]: 13 of 15 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 13.35s]
[0m19:41:27.429669 [debug] [Thread-13 ]: Finished running node model.mta.subway_station_stats
[0m19:41:27.434757 [debug] [MainThread]: Using duckdb connection "master"
[0m19:41:27.435279 [debug] [MainThread]: On master: BEGIN
[0m19:41:27.435623 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:41:27.442350 [debug] [MainThread]: SQL status: OK in 0.007 seconds
[0m19:41:27.442834 [debug] [MainThread]: On master: COMMIT
[0m19:41:27.443211 [debug] [MainThread]: Using duckdb connection "master"
[0m19:41:27.443514 [debug] [MainThread]: On master: COMMIT
[0m19:41:27.443995 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m19:41:27.444287 [debug] [MainThread]: On master: Close
[0m19:41:27.446891 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:41:27.447278 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m19:41:27.447553 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m19:41:27.447780 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m19:41:27.448016 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m19:41:27.448236 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m19:41:27.448446 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m19:41:27.448672 [debug] [MainThread]: Connection 'model.mta.fare_class_per_station' was properly closed.
[0m19:41:27.448901 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m19:41:27.449127 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m19:41:27.449353 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m19:41:27.449556 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m19:41:27.449744 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m19:41:27.449930 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m19:41:27.450131 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m19:41:27.450362 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m19:41:27.450866 [info ] [MainThread]: 
[0m19:41:27.451462 [info ] [MainThread]: Finished running 15 table models in 0 hours 0 minutes and 13.58 seconds (13.58s).
[0m19:41:27.453917 [debug] [MainThread]: Command end result
[0m19:41:27.481118 [info ] [MainThread]: 
[0m19:41:27.481656 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:41:27.482015 [info ] [MainThread]: 
[0m19:41:27.482360 [info ] [MainThread]: Done. PASS=15 WARN=0 ERROR=0 SKIP=0 TOTAL=15
[0m19:41:27.483240 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 14.231268, "process_user_time": 189.49814, "process_kernel_time": 7.584357, "process_mem_max_rss": "803300", "process_out_blocks": "21832", "process_in_blocks": "0"}
[0m19:41:27.483872 [debug] [MainThread]: Command `dbt run` succeeded at 19:41:27.483759 after 14.23 seconds
[0m19:41:27.484259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f28a3400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f1723e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5f1b7a800>]}
[0m19:41:27.485166 [debug] [MainThread]: Flushing usage events
[0m20:01:10.288407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64f6837370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64f63a6bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64f5096b00>]}


============================== 20:01:10.296380 | b8498e2c-84dc-4198-8fea-87ebad97c8ea ==============================
[0m20:01:10.296380 [info ] [MainThread]: Running with dbt=1.8.7
[0m20:01:10.296915 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:01:10.496329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64f1cdaf50>]}
[0m20:01:10.553556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64f1c85b10>]}
[0m20:01:10.557761 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m20:01:10.570346 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m20:01:10.732000 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 3 files changed.
[0m20:01:10.732681 [debug] [MainThread]: Partial parsing: deleted file: mta://models/fare_class_per_station.sql
[0m20:01:10.733195 [debug] [MainThread]: Partial parsing: updated file: mta://models/daily_ridership.sql
[0m20:01:10.733506 [debug] [MainThread]: Partial parsing: updated file: mta://models/subway_station_stats.sql
[0m20:01:10.733800 [debug] [MainThread]: Partial parsing: updated file: mta://models/weekly_riders_per_station.sql
[0m20:01:10.973032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64e99b8130>]}
[0m20:01:11.076854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64e9bd6140>]}
[0m20:01:11.077475 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m20:01:11.077845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64e9c623e0>]}
[0m20:01:11.079889 [info ] [MainThread]: 
[0m20:01:11.080485 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m20:01:11.085178 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m20:01:11.251276 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m20:01:11.251669 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m20:01:11.251978 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:11.318549 [debug] [ThreadPool]: SQL status: OK in 0.066 seconds
[0m20:01:11.319725 [debug] [ThreadPool]: On list_mtastats: Close
[0m20:01:11.322254 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m20:01:11.323479 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m20:01:11.329094 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:01:11.329479 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m20:01:11.329771 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:01:11.355542 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m20:01:11.356770 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:01:11.357074 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m20:01:11.357618 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:01:11.357927 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:01:11.358286 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m20:01:11.358743 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:01:11.359325 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m20:01:11.359563 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:01:11.359807 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m20:01:11.360216 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:01:11.360460 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m20:01:11.363433 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m20:01:11.367759 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m20:01:11.368042 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m20:01:11.368288 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:01:11.376051 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m20:01:11.376410 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m20:01:11.376684 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m20:01:11.411026 [debug] [ThreadPool]: SQL status: OK in 0.034 seconds
[0m20:01:11.412395 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m20:01:11.414265 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m20:01:11.414719 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m20:01:11.417751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64f6761870>]}
[0m20:01:11.418109 [debug] [MainThread]: Using duckdb connection "master"
[0m20:01:11.418345 [debug] [MainThread]: On master: BEGIN
[0m20:01:11.418539 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:01:11.428404 [debug] [MainThread]: SQL status: OK in 0.010 seconds
[0m20:01:11.429048 [debug] [MainThread]: On master: COMMIT
[0m20:01:11.429440 [debug] [MainThread]: Using duckdb connection "master"
[0m20:01:11.429717 [debug] [MainThread]: On master: COMMIT
[0m20:01:11.430220 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m20:01:11.430551 [debug] [MainThread]: On master: Close
[0m20:01:11.432601 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m20:01:11.433068 [info ] [MainThread]: 
[0m20:01:11.443065 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m20:01:11.443925 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m20:01:11.445384 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m20:01:11.446802 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m20:01:11.444981 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m20:01:11.447333 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m20:01:11.447778 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m20:01:11.446135 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m20:01:11.448425 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m20:01:11.448880 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m20:01:11.449420 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m20:01:11.450596 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m20:01:11.451292 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m20:01:11.451888 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m20:01:11.453302 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m20:01:11.454000 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m20:01:11.454834 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m20:01:11.455410 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m20:01:11.456148 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m20:01:11.456618 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m20:01:11.457184 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m20:01:11.458086 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m20:01:11.458841 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m20:01:11.459530 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m20:01:11.460165 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m20:01:11.460670 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m20:01:11.461259 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m20:01:11.461857 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m20:01:11.462441 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m20:01:11.462962 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m20:01:11.463680 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m20:01:11.464306 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m20:01:11.464950 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m20:01:11.465466 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m20:01:11.466232 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m20:01:11.467134 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m20:01:11.467893 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m20:01:11.468738 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m20:01:11.469962 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m20:01:11.476990 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m20:01:11.481651 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m20:01:11.482520 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m20:01:11.485750 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m20:01:11.487667 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m20:01:11.488495 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m20:01:11.489150 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m20:01:11.489721 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m20:01:11.490622 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m20:01:11.491111 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m20:01:11.491764 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m20:01:11.496907 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m20:01:11.497834 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m20:01:11.498377 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m20:01:11.499118 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m20:01:11.500000 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m20:01:11.500845 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m20:01:11.501607 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m20:01:11.502217 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m20:01:11.504759 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m20:01:11.505158 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m20:01:11.508451 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m20:01:11.508978 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m20:01:11.516103 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m20:01:11.516618 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m20:01:11.519331 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m20:01:11.522179 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m20:01:11.522837 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m20:01:11.525919 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m20:01:11.528850 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m20:01:11.532680 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m20:01:11.536127 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m20:01:11.565167 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m20:01:11.565871 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m20:01:11.566557 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m20:01:11.570820 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m20:01:11.573964 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m20:01:11.575340 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m20:01:11.575870 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m20:01:11.578428 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m20:01:11.580238 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m20:01:11.580945 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m20:01:11.582381 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m20:01:11.583255 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m20:01:11.584484 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m20:01:11.584995 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m20:01:11.587747 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m20:01:11.588283 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:11.591772 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m20:01:11.594730 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m20:01:11.595428 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:11.596370 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m20:01:11.599771 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m20:01:11.600281 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m20:01:11.600766 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:11.604017 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m20:01:11.607988 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m20:01:11.611911 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m20:01:11.614649 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m20:01:11.618193 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m20:01:11.618857 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m20:01:11.620029 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:11.620729 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:01:11.621161 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:01:11.621593 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m20:01:11.624255 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m20:01:11.628114 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m20:01:11.628748 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m20:01:11.629443 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:11.631902 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:01:11.632930 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:11.633312 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:11.634343 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:01:11.635004 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m20:01:11.635784 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:01:11.636245 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m20:01:11.636665 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:01:11.637149 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m20:01:11.637750 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:01:11.638789 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:01:11.639593 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m20:01:11.640150 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:01:11.640672 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m20:01:11.641111 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:01:11.641540 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m20:01:11.641932 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m20:01:11.666390 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m20:01:11.667356 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m20:01:11.668087 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:01:11.668613 [debug] [Thread-1 (]: SQL status: OK in 0.034 seconds
[0m20:01:11.669134 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m20:01:11.669636 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m20:01:11.670600 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m20:01:11.672355 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m20:01:11.672816 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m20:01:11.673277 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m20:01:11.673716 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m20:01:11.674131 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m20:01:11.674887 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m20:01:11.675509 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:11.675939 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m20:01:11.676658 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m20:01:11.677290 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m20:01:11.678172 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m20:01:11.683417 [debug] [Thread-2 (]: SQL status: OK in 0.046 seconds
[0m20:01:11.683887 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:11.684291 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m20:01:11.721023 [debug] [Thread-3 (]: SQL status: OK in 0.082 seconds
[0m20:01:11.721554 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:11.721901 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m20:01:11.756336 [debug] [Thread-5 (]: SQL status: OK in 0.090 seconds
[0m20:01:11.756981 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:11.757444 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m20:01:11.758706 [debug] [Thread-6 (]: SQL status: OK in 0.091 seconds
[0m20:01:11.759275 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:01:11.760025 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m20:01:11.762740 [debug] [Thread-4 (]: SQL status: OK in 0.093 seconds
[0m20:01:11.763133 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:01:11.763568 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      WITH ridership_data AS (
    SELECT  
        DATE_TRUNC('week', date) AS week_start,
        SUM(subways_total_ridership) AS ridership,
        'Subway' AS transport_type,
        AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(buses_total_ridership) AS ridership,
        'Buses' AS transport_type,
        AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(lirr_total_ridership) AS ridership,
        'LIRR' AS transport_type,
        AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(metro_north_total_ridership) AS ridership,
        'Metro North' AS transport_type,
        AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(access_a_ride_total_trips) AS ridership,
        'Access-A-Ride' AS transport_type,
        AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(bridges_tunnels_total_traffic) AS ridership,
        'Bridges and Tunnels' AS transport_type,
        AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(staten_island_railway_total_ridership) AS ridership,
        'Staten Island Railway' AS transport_type,
        AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type
),
weather_data AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    rd.week_start, 
    rd.transport_type,
    rd.ridership,
    rd.avg_pct_pre_pandemic,
    wd.avg_weekly_temperature,
    wd.total_weekly_precipitation
FROM 
    ridership_data rd
LEFT JOIN 
    weather_data wd
ON 
    rd.week_start = wd.week_start
ORDER BY 
    rd.week_start, rd.transport_type;
    );
  
  
[0m20:01:11.780941 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      WITH ridership_data AS (
    SELECT  
        DATE_TRUNC('week', date) AS week_start,
        SUM(subways_total_ridership) AS ridership,
        'Subway' AS transport_type,
        AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(buses_total_ridership) AS ridership,
        'Buses' AS transport_type,
        AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(lirr_total_ridership) AS ridership,
        'LIRR' AS transport_type,
        AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(metro_north_total_ridership) AS ridership,
        'Metro North' AS transport_type,
        AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(access_a_ride_total_trips) AS ridership,
        'Access-A-Ride' AS transport_type,
        AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(bridges_tunnels_total_traffic) AS ridership,
        'Bridges and Tunnels' AS transport_type,
        AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(staten_island_railway_total_ridership) AS ridership,
        'Staten Island Railway' AS transport_type,
        AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type
),
weather_data AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    rd.week_start, 
    rd.transport_type,
    rd.ridership,
    rd.avg_pct_pre_pandemic,
    wd.avg_weekly_temperature,
    wd.total_weekly_precipitation
FROM 
    ridership_data rd
LEFT JOIN 
    weather_data wd
ON 
    rd.week_start = wd.week_start
ORDER BY 
    rd.week_start, rd.transport_type;
    );
  
  
[0m20:01:11.781924 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m20:01:11.782782 [debug] [Thread-4 (]: On model.mta.daily_ridership: ROLLBACK
[0m20:01:11.787715 [debug] [Thread-7 (]: SQL status: OK in 0.117 seconds
[0m20:01:11.789559 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:11.793494 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m20:01:11.812737 [debug] [Thread-11 ]: SQL status: OK in 0.140 seconds
[0m20:01:11.817868 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:01:11.819241 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m20:01:11.823443 [debug] [Thread-9 (]: SQL status: OK in 0.150 seconds
[0m20:01:11.825274 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:11.827811 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m20:01:11.845674 [debug] [Thread-8 (]: SQL status: OK in 0.171 seconds
[0m20:01:11.847721 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:11.850746 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m20:01:11.870283 [debug] [Thread-12 ]: SQL status: OK in 0.195 seconds
[0m20:01:11.874242 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:01:11.876936 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers
    FROM 
        mta_hourly_subway_socrata
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024
    FROM 
        mta_hourly_subway_socrata
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    -- Calculate the weekend transfer percentage change compared to the weekday
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m20:01:11.896122 [debug] [Thread-10 ]: SQL status: OK in 0.220 seconds
[0m20:01:11.907750 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:01:11.910268 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m20:01:11.919551 [debug] [Thread-4 (]: Failed to rollback 'model.mta.daily_ridership'
[0m20:01:11.922254 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m20:01:11.933748 [debug] [Thread-4 (]: Runtime Error in model daily_ridership (models/daily_ridership.sql)
  Parser Error: syntax error at or near ";"
[0m20:01:11.934575 [debug] [Thread-14 ]: SQL status: OK in 0.258 seconds
[0m20:01:11.948192 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64f5b2c610>]}
[0m20:01:11.948808 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:01:11.949800 [error] [Thread-4 (]: 4 of 14 ERROR creating sql table model main.daily_ridership .................... [[31mERROR[0m in 0.47s]
[0m20:01:11.950872 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership,
        MIN(latitude) AS latitude,  -- Assuming latitude is the same for each station complex, use MIN() or MAX()
        MIN(longitude) AS longitude  -- Assuming longitude is the same for each station complex, use MIN() or MAX()
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    wr.latitude,
    wr.longitude,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start;
    );
  
  
[0m20:01:11.951826 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m20:01:11.952638 [debug] [Thread-14 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership,
        MIN(latitude) AS latitude,  -- Assuming latitude is the same for each station complex, use MIN() or MAX()
        MIN(longitude) AS longitude  -- Assuming longitude is the same for each station complex, use MIN() or MAX()
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    wr.latitude,
    wr.longitude,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start;
    );
  
  
[0m20:01:11.957319 [debug] [Thread-14 ]: DuckDB adapter: Rolling back transaction.
[0m20:01:11.958184 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: ROLLBACK
[0m20:01:11.959077 [debug] [Thread-13 ]: SQL status: OK in 0.282 seconds
[0m20:01:11.959769 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:01:11.960685 [debug] [Thread-14 ]: Failed to rollback 'model.mta.weekly_riders_per_station'
[0m20:01:11.961196 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m20:01:11.961798 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m20:01:11.963036 [debug] [Thread-14 ]: Runtime Error in model weekly_riders_per_station (models/weekly_riders_per_station.sql)
  Parser Error: syntax error at or near ";"
[0m20:01:11.963541 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64c87564d0>]}
[0m20:01:11.964256 [error] [Thread-14 ]: 14 of 14 ERROR creating sql table model main.weekly_riders_per_station ......... [[31mERROR[0m in 0.47s]
[0m20:01:11.965009 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m20:01:12.250302 [debug] [Thread-5 (]: SQL status: OK in 0.492 seconds
[0m20:01:12.291155 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:12.292175 [debug] [Thread-8 (]: SQL status: OK in 0.440 seconds
[0m20:01:12.293636 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m20:01:12.308766 [debug] [Thread-5 (]: SQL status: OK in 0.003 seconds
[0m20:01:12.303866 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:12.332605 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m20:01:12.347975 [debug] [Thread-7 (]: SQL status: OK in 0.549 seconds
[0m20:01:12.372572 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:12.383710 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m20:01:12.384512 [debug] [Thread-8 (]: SQL status: OK in 0.037 seconds
[0m20:01:12.382679 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m20:01:12.387206 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m20:01:12.388958 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:12.388248 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:12.390115 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m20:01:12.391909 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m20:01:12.394026 [debug] [Thread-7 (]: SQL status: OK in 0.009 seconds
[0m20:01:12.397630 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m20:01:12.398250 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:12.398763 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m20:01:12.409063 [debug] [Thread-8 (]: SQL status: OK in 0.014 seconds
[0m20:01:12.427489 [debug] [Thread-5 (]: SQL status: OK in 0.032 seconds
[0m20:01:12.434801 [debug] [Thread-7 (]: SQL status: OK in 0.035 seconds
[0m20:01:12.455917 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:12.469717 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m20:01:12.465828 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:12.473080 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m20:01:12.474695 [debug] [Thread-7 (]: SQL status: OK in 0.001 seconds
[0m20:01:12.468786 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:12.466869 [debug] [Thread-9 (]: SQL status: OK in 0.638 seconds
[0m20:01:12.480718 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m20:01:12.506103 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64e9a13e20>]}
[0m20:01:12.495352 [debug] [Thread-5 (]: SQL status: OK in 0.024 seconds
[0m20:01:12.496916 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:12.534480 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m20:01:12.524387 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 1.04s]
[0m20:01:12.481667 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m20:01:12.541353 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m20:01:12.552951 [debug] [Thread-9 (]: SQL status: OK in 0.013 seconds
[0m20:01:12.565023 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m20:01:12.570110 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64e9a13cd0>]}
[0m20:01:12.572886 [debug] [Thread-2 (]: SQL status: OK in 0.888 seconds
[0m20:01:12.573720 [debug] [Thread-8 (]: SQL status: OK in 0.006 seconds
[0m20:01:12.584266 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m20:01:12.616726 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:12.607166 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:12.609441 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m20:01:12.620276 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64e9a11d20>]}
[0m20:01:12.622422 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 1.15s]
[0m20:01:12.618868 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m20:01:12.630531 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m20:01:12.617901 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m20:01:12.625496 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m20:01:12.598138 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 1.11s]
[0m20:01:12.642015 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m20:01:12.636970 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m20:01:12.648602 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:12.651088 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m20:01:12.662593 [debug] [Thread-9 (]: SQL status: OK in 0.022 seconds
[0m20:01:12.667523 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:12.670701 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m20:01:12.689477 [debug] [Thread-2 (]: SQL status: OK in 0.036 seconds
[0m20:01:12.695604 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:12.696179 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m20:01:12.696899 [debug] [Thread-9 (]: SQL status: OK in 0.025 seconds
[0m20:01:12.698462 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m20:01:12.699226 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64e9d79a50>]}
[0m20:01:12.700031 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 1.22s]
[0m20:01:12.700685 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m20:01:12.715426 [debug] [Thread-2 (]: SQL status: OK in 0.019 seconds
[0m20:01:12.717979 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m20:01:12.719146 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64cab85660>]}
[0m20:01:12.720329 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 1.27s]
[0m20:01:12.721243 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m20:01:16.282988 [debug] [Thread-3 (]: SQL status: OK in 4.533 seconds
[0m20:01:16.451674 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:16.453879 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m20:01:16.470935 [debug] [Thread-3 (]: SQL status: OK in 0.016 seconds
[0m20:01:16.477371 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m20:01:16.478565 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:16.479592 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m20:01:16.534943 [debug] [Thread-3 (]: SQL status: OK in 0.043 seconds
[0m20:01:16.616087 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:16.635231 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m20:01:16.637516 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m20:01:16.647181 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m20:01:16.671355 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64e9a12080>]}
[0m20:01:16.676573 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 5.20s]
[0m20:01:16.697890 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m20:01:19.785585 [debug] [Thread-1 (]: SQL status: OK in 8.106 seconds
[0m20:01:19.798135 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:19.799155 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m20:01:19.805737 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m20:01:19.813285 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m20:01:19.814183 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:19.814947 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m20:01:19.855719 [debug] [Thread-1 (]: SQL status: OK in 0.040 seconds
[0m20:01:19.860712 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:19.861532 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m20:01:19.862420 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m20:01:19.865119 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m20:01:19.866472 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8498e2c-84dc-4198-8fea-87ebad97c8ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f64e9a18fa0>]}
[0m20:01:19.867637 [info ] [Thread-1 (]: 1 of 14 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 8.41s]
[0m20:01:19.868926 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m20:01:27.736938 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.fare_class_boro. Details: Connection(type='duckdb', name='model.mta.fare_class_boro', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7f64e9d7a3b0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m20:01:27.740865 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.fare_class_boro
[0m20:01:27.742091 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.omny_adoption_by_station. Details: Connection(type='duckdb', name='model.mta.omny_adoption_by_station', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7f64c876a3b0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m20:01:27.743095 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.omny_adoption_by_station
[0m20:01:27.750196 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.omny_adoption_increase. Details: Connection(type='duckdb', name='model.mta.omny_adoption_increase', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7f64f1cdbee0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m20:01:27.751674 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.omny_adoption_increase
[0m20:01:27.752521 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.subway_station_stats. Details: Connection(type='duckdb', name='model.mta.subway_station_stats', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7f64f1cd9990>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m20:01:27.753391 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.subway_station_stats
[0m20:01:27.754515 [debug] [MainThread]: DuckDB adapter: cancelling query on connection model.mta.total_riders_per_station. Details: Connection(type='duckdb', name='model.mta.total_riders_per_station', state=<ConnectionState.OPEN: 'open'>, transaction_open=True, _handle=<dbt.adapters.duckdb.environments.local.DuckDBConnectionWrapper object at 0x7f64c8769ea0>, _credentials=DuckDBCredentials(database='mtastats', schema='main', path='/home/christianocean/mta/mta/mtastats/sources/mta/mtastats.duckdb', config_options=None, extensions=None, settings={'enable_object_cache': True, 'enable_http_metadata_cache': True}, secrets=[], external_root='.', use_credential_provider=None, attach=None, filesystems=None, remote=None, plugins=None, disable_transactions=False, keep_open=False, module_paths=None, retries=None))
[0m20:01:27.755538 [debug] [MainThread]: DuckDB adapter: query cancelled on connection model.mta.total_riders_per_station
[0m20:01:27.756883 [error] [MainThread]: CANCEL query model.mta.expense_type_per_year ................................... [[31mCANCEL[0m]
[0m20:01:27.757831 [error] [MainThread]: CANCEL query model.mta.bond_payment_info ....................................... [[31mCANCEL[0m]
[0m20:01:27.758352 [error] [MainThread]: CANCEL query model.mta.avg_riders_per_day ...................................... [[31mCANCEL[0m]
[0m20:01:32.719342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc22821b4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2268c65c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2268c66e0>]}


============================== 20:01:32.721778 | 9e4a450a-5584-4b96-90e4-3dc056dcbcec ==============================
[0m20:01:32.721778 [info ] [MainThread]: Running with dbt=1.8.7
[0m20:01:32.722212 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:01:32.912672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc226817c40>]}
[0m20:01:32.972321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2267fa6b0>]}
[0m20:01:32.976064 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m20:01:32.985559 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m20:01:33.080499 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m20:01:33.081187 [debug] [MainThread]: Partial parsing: updated file: mta://models/daily_ridership.sql
[0m20:01:33.081557 [debug] [MainThread]: Partial parsing: updated file: mta://models/weekly_riders_per_station.sql
[0m20:01:33.323619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b3e01c0>]}
[0m20:01:33.394084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b3f6b60>]}
[0m20:01:33.394657 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m20:01:33.395046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b3f7490>]}
[0m20:01:33.396674 [info ] [MainThread]: 
[0m20:01:33.397218 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m20:01:33.401642 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m20:01:33.491356 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m20:01:33.491816 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m20:01:33.492109 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:33.506010 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m20:01:33.507452 [debug] [ThreadPool]: On list_mtastats: Close
[0m20:01:33.510377 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m20:01:33.510927 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m20:01:33.516155 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:01:33.516488 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m20:01:33.516773 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:01:33.523445 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m20:01:33.524806 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:01:33.525131 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m20:01:33.525633 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:01:33.525925 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:01:33.526170 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m20:01:33.526700 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:01:33.527378 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m20:01:33.527620 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:01:33.527871 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m20:01:33.528295 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:01:33.528582 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m20:01:33.533259 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m20:01:33.539394 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m20:01:33.539904 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m20:01:33.540234 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:01:33.547696 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m20:01:33.548222 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m20:01:33.548514 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m20:01:33.568701 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m20:01:33.570636 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m20:01:33.571311 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m20:01:33.571655 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m20:01:33.575515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc228506d70>]}
[0m20:01:33.576016 [debug] [MainThread]: Using duckdb connection "master"
[0m20:01:33.576329 [debug] [MainThread]: On master: BEGIN
[0m20:01:33.576593 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:01:33.584157 [debug] [MainThread]: SQL status: OK in 0.007 seconds
[0m20:01:33.584762 [debug] [MainThread]: On master: COMMIT
[0m20:01:33.585052 [debug] [MainThread]: Using duckdb connection "master"
[0m20:01:33.585281 [debug] [MainThread]: On master: COMMIT
[0m20:01:33.585716 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m20:01:33.585968 [debug] [MainThread]: On master: Close
[0m20:01:33.588412 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m20:01:33.588814 [info ] [MainThread]: 
[0m20:01:33.597235 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m20:01:33.597803 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m20:01:33.598453 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m20:01:33.599047 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m20:01:33.599514 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m20:01:33.600032 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m20:01:33.600473 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m20:01:33.600978 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m20:01:33.601495 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m20:01:33.602000 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m20:01:33.602454 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m20:01:33.603056 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m20:01:33.603545 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m20:01:33.604161 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m20:01:33.605162 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m20:01:33.605853 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m20:01:33.606558 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m20:01:33.607743 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m20:01:33.608817 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m20:01:33.609656 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m20:01:33.610428 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m20:01:33.611179 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m20:01:33.611969 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m20:01:33.612665 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m20:01:33.613777 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m20:01:33.614580 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m20:01:33.615482 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m20:01:33.616402 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m20:01:33.617045 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m20:01:33.618519 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m20:01:33.617674 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m20:01:33.619151 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m20:01:33.620388 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m20:01:33.620939 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m20:01:33.621569 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m20:01:33.622367 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m20:01:33.623303 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m20:01:33.629745 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m20:01:33.632748 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m20:01:33.634057 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m20:01:33.634803 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m20:01:33.635402 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m20:01:33.635977 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m20:01:33.636442 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m20:01:33.636951 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m20:01:33.637527 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m20:01:33.638006 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m20:01:33.640891 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m20:01:33.641685 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m20:01:33.642350 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m20:01:33.642930 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m20:01:33.643411 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m20:01:33.644094 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m20:01:33.648052 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m20:01:33.652020 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m20:01:33.652665 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m20:01:33.653041 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m20:01:33.655478 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m20:01:33.655962 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m20:01:33.656395 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m20:01:33.661182 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m20:01:33.662037 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m20:01:33.662611 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m20:01:33.663217 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m20:01:33.666025 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m20:01:33.668358 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m20:01:33.670427 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m20:01:33.671472 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m20:01:33.673982 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m20:01:33.674543 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m20:01:33.693762 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m20:01:33.696517 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m20:01:33.704545 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m20:01:33.705124 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m20:01:33.708072 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m20:01:33.710322 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m20:01:33.710785 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m20:01:33.713376 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m20:01:33.714661 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m20:01:33.715217 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m20:01:33.717946 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m20:01:33.718482 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m20:01:33.721409 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m20:01:33.721849 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m20:01:33.722484 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m20:01:33.723881 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m20:01:33.724546 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:33.728069 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m20:01:33.728737 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m20:01:33.731264 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m20:01:33.734212 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m20:01:33.734770 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m20:01:33.735320 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:33.737961 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m20:01:33.738623 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:33.742059 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m20:01:33.745749 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m20:01:33.749352 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m20:01:33.749820 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:01:33.752709 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m20:01:33.753246 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m20:01:33.754023 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:33.757032 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m20:01:33.758317 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:33.762862 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m20:01:33.763687 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:01:33.764158 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m20:01:33.764944 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m20:01:33.765575 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:33.766872 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m20:01:33.767795 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:33.768299 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:01:33.769302 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:01:33.769712 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:01:33.770126 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:01:33.770536 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m20:01:33.771139 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m20:01:33.771612 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:01:33.772213 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m20:01:33.772659 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:01:33.773069 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:01:33.773586 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:01:33.774339 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m20:01:33.774934 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m20:01:33.775433 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m20:01:33.775978 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m20:01:33.776567 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m20:01:33.784208 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m20:01:33.785094 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m20:01:33.785886 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m20:01:33.786332 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m20:01:33.786808 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m20:01:33.787214 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:01:33.787672 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m20:01:33.788866 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m20:01:33.789812 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m20:01:33.790568 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m20:01:33.791137 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m20:01:33.792035 [debug] [Thread-2 (]: SQL status: OK in 0.019 seconds
[0m20:01:33.792526 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m20:01:33.793633 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:33.794073 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m20:01:33.795123 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m20:01:33.797987 [debug] [Thread-3 (]: SQL status: OK in 0.024 seconds
[0m20:01:33.798668 [debug] [Thread-4 (]: SQL status: OK in 0.024 seconds
[0m20:01:33.799345 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:33.800372 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m20:01:33.802209 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:33.802950 [debug] [Thread-5 (]: SQL status: OK in 0.018 seconds
[0m20:01:33.803485 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:01:33.804138 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m20:01:33.805000 [debug] [Thread-8 (]: SQL status: OK in 0.019 seconds
[0m20:01:33.806167 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m20:01:33.807735 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:33.808333 [debug] [Thread-6 (]: SQL status: OK in 0.021 seconds
[0m20:01:33.809128 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      WITH ridership_data AS (
    SELECT  
        DATE_TRUNC('week', date) AS week_start,
        SUM(subways_total_ridership) AS ridership,
        'Subway' AS transport_type,
        AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(buses_total_ridership) AS ridership,
        'Buses' AS transport_type,
        AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(lirr_total_ridership) AS ridership,
        'LIRR' AS transport_type,
        AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(metro_north_total_ridership) AS ridership,
        'Metro North' AS transport_type,
        AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(access_a_ride_total_trips) AS ridership,
        'Access-A-Ride' AS transport_type,
        AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(bridges_tunnels_total_traffic) AS ridership,
        'Bridges and Tunnels' AS transport_type,
        AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(staten_island_railway_total_ridership) AS ridership,
        'Staten Island Railway' AS transport_type,
        AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type
),
weather_data AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    rd.week_start, 
    rd.transport_type,
    rd.ridership,
    rd.avg_pct_pre_pandemic,
    wd.avg_weekly_temperature,
    wd.total_weekly_precipitation
FROM 
    ridership_data rd
LEFT JOIN 
    weather_data wd
ON 
    rd.week_start = wd.week_start
ORDER BY 
    rd.week_start, rd.transport_type
    );
  
  
[0m20:01:33.810796 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:33.812360 [debug] [Thread-7 (]: SQL status: OK in 0.023 seconds
[0m20:01:33.812957 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m20:01:33.813546 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:01:33.814653 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m20:01:33.815648 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:33.817675 [debug] [Thread-9 (]: SQL status: OK in 0.028 seconds
[0m20:01:33.818408 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m20:01:33.819415 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m20:01:33.820338 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:33.830428 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m20:01:33.831137 [debug] [Thread-11 ]: SQL status: OK in 0.041 seconds
[0m20:01:33.833321 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:01:33.833841 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m20:01:33.843504 [debug] [Thread-10 ]: SQL status: OK in 0.052 seconds
[0m20:01:33.844049 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:01:33.844502 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m20:01:33.855792 [debug] [Thread-12 ]: SQL status: OK in 0.063 seconds
[0m20:01:33.857600 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:01:33.858131 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers
    FROM 
        mta_hourly_subway_socrata
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024
    FROM 
        mta_hourly_subway_socrata
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    -- Calculate the weekend transfer percentage change compared to the weekday
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m20:01:33.875732 [debug] [Thread-13 ]: SQL status: OK in 0.082 seconds
[0m20:01:33.876384 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:01:33.876805 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m20:01:33.882853 [debug] [Thread-14 ]: SQL status: OK in 0.088 seconds
[0m20:01:33.883389 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:01:33.883760 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership,
        MIN(latitude) AS latitude,  -- Assuming latitude is the same for each station complex, use MIN() or MAX()
        MIN(longitude) AS longitude  -- Assuming longitude is the same for each station complex, use MIN() or MAX()
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    wr.latitude,
    wr.longitude,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
ORDER BY 
    wr.station_complex, 
    wr.week_start
    );
  
  
[0m20:01:34.107599 [debug] [Thread-5 (]: SQL status: OK in 0.291 seconds
[0m20:01:34.138562 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:34.139625 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m20:01:34.141123 [debug] [Thread-5 (]: SQL status: OK in 0.001 seconds
[0m20:01:34.210974 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m20:01:34.212827 [debug] [Thread-8 (]: SQL status: OK in 0.394 seconds
[0m20:01:34.214258 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:34.223180 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:34.224725 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m20:01:34.226282 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m20:01:34.237156 [debug] [Thread-8 (]: SQL status: OK in 0.007 seconds
[0m20:01:34.240596 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m20:01:34.241618 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:34.242533 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m20:01:34.244610 [debug] [Thread-5 (]: SQL status: OK in 0.014 seconds
[0m20:01:34.255683 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:01:34.256925 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m20:01:34.258565 [debug] [Thread-9 (]: SQL status: OK in 0.427 seconds
[0m20:01:34.265963 [debug] [Thread-5 (]: SQL status: OK in 0.008 seconds
[0m20:01:34.267469 [debug] [Thread-8 (]: SQL status: OK in 0.024 seconds
[0m20:01:34.270515 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:34.275915 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m20:01:34.299580 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:01:34.300898 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m20:01:34.318100 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m20:01:34.331564 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b451cf0>]}
[0m20:01:34.333978 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 0.70s]
[0m20:01:34.335368 [debug] [Thread-9 (]: SQL status: OK in 0.016 seconds
[0m20:01:34.336281 [debug] [Thread-8 (]: SQL status: OK in 0.011 seconds
[0m20:01:34.338244 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m20:01:34.343949 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m20:01:34.349144 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m20:01:34.350855 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:34.352488 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m20:01:34.356157 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc226889210>]}
[0m20:01:34.359991 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 0.72s]
[0m20:01:34.397401 [debug] [Thread-9 (]: SQL status: OK in 0.038 seconds
[0m20:01:34.417058 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:01:34.419339 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m20:01:34.443456 [debug] [Thread-7 (]: SQL status: OK in 0.619 seconds
[0m20:01:34.450111 [debug] [Thread-4 (]: SQL status: OK in 0.636 seconds
[0m20:01:34.451220 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m20:01:34.461850 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:34.478845 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m20:01:34.481019 [debug] [Thread-9 (]: SQL status: OK in 0.002 seconds
[0m20:01:34.484055 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m20:01:34.485429 [debug] [Thread-7 (]: SQL status: OK in 0.005 seconds
[0m20:01:34.469446 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:01:34.513816 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m20:01:34.512465 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m20:01:34.510979 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b453d90>]}
[0m20:01:34.589573 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 0.88s]
[0m20:01:34.591365 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m20:01:34.587751 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:34.561145 [debug] [Thread-4 (]: SQL status: OK in 0.025 seconds
[0m20:01:34.596893 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m20:01:34.592685 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m20:01:34.573865 [debug] [Thread-2 (]: SQL status: OK in 0.763 seconds
[0m20:01:34.598770 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:01:34.615093 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m20:01:34.621797 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:34.623136 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m20:01:34.626434 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m20:01:34.628144 [debug] [Thread-7 (]: SQL status: OK in 0.024 seconds
[0m20:01:34.632483 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m20:01:34.634535 [debug] [Thread-4 (]: SQL status: OK in 0.016 seconds
[0m20:01:34.661674 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:01:34.663104 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:34.699044 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m20:01:34.672594 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:01:34.701936 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m20:01:34.674648 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m20:01:34.718094 [debug] [Thread-2 (]: SQL status: OK in 0.018 seconds
[0m20:01:34.722983 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:01:34.724005 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m20:01:34.736041 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m20:01:34.739983 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m20:01:34.741186 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc226907a30>]}
[0m20:01:34.756273 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 1.13s]
[0m20:01:34.757702 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m20:01:34.737130 [debug] [Thread-7 (]: SQL status: OK in 0.022 seconds
[0m20:01:34.734656 [debug] [Thread-4 (]: SQL status: OK in 0.032 seconds
[0m20:01:34.761421 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m20:01:34.765169 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m20:01:34.778565 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc128d29150>]}
[0m20:01:34.782066 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc128d537c0>]}
[0m20:01:34.786589 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 1.16s]
[0m20:01:34.843015 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m20:01:34.792394 [info ] [Thread-4 (]: 4 of 14 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 1.17s]
[0m20:01:34.845104 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m20:01:36.268809 [debug] [Thread-1 (]: SQL status: OK in 2.463 seconds
[0m20:01:36.277145 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:36.278902 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m20:01:36.281750 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m20:01:36.307697 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m20:01:36.324473 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:36.334753 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m20:01:36.372597 [debug] [Thread-1 (]: SQL status: OK in 0.037 seconds
[0m20:01:36.381584 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:01:36.395164 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m20:01:36.397555 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m20:01:36.400532 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m20:01:36.408883 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b452bc0>]}
[0m20:01:36.410487 [info ] [Thread-1 (]: 1 of 14 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 2.81s]
[0m20:01:36.413172 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m20:01:39.705649 [debug] [Thread-3 (]: SQL status: OK in 5.894 seconds
[0m20:01:39.713961 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:39.721055 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m20:01:39.722883 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m20:01:39.736178 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m20:01:39.737466 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:39.738272 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m20:01:39.745157 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m20:01:39.752673 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:01:39.753657 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m20:01:39.755375 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m20:01:39.758488 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m20:01:39.759779 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b452770>]}
[0m20:01:39.760998 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 6.14s]
[0m20:01:39.762100 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m20:01:44.448865 [debug] [Thread-13 ]: SQL status: OK in 10.571 seconds
[0m20:01:44.475136 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:01:44.476441 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m20:01:44.478321 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m20:01:44.482142 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m20:01:44.483178 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:01:44.484191 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m20:01:44.500132 [debug] [Thread-13 ]: SQL status: OK in 0.015 seconds
[0m20:01:44.510752 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:01:44.512291 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m20:01:44.535779 [debug] [Thread-13 ]: SQL status: OK in 0.001 seconds
[0m20:01:44.539340 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: Close
[0m20:01:44.540788 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b454820>]}
[0m20:01:44.542265 [info ] [Thread-13 ]: 13 of 14 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 10.90s]
[0m20:01:44.544276 [debug] [Thread-13 ]: Finished running node model.mta.total_riders_per_station
[0m20:01:44.604281 [debug] [Thread-14 ]: SQL status: OK in 10.720 seconds
[0m20:01:44.610941 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:01:44.612054 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m20:01:44.613521 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m20:01:44.616810 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m20:01:44.618019 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:01:44.626651 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m20:01:44.686116 [debug] [Thread-14 ]: SQL status: OK in 0.058 seconds
[0m20:01:44.691525 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:01:44.692454 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m20:01:44.693658 [debug] [Thread-14 ]: SQL status: OK in 0.000 seconds
[0m20:01:44.696373 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m20:01:44.697628 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b454dc0>]}
[0m20:01:44.715420 [info ] [Thread-14 ]: 14 of 14 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 11.06s]
[0m20:01:44.716924 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m20:01:45.356775 [debug] [Thread-10 ]: SQL status: OK in 11.512 seconds
[0m20:01:45.366742 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:01:45.367899 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m20:01:45.375833 [debug] [Thread-10 ]: SQL status: OK in 0.004 seconds
[0m20:01:45.378610 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m20:01:45.379269 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:01:45.380027 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m20:01:45.387831 [debug] [Thread-10 ]: SQL status: OK in 0.007 seconds
[0m20:01:45.393388 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:01:45.394327 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m20:01:45.396381 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m20:01:45.399445 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: Close
[0m20:01:45.400748 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b4513c0>]}
[0m20:01:45.402248 [info ] [Thread-10 ]: 10 of 14 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 11.76s]
[0m20:01:45.403477 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_by_station
[0m20:01:46.260102 [debug] [Thread-12 ]: SQL status: OK in 12.398 seconds
[0m20:01:46.267717 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:01:46.268910 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m20:01:46.270572 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m20:01:46.273845 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m20:01:46.274926 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:01:46.275764 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m20:01:46.286756 [debug] [Thread-12 ]: SQL status: OK in 0.010 seconds
[0m20:01:46.292054 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:01:46.293005 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m20:01:46.294456 [debug] [Thread-12 ]: SQL status: OK in 0.000 seconds
[0m20:01:46.296994 [debug] [Thread-12 ]: On model.mta.subway_station_stats: Close
[0m20:01:46.298144 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc21b4503d0>]}
[0m20:01:46.299539 [info ] [Thread-12 ]: 12 of 14 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 12.66s]
[0m20:01:46.301246 [debug] [Thread-12 ]: Finished running node model.mta.subway_station_stats
[0m20:01:47.196884 [debug] [Thread-6 (]: SQL status: OK in 13.376 seconds
[0m20:01:47.211214 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:01:47.212507 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m20:01:47.214148 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m20:01:47.217842 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m20:01:47.219174 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:01:47.220054 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m20:01:47.245895 [debug] [Thread-6 (]: SQL status: OK in 0.025 seconds
[0m20:01:47.256351 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:01:47.257639 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m20:01:47.275600 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m20:01:47.279041 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m20:01:47.281861 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc218688610>]}
[0m20:01:47.283452 [info ] [Thread-6 (]: 6 of 14 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 13.66s]
[0m20:01:47.285170 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m20:01:47.517774 [debug] [Thread-11 ]: SQL status: OK in 13.683 seconds
[0m20:01:47.521384 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:01:47.521919 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m20:01:47.522986 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m20:01:47.524806 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m20:01:47.525246 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:01:47.525617 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m20:01:47.530301 [debug] [Thread-11 ]: SQL status: OK in 0.004 seconds
[0m20:01:47.532953 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:01:47.533387 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m20:01:47.534156 [debug] [Thread-11 ]: SQL status: OK in 0.000 seconds
[0m20:01:47.535686 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: Close
[0m20:01:47.628696 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e4a450a-5584-4b96-90e4-3dc056dcbcec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc218557f40>]}
[0m20:01:47.629512 [info ] [Thread-11 ]: 11 of 14 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 13.99s]
[0m20:01:47.630115 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_increase
[0m20:01:47.634807 [debug] [MainThread]: Using duckdb connection "master"
[0m20:01:47.635271 [debug] [MainThread]: On master: BEGIN
[0m20:01:47.635580 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:01:47.643234 [debug] [MainThread]: SQL status: OK in 0.008 seconds
[0m20:01:47.643697 [debug] [MainThread]: On master: COMMIT
[0m20:01:47.643985 [debug] [MainThread]: Using duckdb connection "master"
[0m20:01:47.644220 [debug] [MainThread]: On master: COMMIT
[0m20:01:47.644781 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m20:01:47.645106 [debug] [MainThread]: On master: Close
[0m20:01:47.649988 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:01:47.650583 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m20:01:47.650876 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m20:01:47.651101 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m20:01:47.651323 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m20:01:47.651530 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m20:01:47.651749 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m20:01:47.651941 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m20:01:47.652147 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m20:01:47.652353 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m20:01:47.652549 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m20:01:47.652751 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m20:01:47.652949 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m20:01:47.653143 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m20:01:47.653337 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m20:01:47.653761 [info ] [MainThread]: 
[0m20:01:47.654123 [info ] [MainThread]: Finished running 14 table models in 0 hours 0 minutes and 14.26 seconds (14.26s).
[0m20:01:47.655765 [debug] [MainThread]: Command end result
[0m20:01:47.687221 [info ] [MainThread]: 
[0m20:01:47.687795 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:01:47.688142 [info ] [MainThread]: 
[0m20:01:47.688456 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m20:01:47.690471 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.01887, "process_user_time": 190.85478, "process_kernel_time": 9.0888, "process_mem_max_rss": "784080", "process_in_blocks": "5896", "process_out_blocks": "23072"}
[0m20:01:47.691107 [debug] [MainThread]: Command `dbt run` succeeded at 20:01:47.690995 after 15.02 seconds
[0m20:01:47.691474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc22821b4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2285f7b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc128d3e890>]}
[0m20:01:47.691895 [debug] [MainThread]: Flushing usage events
[0m20:14:45.215283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b82bbf400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b827943a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b81271ea0>]}


============================== 20:14:45.224768 | a945c766-a986-484c-b3a4-a1f1cdd46181 ==============================
[0m20:14:45.224768 [info ] [MainThread]: Running with dbt=1.8.7
[0m20:14:45.225326 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:14:45.470258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b812120e0>]}
[0m20:14:45.538644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b81eca800>]}
[0m20:14:45.546195 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m20:14:45.561300 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m20:14:45.700603 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m20:14:45.701460 [debug] [MainThread]: Partial parsing: updated file: mta://models/daily_ridership.sql
[0m20:14:45.701945 [debug] [MainThread]: Partial parsing: updated file: mta://models/subway_station_stats.sql
[0m20:14:45.702364 [debug] [MainThread]: Partial parsing: updated file: mta://models/weekly_riders_per_station.sql
[0m20:14:45.972608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75ce0130>]}
[0m20:14:46.061892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d0a650>]}
[0m20:14:46.062510 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m20:14:46.062897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d0a980>]}
[0m20:14:46.064853 [info ] [MainThread]: 
[0m20:14:46.065533 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m20:14:46.071346 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m20:14:46.204895 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m20:14:46.205642 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m20:14:46.206098 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:14:46.217288 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m20:14:46.218700 [debug] [ThreadPool]: On list_mtastats: Close
[0m20:14:46.222184 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m20:14:46.222873 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m20:14:46.229190 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:14:46.229730 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m20:14:46.230091 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:14:46.237457 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m20:14:46.238882 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:14:46.239258 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m20:14:46.239771 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:14:46.240078 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:14:46.240334 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m20:14:46.241990 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m20:14:46.243112 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m20:14:46.243508 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:14:46.243773 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m20:14:46.244258 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:14:46.244657 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m20:14:46.248337 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m20:14:46.255298 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m20:14:46.255843 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m20:14:46.256182 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:14:46.263711 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m20:14:46.264253 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m20:14:46.264701 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m20:14:46.288723 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m20:14:46.290511 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m20:14:46.291553 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m20:14:46.291939 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m20:14:46.296733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b82eaacb0>]}
[0m20:14:46.297332 [debug] [MainThread]: Using duckdb connection "master"
[0m20:14:46.297675 [debug] [MainThread]: On master: BEGIN
[0m20:14:46.297959 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:14:46.306452 [debug] [MainThread]: SQL status: OK in 0.008 seconds
[0m20:14:46.307031 [debug] [MainThread]: On master: COMMIT
[0m20:14:46.307383 [debug] [MainThread]: Using duckdb connection "master"
[0m20:14:46.307674 [debug] [MainThread]: On master: COMMIT
[0m20:14:46.308263 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m20:14:46.308629 [debug] [MainThread]: On master: Close
[0m20:14:46.311276 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m20:14:46.311835 [info ] [MainThread]: 
[0m20:14:46.323777 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m20:14:46.324922 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m20:14:46.325941 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m20:14:46.327516 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m20:14:46.328345 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m20:14:46.329248 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m20:14:46.330360 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m20:14:46.331240 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m20:14:46.332424 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m20:14:46.333822 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m20:14:46.334910 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m20:14:46.335738 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m20:14:46.337492 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m20:14:46.336701 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m20:14:46.339035 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m20:14:46.339930 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m20:14:46.340541 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m20:14:46.341313 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m20:14:46.342266 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m20:14:46.342845 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m20:14:46.343511 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m20:14:46.349829 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m20:14:46.353964 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m20:14:46.355017 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m20:14:46.356198 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m20:14:46.356984 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m20:14:46.357841 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m20:14:46.358629 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m20:14:46.359378 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m20:14:46.359963 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m20:14:46.360602 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m20:14:46.361395 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m20:14:46.362268 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m20:14:46.363207 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m20:14:46.364888 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m20:14:46.366098 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m20:14:46.367643 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m20:14:46.368823 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m20:14:46.370363 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m20:14:46.371419 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m20:14:46.372417 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m20:14:46.373013 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m20:14:46.373651 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m20:14:46.374418 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m20:14:46.375141 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m20:14:46.376026 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m20:14:46.379840 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m20:14:46.381662 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m20:14:46.383548 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m20:14:46.384590 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m20:14:46.385502 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m20:14:46.386559 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m20:14:46.387388 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m20:14:46.388089 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m20:14:46.397345 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m20:14:46.430979 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m20:14:46.431524 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m20:14:46.432379 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m20:14:46.436440 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m20:14:46.437325 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m20:14:46.438484 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m20:14:46.439127 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m20:14:46.439842 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m20:14:46.443506 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m20:14:46.444078 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m20:14:46.447998 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m20:14:46.450830 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m20:14:46.453568 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m20:14:46.457370 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m20:14:46.462405 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m20:14:46.463293 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m20:14:46.463913 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m20:14:46.468589 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m20:14:46.469320 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:14:46.469909 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m20:14:46.472913 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m20:14:46.475713 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m20:14:46.479290 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m20:14:46.480467 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m20:14:46.481543 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m20:14:46.481983 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m20:14:46.483497 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m20:14:46.486803 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m20:14:46.487239 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m20:14:46.487738 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m20:14:46.490671 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m20:14:46.491310 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m20:14:46.495536 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m20:14:46.496201 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m20:14:46.500967 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m20:14:46.501642 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:14:46.502428 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m20:14:46.506285 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m20:14:46.511493 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m20:14:46.512443 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m20:14:46.516222 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m20:14:46.521201 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m20:14:46.527733 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m20:14:46.528699 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:14:46.530137 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:14:46.535621 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m20:14:46.536685 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:14:46.537861 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m20:14:46.538775 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:14:46.543631 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m20:14:46.545159 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:14:46.549674 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m20:14:46.551576 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:14:46.552400 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:14:46.554936 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:14:46.556577 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:14:46.557542 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m20:14:46.558456 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:14:46.569376 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m20:14:46.570718 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:14:46.572281 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:14:46.573596 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m20:14:46.575382 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m20:14:46.576307 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:14:46.577736 [debug] [Thread-1 (]: SQL status: OK in 0.048 seconds
[0m20:14:46.578734 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m20:14:46.579694 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:14:46.580319 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m20:14:46.580807 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m20:14:46.581404 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m20:14:46.582219 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:14:46.583190 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m20:14:46.584046 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m20:14:46.584710 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m20:14:46.585792 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m20:14:46.586607 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:14:46.587214 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m20:14:46.587951 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:14:46.589012 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m20:14:46.589930 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m20:14:46.590749 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m20:14:46.591994 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m20:14:46.592815 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m20:14:46.594001 [debug] [Thread-2 (]: SQL status: OK in 0.022 seconds
[0m20:14:46.595004 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m20:14:46.596211 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m20:14:46.597435 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m20:14:46.598556 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m20:14:46.600328 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m20:14:46.602565 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:14:46.606417 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m20:14:46.611529 [debug] [Thread-3 (]: SQL status: OK in 0.029 seconds
[0m20:14:46.613004 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:14:46.614380 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m20:14:46.615947 [debug] [Thread-4 (]: SQL status: OK in 0.032 seconds
[0m20:14:46.616540 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:14:46.617101 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      WITH ridership_data AS (
    SELECT  
        DATE_TRUNC('week', date) AS week_start,
        SUM(subways_total_ridership) AS ridership,
        'Subway' AS transport_type,
        AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(buses_total_ridership) AS ridership,
        'Buses' AS transport_type,
        AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(lirr_total_ridership) AS ridership,
        'LIRR' AS transport_type,
        AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(metro_north_total_ridership) AS ridership,
        'Metro North' AS transport_type,
        AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(access_a_ride_total_trips) AS ridership,
        'Access-A-Ride' AS transport_type,
        AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(bridges_tunnels_total_traffic) AS ridership,
        'Bridges and Tunnels' AS transport_type,
        AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(staten_island_railway_total_ridership) AS ridership,
        'Staten Island Railway' AS transport_type,
        AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type
),
weather_data AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    rd.week_start, 
    rd.transport_type,
    rd.ridership,
    rd.avg_pct_pre_pandemic,
    wd.avg_weekly_temperature,
    wd.total_weekly_precipitation
FROM 
    ridership_data rd
LEFT JOIN 
    weather_data wd
ON 
    rd.week_start = wd.week_start
WHERE 
    rd.week_start < '2024-10-15'
ORDER BY 
    rd.week_start, rd.transport_type
    );
  
  
[0m20:14:46.619199 [debug] [Thread-5 (]: SQL status: OK in 0.033 seconds
[0m20:14:46.619791 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:14:46.620399 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m20:14:46.623368 [debug] [Thread-6 (]: SQL status: OK in 0.037 seconds
[0m20:14:46.624211 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:14:46.624823 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m20:14:46.627650 [debug] [Thread-8 (]: SQL status: OK in 0.039 seconds
[0m20:14:46.628237 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:14:46.628715 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m20:14:46.635840 [debug] [Thread-7 (]: SQL status: OK in 0.045 seconds
[0m20:14:46.636515 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:14:46.637656 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m20:14:46.644760 [debug] [Thread-9 (]: SQL status: OK in 0.053 seconds
[0m20:14:46.645339 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:14:46.646414 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m20:14:46.651981 [debug] [Thread-10 ]: SQL status: OK in 0.059 seconds
[0m20:14:46.652816 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:14:46.653299 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m20:14:46.661792 [debug] [Thread-11 ]: SQL status: OK in 0.067 seconds
[0m20:14:46.662628 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:14:46.663380 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m20:14:46.673642 [debug] [Thread-12 ]: SQL status: OK in 0.077 seconds
[0m20:14:46.674261 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:14:46.674908 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers,
        MIN(latitude) AS latitude,   -- Add latitude
        MIN(longitude) AS longitude  -- Add longitude
    FROM 
        "mtastats"."main"."mta_daily_ridership"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024,
        MIN(latitude) AS latitude,   -- Add latitude
        MIN(longitude) AS longitude  -- Add longitude
    FROM 
        "mtastats"."main"."mta_daily_ridership"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change,
    MAX(rbd.latitude) AS latitude,   -- Add latitude
    MAX(rbd.longitude) AS longitude  -- Add longitude
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m20:14:46.698805 [debug] [Thread-13 ]: SQL status: OK in 0.101 seconds
[0m20:14:46.699756 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:14:46.700583 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m20:14:46.720611 [debug] [Thread-14 ]: SQL status: OK in 0.120 seconds
[0m20:14:46.721297 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:14:46.721821 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership,
        MIN(latitude) AS latitude,  -- Assuming latitude is the same for each station complex, use MIN() or MAX()
        MIN(longitude) AS longitude  -- Assuming longitude is the same for each station complex, use MIN() or MAX()
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    wr.latitude,
    wr.longitude,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
WHERE 
    wr.week_start < '2024-09-17'
ORDER BY 
    wr.station_complex, 
    wr.week_start
    );
  
  
[0m20:14:46.813303 [debug] [Thread-12 ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers,
        MIN(latitude) AS latitude,   -- Add latitude
        MIN(longitude) AS longitude  -- Add longitude
    FROM 
        "mtastats"."main"."mta_daily_ridership"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024,
        MIN(latitude) AS latitude,   -- Add latitude
        MIN(longitude) AS longitude  -- Add longitude
    FROM 
        "mtastats"."main"."mta_daily_ridership"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change,
    MAX(rbd.latitude) AS latitude,   -- Add latitude
    MAX(rbd.longitude) AS longitude  -- Add longitude
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m20:14:46.816236 [debug] [Thread-12 ]: DuckDB adapter: Rolling back transaction.
[0m20:14:46.816978 [debug] [Thread-12 ]: On model.mta.subway_station_stats: ROLLBACK
[0m20:14:46.895698 [debug] [Thread-12 ]: Failed to rollback 'model.mta.subway_station_stats'
[0m20:14:46.896501 [debug] [Thread-12 ]: On model.mta.subway_station_stats: Close
[0m20:14:46.897947 [debug] [Thread-12 ]: Runtime Error in model subway_station_stats (models/subway_station_stats.sql)
  Binder Error: Referenced column "transit_timestamp" not found in FROM clause!
  Candidate bindings: "mta_daily_ridership.date"
  LINE 24:         YEAR(transit_timestamp) = 2024
                        ^
[0m20:14:46.901473 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b81ef0670>]}
[0m20:14:46.902785 [error] [Thread-12 ]: 12 of 14 ERROR creating sql table model main.subway_station_stats .............. [[31mERROR[0m in 0.52s]
[0m20:14:46.903852 [debug] [Thread-12 ]: Finished running node model.mta.subway_station_stats
[0m20:14:47.163127 [debug] [Thread-5 (]: SQL status: OK in 0.542 seconds
[0m20:14:47.226860 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:14:47.254538 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m20:14:47.259197 [debug] [Thread-5 (]: SQL status: OK in 0.003 seconds
[0m20:14:47.499347 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m20:14:47.500743 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:14:47.502211 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m20:14:47.503719 [debug] [Thread-8 (]: SQL status: OK in 0.874 seconds
[0m20:14:47.513941 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:14:47.514745 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m20:14:47.516438 [debug] [Thread-5 (]: SQL status: OK in 0.013 seconds
[0m20:14:47.528065 [debug] [Thread-9 (]: SQL status: OK in 0.880 seconds
[0m20:14:47.528778 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:14:47.533755 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:14:47.535140 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m20:14:47.536441 [debug] [Thread-8 (]: SQL status: OK in 0.021 seconds
[0m20:14:47.537259 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m20:14:47.540373 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m20:14:47.542327 [debug] [Thread-5 (]: SQL status: OK in 0.004 seconds
[0m20:14:47.543200 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:14:47.548512 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m20:14:47.549850 [debug] [Thread-9 (]: SQL status: OK in 0.008 seconds
[0m20:14:47.550807 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m20:14:47.552088 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d64700>]}
[0m20:14:47.559232 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m20:14:47.561198 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 1.19s]
[0m20:14:47.562371 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:14:47.563732 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m20:14:47.564636 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m20:14:47.575432 [debug] [Thread-8 (]: SQL status: OK in 0.015 seconds
[0m20:14:47.580906 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:14:47.583211 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m20:14:47.595727 [debug] [Thread-9 (]: SQL status: OK in 0.029 seconds
[0m20:14:47.677288 [debug] [Thread-8 (]: SQL status: OK in 0.093 seconds
[0m20:14:47.775827 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:14:47.780840 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m20:14:47.781859 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m20:14:47.783019 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d66320>]}
[0m20:14:47.786166 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 1.41s]
[0m20:14:47.787296 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m20:14:47.788056 [debug] [Thread-9 (]: SQL status: OK in 0.004 seconds
[0m20:14:47.790741 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m20:14:47.792074 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d654b0>]}
[0m20:14:47.793306 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 1.42s]
[0m20:14:47.794068 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m20:14:48.448962 [debug] [Thread-4 (]: SQL status: OK in 1.831 seconds
[0m20:14:48.473631 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:14:48.474278 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m20:14:48.475512 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m20:14:48.487483 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m20:14:48.488410 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:14:48.488997 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m20:14:48.501973 [debug] [Thread-4 (]: SQL status: OK in 0.012 seconds
[0m20:14:48.556273 [debug] [Thread-7 (]: SQL status: OK in 1.916 seconds
[0m20:14:48.647996 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:14:48.649998 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m20:14:48.630625 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:14:48.655219 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m20:14:48.657056 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m20:14:48.660168 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m20:14:48.661152 [debug] [Thread-7 (]: SQL status: OK in 0.007 seconds
[0m20:14:48.662923 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d64b50>]}
[0m20:14:48.666499 [debug] [Thread-2 (]: SQL status: OK in 2.059 seconds
[0m20:14:48.671302 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m20:14:48.694689 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:14:48.711029 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m20:14:48.697636 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:14:48.714550 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m20:14:48.680365 [info ] [Thread-4 (]: 4 of 14 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 2.30s]
[0m20:14:48.719657 [debug] [Thread-2 (]: SQL status: OK in 0.007 seconds
[0m20:14:48.730766 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m20:14:48.731774 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:14:48.732405 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m20:14:48.726396 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m20:14:48.746216 [debug] [Thread-7 (]: SQL status: OK in 0.029 seconds
[0m20:14:48.757360 [debug] [Thread-2 (]: SQL status: OK in 0.024 seconds
[0m20:14:48.777660 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:14:48.790149 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m20:14:48.817785 [debug] [Thread-7 (]: SQL status: OK in 0.002 seconds
[0m20:14:48.820879 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m20:14:48.823657 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:14:48.824717 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m20:14:48.826156 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m20:14:48.829539 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m20:14:48.830898 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b7dec2170>]}
[0m20:14:48.832527 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 2.49s]
[0m20:14:48.834624 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m20:14:48.844867 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a99741c00>]}
[0m20:14:48.852132 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 2.45s]
[0m20:14:48.853989 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m20:14:52.180426 [debug] [Thread-3 (]: SQL status: OK in 5.565 seconds
[0m20:14:52.189137 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:14:52.190719 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m20:14:52.192929 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m20:14:52.196697 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m20:14:52.197665 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:14:52.198986 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m20:14:52.214661 [debug] [Thread-3 (]: SQL status: OK in 0.015 seconds
[0m20:14:52.244672 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:14:52.247400 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m20:14:52.275281 [debug] [Thread-3 (]: SQL status: OK in 0.025 seconds
[0m20:14:52.278813 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m20:14:52.280216 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d66440>]}
[0m20:14:52.281566 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 5.92s]
[0m20:14:52.284613 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m20:14:53.601600 [debug] [Thread-1 (]: SQL status: OK in 6.988 seconds
[0m20:14:53.627241 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:14:53.628389 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m20:14:53.636592 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m20:14:53.653251 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m20:14:53.654433 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:14:53.655227 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m20:14:53.850173 [debug] [Thread-1 (]: SQL status: OK in 0.194 seconds
[0m20:14:53.855364 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:14:53.856340 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m20:14:53.857487 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m20:14:53.862091 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m20:14:53.863351 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d65d80>]}
[0m20:14:53.864570 [info ] [Thread-1 (]: 1 of 14 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 7.54s]
[0m20:14:53.866360 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m20:14:59.346288 [debug] [Thread-14 ]: SQL status: OK in 12.623 seconds
[0m20:14:59.355540 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:14:59.368860 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m20:14:59.370721 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m20:14:59.374313 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m20:14:59.375335 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:14:59.376075 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m20:14:59.488778 [debug] [Thread-13 ]: SQL status: OK in 12.787 seconds
[0m20:14:59.507663 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:14:59.509045 [debug] [Thread-14 ]: SQL status: OK in 0.132 seconds
[0m20:14:59.509988 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m20:14:59.530246 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:14:59.535554 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m20:14:59.536844 [debug] [Thread-13 ]: SQL status: OK in 0.002 seconds
[0m20:14:59.540182 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m20:14:59.541109 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:14:59.541960 [debug] [Thread-14 ]: SQL status: OK in 0.005 seconds
[0m20:14:59.542670 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m20:14:59.545839 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m20:14:59.547955 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75e08fa0>]}
[0m20:14:59.549941 [info ] [Thread-14 ]: 14 of 14 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 13.16s]
[0m20:14:59.552076 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m20:14:59.553975 [debug] [Thread-13 ]: SQL status: OK in 0.007 seconds
[0m20:14:59.559130 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:14:59.560225 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m20:14:59.561502 [debug] [Thread-13 ]: SQL status: OK in 0.000 seconds
[0m20:14:59.566066 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: Close
[0m20:14:59.567364 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b46da0280>]}
[0m20:14:59.568701 [info ] [Thread-13 ]: 13 of 14 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 13.18s]
[0m20:14:59.587547 [debug] [Thread-13 ]: Finished running node model.mta.total_riders_per_station
[0m20:14:59.715968 [debug] [Thread-11 ]: SQL status: OK in 13.051 seconds
[0m20:14:59.745668 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:14:59.746704 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m20:14:59.748335 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m20:14:59.752730 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m20:14:59.753570 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:14:59.754226 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m20:14:59.778561 [debug] [Thread-11 ]: SQL status: OK in 0.023 seconds
[0m20:14:59.783362 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:14:59.784255 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m20:14:59.785795 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m20:14:59.789320 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: Close
[0m20:14:59.790544 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d66110>]}
[0m20:14:59.791687 [info ] [Thread-11 ]: 11 of 14 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 13.41s]
[0m20:14:59.793269 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_increase
[0m20:15:00.254112 [debug] [Thread-10 ]: SQL status: OK in 13.600 seconds
[0m20:15:00.259970 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:15:00.261003 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m20:15:00.262396 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m20:15:00.265817 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m20:15:00.266777 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:15:00.267552 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m20:15:00.277199 [debug] [Thread-10 ]: SQL status: OK in 0.008 seconds
[0m20:15:00.282944 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:15:00.295259 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m20:15:00.297007 [debug] [Thread-10 ]: SQL status: OK in 0.000 seconds
[0m20:15:00.299840 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: Close
[0m20:15:00.301174 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d65c00>]}
[0m20:15:00.304750 [info ] [Thread-10 ]: 10 of 14 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 13.93s]
[0m20:15:00.307343 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_by_station
[0m20:15:01.581011 [debug] [Thread-6 (]: SQL status: OK in 14.955 seconds
[0m20:15:01.588920 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:15:01.590491 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m20:15:01.593304 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m20:15:01.597602 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m20:15:01.598442 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:15:01.599066 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m20:15:01.607292 [debug] [Thread-6 (]: SQL status: OK in 0.007 seconds
[0m20:15:01.617315 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:15:01.618715 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m20:15:01.620812 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m20:15:01.626554 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m20:15:01.928263 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a945c766-a986-484c-b3a4-a1f1cdd46181', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b75d641f0>]}
[0m20:15:01.929833 [info ] [Thread-6 (]: 6 of 14 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 15.56s]
[0m20:15:01.930885 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m20:15:01.940926 [debug] [MainThread]: Using duckdb connection "master"
[0m20:15:01.942166 [debug] [MainThread]: On master: BEGIN
[0m20:15:01.942883 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:15:01.960057 [debug] [MainThread]: SQL status: OK in 0.017 seconds
[0m20:15:01.961044 [debug] [MainThread]: On master: COMMIT
[0m20:15:01.961493 [debug] [MainThread]: Using duckdb connection "master"
[0m20:15:01.961854 [debug] [MainThread]: On master: COMMIT
[0m20:15:01.963015 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m20:15:01.963618 [debug] [MainThread]: On master: Close
[0m20:15:01.968606 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:15:01.969604 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m20:15:01.970324 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m20:15:01.970964 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m20:15:01.971543 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m20:15:01.972386 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m20:15:01.972983 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m20:15:01.973547 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m20:15:01.974109 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m20:15:01.974897 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m20:15:01.975510 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m20:15:01.976091 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m20:15:01.976684 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m20:15:01.977247 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m20:15:01.977832 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m20:15:01.978782 [info ] [MainThread]: 
[0m20:15:01.979338 [info ] [MainThread]: Finished running 14 table models in 0 hours 0 minutes and 15.91 seconds (15.91s).
[0m20:15:01.981761 [debug] [MainThread]: Command end result
[0m20:15:02.024735 [info ] [MainThread]: 
[0m20:15:02.025691 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:15:02.026197 [info ] [MainThread]: 
[0m20:15:02.026964 [error] [MainThread]:   Runtime Error in model subway_station_stats (models/subway_station_stats.sql)
  Binder Error: Referenced column "transit_timestamp" not found in FROM clause!
  Candidate bindings: "mta_daily_ridership.date"
  LINE 24:         YEAR(transit_timestamp) = 2024
                        ^
[0m20:15:02.027633 [info ] [MainThread]: 
[0m20:15:02.028234 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=1 SKIP=0 TOTAL=14
[0m20:15:02.031906 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 16.879145, "process_user_time": 141.76242, "process_kernel_time": 42.443336, "process_mem_max_rss": "527776", "process_in_blocks": "114552", "process_out_blocks": "22272", "command_success": false}
[0m20:15:02.033746 [debug] [MainThread]: Command `dbt run` failed at 20:15:02.033103 after 16.88 seconds
[0m20:15:02.034777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b82bbf400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b81eca800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b811e98d0>]}
[0m20:15:02.035725 [debug] [MainThread]: Flushing usage events
[0m20:15:44.902934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cf7487400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cf705c3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cf5b31ea0>]}


============================== 20:15:44.907117 | c349bea0-e27d-4f0a-842d-f4338a976378 ==============================
[0m20:15:44.907117 [info ] [MainThread]: Running with dbt=1.8.7
[0m20:15:44.907704 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/christianocean/mta/mta/transformations/dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/christianocean/mta/mta/transformations/dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:15:45.164416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cf5ada0e0>]}
[0m20:15:45.242867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cf6792800>]}
[0m20:15:45.246658 [info ] [MainThread]: Registered adapter: duckdb=1.8.3
[0m20:15:45.261415 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m20:15:45.369735 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:15:45.370780 [debug] [MainThread]: Partial parsing: updated file: mta://models/subway_station_stats.sql
[0m20:15:45.662075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea5dc130>]}
[0m20:15:45.747632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea60d720>]}
[0m20:15:45.748249 [info ] [MainThread]: Found 14 models, 9 sources, 416 macros
[0m20:15:45.748669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea60d630>]}
[0m20:15:45.750648 [info ] [MainThread]: 
[0m20:15:45.751294 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m20:15:45.757694 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_mtastats'
[0m20:15:45.849143 [debug] [ThreadPool]: Using duckdb connection "list_mtastats"
[0m20:15:45.849707 [debug] [ThreadPool]: On list_mtastats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where catalog_name = '"mtastats"'
    
  
  
[0m20:15:45.850052 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:15:45.859410 [debug] [ThreadPool]: SQL status: OK in 0.009 seconds
[0m20:15:45.860811 [debug] [ThreadPool]: On list_mtastats: Close
[0m20:15:45.864772 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mtastats, now create_mtastats_main)
[0m20:15:45.865433 [debug] [ThreadPool]: Creating schema "database: "mtastats"
schema: "main"
"
[0m20:15:45.872298 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:15:45.872848 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
        select type from duckdb_databases()
        where database_name='mtastats'
        and type='sqlite'
    
  
[0m20:15:45.873189 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:15:45.881465 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m20:15:45.883016 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:15:45.883452 [debug] [ThreadPool]: On create_mtastats_main: BEGIN
[0m20:15:45.883980 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:15:45.884323 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:15:45.884635 [debug] [ThreadPool]: On create_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "create_mtastats_main"} */

    
    
        create schema if not exists "mtastats"."main"
    
[0m20:15:45.885215 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:15:45.886052 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m20:15:45.886404 [debug] [ThreadPool]: Using duckdb connection "create_mtastats_main"
[0m20:15:45.886687 [debug] [ThreadPool]: On create_mtastats_main: COMMIT
[0m20:15:45.887234 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m20:15:45.887579 [debug] [ThreadPool]: On create_mtastats_main: Close
[0m20:15:45.892977 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_mtastats_main, now list_mtastats_main)
[0m20:15:45.899444 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m20:15:45.899974 [debug] [ThreadPool]: On list_mtastats_main: BEGIN
[0m20:15:45.900306 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:15:45.908085 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m20:15:45.908670 [debug] [ThreadPool]: Using duckdb connection "list_mtastats_main"
[0m20:15:45.909033 [debug] [ThreadPool]: On list_mtastats_main: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "connection_name": "list_mtastats_main"} */
select
      'mtastats' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where table_schema = 'main'
    and table_catalog = 'mtastats'
  
[0m20:15:45.932817 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m20:15:45.934872 [debug] [ThreadPool]: On list_mtastats_main: ROLLBACK
[0m20:15:45.935538 [debug] [ThreadPool]: Failed to rollback 'list_mtastats_main'
[0m20:15:45.935910 [debug] [ThreadPool]: On list_mtastats_main: Close
[0m20:15:45.940154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cf7388580>]}
[0m20:15:45.940714 [debug] [MainThread]: Using duckdb connection "master"
[0m20:15:45.941038 [debug] [MainThread]: On master: BEGIN
[0m20:15:45.941303 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:15:45.949917 [debug] [MainThread]: SQL status: OK in 0.009 seconds
[0m20:15:45.950395 [debug] [MainThread]: On master: COMMIT
[0m20:15:45.950704 [debug] [MainThread]: Using duckdb connection "master"
[0m20:15:45.950964 [debug] [MainThread]: On master: COMMIT
[0m20:15:45.951476 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m20:15:45.951844 [debug] [MainThread]: On master: Close
[0m20:15:45.954110 [info ] [MainThread]: Concurrency: 16 threads (target='dev')
[0m20:15:45.954582 [info ] [MainThread]: 
[0m20:15:45.960150 [debug] [Thread-1 (]: Began running node model.mta.avg_riders_per_day
[0m20:15:45.960606 [debug] [Thread-2 (]: Began running node model.mta.bond_payment_info
[0m20:15:45.961123 [debug] [Thread-3 (]: Began running node model.mta.busiest_specific_times
[0m20:15:45.961951 [debug] [Thread-4 (]: Began running node model.mta.daily_ridership
[0m20:15:45.961613 [info ] [Thread-1 (]: 1 of 14 START sql table model main.avg_riders_per_day .......................... [RUN]
[0m20:15:45.962490 [info ] [Thread-2 (]: 2 of 14 START sql table model main.bond_payment_info ........................... [RUN]
[0m20:15:45.963577 [debug] [Thread-5 (]: Began running node model.mta.expense_type_per_year
[0m20:15:45.963946 [debug] [Thread-6 (]: Began running node model.mta.fare_class_boro
[0m20:15:45.964704 [debug] [Thread-7 (]: Began running node model.mta.forecast_accuracy_2023
[0m20:15:45.965072 [debug] [Thread-8 (]: Began running node model.mta.labor_expenses_per_agency
[0m20:15:45.963260 [info ] [Thread-3 (]: 3 of 14 START sql table model main.busiest_specific_times ...................... [RUN]
[0m20:15:45.966015 [debug] [Thread-9 (]: Began running node model.mta.largest_expense_differences_2023
[0m20:15:45.966427 [debug] [Thread-10 ]: Began running node model.mta.omny_adoption_by_station
[0m20:15:45.966967 [debug] [Thread-11 ]: Began running node model.mta.omny_adoption_increase
[0m20:15:45.967720 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.mta.avg_riders_per_day'
[0m20:15:45.965543 [info ] [Thread-4 (]: 4 of 14 START sql table model main.daily_ridership ............................. [RUN]
[0m20:15:45.968618 [debug] [Thread-12 ]: Began running node model.mta.subway_station_stats
[0m20:15:45.969629 [debug] [Thread-13 ]: Began running node model.mta.total_riders_per_station
[0m20:15:45.970419 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.mta.bond_payment_info'
[0m20:15:45.971112 [debug] [Thread-14 ]: Began running node model.mta.weekly_riders_per_station
[0m20:15:45.972263 [info ] [Thread-5 (]: 5 of 14 START sql table model main.expense_type_per_year ....................... [RUN]
[0m20:15:45.973430 [info ] [Thread-6 (]: 6 of 14 START sql table model main.fare_class_boro ............................. [RUN]
[0m20:15:45.975072 [info ] [Thread-7 (]: 7 of 14 START sql table model main.forecast_accuracy_2023 ...................... [RUN]
[0m20:15:45.977682 [info ] [Thread-8 (]: 8 of 14 START sql table model main.labor_expenses_per_agency ................... [RUN]
[0m20:15:45.978973 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.mta.busiest_specific_times'
[0m20:15:45.979988 [info ] [Thread-9 (]: 9 of 14 START sql table model main.largest_expense_differences_2023 ............ [RUN]
[0m20:15:45.980829 [info ] [Thread-10 ]: 10 of 14 START sql table model main.omny_adoption_by_station ................... [RUN]
[0m20:15:45.981887 [info ] [Thread-11 ]: 11 of 14 START sql table model main.omny_adoption_increase ..................... [RUN]
[0m20:15:45.982563 [debug] [Thread-1 (]: Began compiling node model.mta.avg_riders_per_day
[0m20:15:45.983550 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.mta.daily_ridership'
[0m20:15:45.984263 [info ] [Thread-12 ]: 12 of 14 START sql table model main.subway_station_stats ....................... [RUN]
[0m20:15:45.985017 [info ] [Thread-13 ]: 13 of 14 START sql table model main.total_riders_per_station ................... [RUN]
[0m20:15:45.985606 [debug] [Thread-2 (]: Began compiling node model.mta.bond_payment_info
[0m20:15:45.986333 [info ] [Thread-14 ]: 14 of 14 START sql table model main.weekly_riders_per_station .................. [RUN]
[0m20:15:45.987233 [debug] [Thread-5 (]: Re-using an available connection from the pool (formerly list_mtastats_main, now model.mta.expense_type_per_year)
[0m20:15:45.988265 [debug] [Thread-6 (]: Acquiring new duckdb connection 'model.mta.fare_class_boro'
[0m20:15:45.989174 [debug] [Thread-7 (]: Acquiring new duckdb connection 'model.mta.forecast_accuracy_2023'
[0m20:15:45.990155 [debug] [Thread-8 (]: Acquiring new duckdb connection 'model.mta.labor_expenses_per_agency'
[0m20:15:45.992144 [debug] [Thread-3 (]: Began compiling node model.mta.busiest_specific_times
[0m20:15:45.993772 [debug] [Thread-9 (]: Acquiring new duckdb connection 'model.mta.largest_expense_differences_2023'
[0m20:15:45.995191 [debug] [Thread-10 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_by_station'
[0m20:15:45.996585 [debug] [Thread-11 ]: Acquiring new duckdb connection 'model.mta.omny_adoption_increase'
[0m20:15:46.009712 [debug] [Thread-4 (]: Began compiling node model.mta.daily_ridership
[0m20:15:46.010649 [debug] [Thread-1 (]: Writing injected SQL for node "model.mta.avg_riders_per_day"
[0m20:15:46.011819 [debug] [Thread-12 ]: Acquiring new duckdb connection 'model.mta.subway_station_stats'
[0m20:15:46.012715 [debug] [Thread-13 ]: Acquiring new duckdb connection 'model.mta.total_riders_per_station'
[0m20:15:46.016918 [debug] [Thread-2 (]: Writing injected SQL for node "model.mta.bond_payment_info"
[0m20:15:46.018173 [debug] [Thread-14 ]: Acquiring new duckdb connection 'model.mta.weekly_riders_per_station'
[0m20:15:46.019060 [debug] [Thread-5 (]: Began compiling node model.mta.expense_type_per_year
[0m20:15:46.020008 [debug] [Thread-6 (]: Began compiling node model.mta.fare_class_boro
[0m20:15:46.020851 [debug] [Thread-7 (]: Began compiling node model.mta.forecast_accuracy_2023
[0m20:15:46.021474 [debug] [Thread-8 (]: Began compiling node model.mta.labor_expenses_per_agency
[0m20:15:46.027789 [debug] [Thread-3 (]: Writing injected SQL for node "model.mta.busiest_specific_times"
[0m20:15:46.028832 [debug] [Thread-9 (]: Began compiling node model.mta.largest_expense_differences_2023
[0m20:15:46.029898 [debug] [Thread-10 ]: Began compiling node model.mta.omny_adoption_by_station
[0m20:15:46.030686 [debug] [Thread-11 ]: Began compiling node model.mta.omny_adoption_increase
[0m20:15:46.036344 [debug] [Thread-4 (]: Writing injected SQL for node "model.mta.daily_ridership"
[0m20:15:46.038293 [debug] [Thread-12 ]: Began compiling node model.mta.subway_station_stats
[0m20:15:46.039094 [debug] [Thread-13 ]: Began compiling node model.mta.total_riders_per_station
[0m20:15:46.039914 [debug] [Thread-1 (]: Began executing node model.mta.avg_riders_per_day
[0m20:15:46.040728 [debug] [Thread-14 ]: Began compiling node model.mta.weekly_riders_per_station
[0m20:15:46.041544 [debug] [Thread-2 (]: Began executing node model.mta.bond_payment_info
[0m20:15:46.045302 [debug] [Thread-5 (]: Writing injected SQL for node "model.mta.expense_type_per_year"
[0m20:15:46.053291 [debug] [Thread-6 (]: Writing injected SQL for node "model.mta.fare_class_boro"
[0m20:15:46.057906 [debug] [Thread-7 (]: Writing injected SQL for node "model.mta.forecast_accuracy_2023"
[0m20:15:46.062103 [debug] [Thread-8 (]: Writing injected SQL for node "model.mta.labor_expenses_per_agency"
[0m20:15:46.065650 [debug] [Thread-9 (]: Writing injected SQL for node "model.mta.largest_expense_differences_2023"
[0m20:15:46.068625 [debug] [Thread-10 ]: Writing injected SQL for node "model.mta.omny_adoption_by_station"
[0m20:15:46.069174 [debug] [Thread-3 (]: Began executing node model.mta.busiest_specific_times
[0m20:15:46.071603 [debug] [Thread-11 ]: Writing injected SQL for node "model.mta.omny_adoption_increase"
[0m20:15:46.074864 [debug] [Thread-12 ]: Writing injected SQL for node "model.mta.subway_station_stats"
[0m20:15:46.077912 [debug] [Thread-13 ]: Writing injected SQL for node "model.mta.total_riders_per_station"
[0m20:15:46.078768 [debug] [Thread-4 (]: Began executing node model.mta.daily_ridership
[0m20:15:46.097618 [debug] [Thread-14 ]: Writing injected SQL for node "model.mta.weekly_riders_per_station"
[0m20:15:46.115327 [debug] [Thread-2 (]: Writing runtime sql for node "model.mta.bond_payment_info"
[0m20:15:46.113846 [debug] [Thread-1 (]: Writing runtime sql for node "model.mta.avg_riders_per_day"
[0m20:15:46.117913 [debug] [Thread-5 (]: Began executing node model.mta.expense_type_per_year
[0m20:15:46.118748 [debug] [Thread-6 (]: Began executing node model.mta.fare_class_boro
[0m20:15:46.119350 [debug] [Thread-7 (]: Began executing node model.mta.forecast_accuracy_2023
[0m20:15:46.124961 [debug] [Thread-3 (]: Writing runtime sql for node "model.mta.busiest_specific_times"
[0m20:15:46.125837 [debug] [Thread-8 (]: Began executing node model.mta.labor_expenses_per_agency
[0m20:15:46.126477 [debug] [Thread-9 (]: Began executing node model.mta.largest_expense_differences_2023
[0m20:15:46.127078 [debug] [Thread-10 ]: Began executing node model.mta.omny_adoption_by_station
[0m20:15:46.129216 [debug] [Thread-12 ]: Began executing node model.mta.subway_station_stats
[0m20:15:46.129814 [debug] [Thread-11 ]: Began executing node model.mta.omny_adoption_increase
[0m20:15:46.130303 [debug] [Thread-13 ]: Began executing node model.mta.total_riders_per_station
[0m20:15:46.133611 [debug] [Thread-4 (]: Writing runtime sql for node "model.mta.daily_ridership"
[0m20:15:46.135057 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:15:46.140266 [debug] [Thread-5 (]: Writing runtime sql for node "model.mta.expense_type_per_year"
[0m20:15:46.140975 [debug] [Thread-14 ]: Began executing node model.mta.weekly_riders_per_station
[0m20:15:46.141663 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:15:46.145692 [debug] [Thread-6 (]: Writing runtime sql for node "model.mta.fare_class_boro"
[0m20:15:46.149002 [debug] [Thread-7 (]: Writing runtime sql for node "model.mta.forecast_accuracy_2023"
[0m20:15:46.152692 [debug] [Thread-8 (]: Writing runtime sql for node "model.mta.labor_expenses_per_agency"
[0m20:15:46.153384 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:15:46.157498 [debug] [Thread-9 (]: Writing runtime sql for node "model.mta.largest_expense_differences_2023"
[0m20:15:46.162672 [debug] [Thread-10 ]: Writing runtime sql for node "model.mta.omny_adoption_by_station"
[0m20:15:46.167346 [debug] [Thread-12 ]: Writing runtime sql for node "model.mta.subway_station_stats"
[0m20:15:46.170929 [debug] [Thread-11 ]: Writing runtime sql for node "model.mta.omny_adoption_increase"
[0m20:15:46.174736 [debug] [Thread-13 ]: Writing runtime sql for node "model.mta.total_riders_per_station"
[0m20:15:46.175816 [debug] [Thread-2 (]: On model.mta.bond_payment_info: BEGIN
[0m20:15:46.177007 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:15:46.181432 [debug] [Thread-14 ]: Writing runtime sql for node "model.mta.weekly_riders_per_station"
[0m20:15:46.182256 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:15:46.183055 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: BEGIN
[0m20:15:46.184286 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:15:46.185065 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: BEGIN
[0m20:15:46.185539 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:15:46.186133 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:15:46.188264 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:15:46.189378 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:15:46.190124 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:15:46.190814 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:15:46.191528 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m20:15:46.192035 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:15:46.192378 [debug] [Thread-4 (]: On model.mta.daily_ridership: BEGIN
[0m20:15:46.193243 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: BEGIN
[0m20:15:46.193679 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:15:46.194146 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:15:46.194646 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: BEGIN
[0m20:15:46.195103 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m20:15:46.195528 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: BEGIN
[0m20:15:46.195986 [debug] [Thread-6 (]: On model.mta.fare_class_boro: BEGIN
[0m20:15:46.196420 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: BEGIN
[0m20:15:46.196894 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: BEGIN
[0m20:15:46.197322 [debug] [Thread-12 ]: On model.mta.subway_station_stats: BEGIN
[0m20:15:46.197807 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: BEGIN
[0m20:15:46.206950 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: BEGIN
[0m20:15:46.207594 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m20:15:46.208224 [debug] [Thread-5 (]: Opening a new connection, currently in state closed
[0m20:15:46.208997 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: BEGIN
[0m20:15:46.209865 [debug] [Thread-2 (]: SQL status: OK in 0.018 seconds
[0m20:15:46.210379 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m20:15:46.211643 [debug] [Thread-8 (]: Opening a new connection, currently in state init
[0m20:15:46.212216 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:15:46.212701 [debug] [Thread-9 (]: Opening a new connection, currently in state init
[0m20:15:46.213126 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m20:15:46.214256 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m20:15:46.215004 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m20:15:46.215990 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m20:15:46.216813 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m20:15:46.219916 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m20:15:46.220552 [debug] [Thread-3 (]: SQL status: OK in 0.025 seconds
[0m20:15:46.221414 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:15:46.223060 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:15:46.224633 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:15:46.225305 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */

  
    
    

    create  table
      "mtastats"."main"."bond_payment_info__dbt_tmp"
  
    as (
      WITH payment_stats AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        MAX(timestamp) AS last_payment_date,
        AVG(amount) AS average_payment,
        COUNT(DISTINCT timestamp) AS total_payments,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) AS median_payment
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
first_payment AS (
    SELECT
        general_ledger,
        MIN(timestamp) AS first_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = fp.general_ledger AND timestamp = MIN(fp.timestamp)) AS first_payment_amount
    FROM
        mta_operations_statement fp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
),
last_payment AS (
    SELECT
        general_ledger,
        MAX(timestamp) AS last_payment_date,
        (SELECT amount FROM mta_operations_statement WHERE general_ledger = lp.general_ledger AND timestamp = MAX(lp.timestamp)) AS last_payment_amount
    FROM
        mta_operations_statement lp
    WHERE
        scenario = 'Actual'
        AND type = 'Debt Service Expenses'
    GROUP BY
        general_ledger
)
-- Wrapping everything inside another DISTINCT query
SELECT DISTINCT
    ps.general_ledger,
    fp.first_payment_date,
    fp.first_payment_amount,
    ps.last_payment_date,
    lp.last_payment_amount,
    ps.average_payment,
    ps.median_payment,
    ps.total_payments
FROM
    payment_stats ps
LEFT JOIN
    first_payment fp ON ps.general_ledger = fp.general_ledger
LEFT JOIN
    last_payment lp ON ps.general_ledger = lp.general_ledger
ORDER BY
    ps.general_ledger
    );
  
  
[0m20:15:46.226020 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */

  
    
    

    create  table
      "mtastats"."main"."avg_riders_per_day__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    EXTRACT(DAYOFWEEK FROM transit_timestamp) AS day_of_week, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    day_of_week
ORDER BY 
    average_ridership DESC
    );
  
  
[0m20:15:46.226735 [debug] [Thread-4 (]: SQL status: OK in 0.019 seconds
[0m20:15:46.227204 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */

  
    
    

    create  table
      "mtastats"."main"."busiest_specific_times__dbt_tmp"
  
    as (
      SELECT 
    EXTRACT(HOUR FROM transit_timestamp) AS transit_hour, 
    AVG(ridership) AS average_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    transit_hour
ORDER BY 
    average_ridership DESC
    );
  
  
[0m20:15:46.228919 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:15:46.230705 [debug] [Thread-5 (]: SQL status: OK in 0.022 seconds
[0m20:15:46.232006 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */

  
    
    

    create  table
      "mtastats"."main"."daily_ridership__dbt_tmp"
  
    as (
      WITH ridership_data AS (
    SELECT  
        DATE_TRUNC('week', date) AS week_start,
        SUM(subways_total_ridership) AS ridership,
        'Subway' AS transport_type,
        AVG(subways_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(buses_total_ridership) AS ridership,
        'Buses' AS transport_type,
        AVG(buses_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(lirr_total_ridership) AS ridership,
        'LIRR' AS transport_type,
        AVG(lirr_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(metro_north_total_ridership) AS ridership,
        'Metro North' AS transport_type,
        AVG(metro_north_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(access_a_ride_total_trips) AS ridership,
        'Access-A-Ride' AS transport_type,
        AVG(access_a_ride_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(bridges_tunnels_total_traffic) AS ridership,
        'Bridges and Tunnels' AS transport_type,
        AVG(bridges_tunnels_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type

    UNION ALL

    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        SUM(staten_island_railway_total_ridership) AS ridership,
        'Staten Island Railway' AS transport_type,
        AVG(staten_island_railway_pct_pre_pandemic) AS avg_pct_pre_pandemic
    FROM "mtastats"."main"."mta_daily_ridership"
    GROUP BY week_start, transport_type
),
weather_data AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    rd.week_start, 
    rd.transport_type,
    rd.ridership,
    rd.avg_pct_pre_pandemic,
    wd.avg_weekly_temperature,
    wd.total_weekly_precipitation
FROM 
    ridership_data rd
LEFT JOIN 
    weather_data wd
ON 
    rd.week_start = wd.week_start
WHERE 
    rd.week_start < '2024-10-15'
ORDER BY 
    rd.week_start, rd.transport_type
    );
  
  
[0m20:15:46.233256 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:15:46.236008 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */

  
    
    

    create  table
      "mtastats"."main"."expense_type_per_year__dbt_tmp"
  
    as (
      SELECT 
    agency,
    fiscal_year,
    general_ledger,
    SUM(amount) AS total_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    scenario = 'Actual'
    AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY 
    agency,
    fiscal_year,
    general_ledger
ORDER BY 
    agency, fiscal_year, general_ledger
    );
  
  
[0m20:15:46.237848 [debug] [Thread-7 (]: SQL status: OK in 0.027 seconds
[0m20:15:46.238529 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:15:46.239548 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */

  
    
    

    create  table
      "mtastats"."main"."forecast_accuracy_2023__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    general_ledger,
    expense_type,

    -- Total amounts for Adopted Budget by Financial Plan Year
    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2019,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2020,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2021,

    SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS total_adopted_budget_2022,

    -- Total amount for Actual 2023
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) AS total_actual_2023,

    -- Differences between Actual and Adopted Budget for each year
    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2019 
        THEN amount 
        ELSE 0 
    END) AS difference_2019_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2020 
        THEN amount 
        ELSE 0 
    END) AS difference_2020_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2021 
        THEN amount 
        ELSE 0 
    END) AS difference_2021_vs_actual,

    SUM(CASE 
        WHEN scenario = 'Actual' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2023 
        THEN amount 
        ELSE 0 
    END) - SUM(CASE 
        WHEN scenario = 'Adopted Budget' 
            AND fiscal_year = 2023 
            AND financial_plan_year = 2022 
        THEN amount 
        ELSE 0 
    END) AS difference_2022_vs_actual,

    -- Percentage differences for each year
    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2019 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2019_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2020 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2020_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2021 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2021_vs_actual,

    CASE 
        WHEN SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END) <> 0 
        THEN (SUM(CASE 
            WHEN scenario = 'Actual' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2023 
            THEN amount 
            ELSE 0 
        END) - SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)) / SUM(CASE 
            WHEN scenario = 'Adopted Budget' 
                AND fiscal_year = 2023 
                AND financial_plan_year = 2022 
            THEN amount 
            ELSE 0 
        END)
        ELSE NULL 
    END AS percentage_diff_2022_vs_actual

FROM "mtastats"."main"."mta_operations_statement"
WHERE type = 'Total Expenses Before Non-Cash Liability Adjs.'
GROUP BY agency_full_name, general_ledger, expense_type
    );
  
  
[0m20:15:46.256265 [debug] [Thread-8 (]: SQL status: OK in 0.043 seconds
[0m20:15:46.257159 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:15:46.257631 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */

  
    
    

    create  table
      "mtastats"."main"."labor_expenses_per_agency__dbt_tmp"
  
    as (
      SELECT 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger,
    SUM(amount) AS total_labor_expenses
FROM 
    "mtastats"."main"."mta_operations_statement"
WHERE 
    subtype = 'Labor Expenses' AND 
    scenario = 'Actual'
GROUP BY 
    agency_full_name,
    financial_plan_year,
    expense_type,
    general_ledger
ORDER BY 
    agency_full_name, financial_plan_year, expense_type, general_ledger
    );
  
  
[0m20:15:46.261142 [debug] [Thread-6 (]: SQL status: OK in 0.049 seconds
[0m20:15:46.261919 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:15:46.262361 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */

  
    
    

    create  table
      "mtastats"."main"."fare_class_boro__dbt_tmp"
  
    as (
      WITH total_ridership_per_borough_daytype AS (
    -- Calculate total ridership for each borough and day type (Weekday or Weekend)
    SELECT 
        borough, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership_borough_daytype
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, day_type
),
ridership_by_fare_class AS (
    -- Calculate total ridership by fare class category, borough, and day type (Weekday/Weekend)
    SELECT 
        borough, 
        fare_class_category, 
        CASE 
            WHEN EXTRACT(DAYOFWEEK FROM transit_timestamp) IN (1, 7) THEN 'Weekend'
            ELSE 'Weekday'
        END AS day_type,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        borough, fare_class_category, day_type
)
SELECT 
    r.borough, 
    r.fare_class_category, 
    r.day_type,
    r.total_ridership, 
    ROUND(r.total_ridership / t.total_ridership_borough_daytype, 4) AS ridership_percentage
FROM 
    ridership_by_fare_class r
JOIN 
    total_ridership_per_borough_daytype t
    ON r.borough = t.borough 
    AND r.day_type = t.day_type
ORDER BY 
    total_ridership DESC
    );
  
  
[0m20:15:46.266369 [debug] [Thread-9 (]: SQL status: OK in 0.054 seconds
[0m20:15:46.266978 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:15:46.268521 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */

  
    
    

    create  table
      "mtastats"."main"."largest_expense_differences_2023__dbt_tmp"
  
    as (
      WITH expense_differences AS (
    SELECT
        agency,
        general_ledger,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) AS actual_expenses,
        SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS adopted_plan_expenses,
        SUM(CASE WHEN scenario = 'Actual' THEN amount ELSE 0 END) - SUM(CASE WHEN scenario = 'Adopted Budget' AND financial_plan_year = 2023 THEN amount ELSE 0 END) AS difference
    FROM
        "mtastats"."main"."mta_operations_statement"
    WHERE
        fiscal_year = 2023
        AND scenario IN ('Actual', 'Adopted Budget')
        AND financial_plan_year = 2023  -- Ensure we use only the 2023 Adopted Budget
        AND type = 'Total Expenses Before Non-Cash Liability Adjs.'
        AND general_ledger != 'Reimbursable Overhead'  -- Exclude Reimbursable Overhead
    GROUP BY
        agency, general_ledger
),
ranked_expenses AS (
    SELECT
        agency,
        general_ledger,
        actual_expenses,
        adopted_plan_expenses,
        difference,
        ROW_NUMBER() OVER (PARTITION BY agency ORDER BY difference DESC) AS rank
    FROM
        expense_differences
    WHERE
        difference > 0  -- Only include cases where actual expenses are larger than the adopted budget
)
SELECT
    agency,
    general_ledger,
    actual_expenses,
    adopted_plan_expenses,
    difference
FROM
    ranked_expenses
WHERE
    rank <= 5  -- Select the top 5 items per agency
ORDER BY
    difference, rank
    );
  
  
[0m20:15:46.277255 [debug] [Thread-10 ]: SQL status: OK in 0.064 seconds
[0m20:15:46.277903 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:15:46.278317 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_by_station__dbt_tmp"
  
    as (
      WITH yearly_data AS (
    SELECT 
        station_complex,
        YEAR(transit_timestamp) AS year,
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) * 1.0 / SUM(ridership) AS omny_percentage
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2022, 2023, 2024)
    GROUP BY 
        station_complex, 
        YEAR(transit_timestamp)
),
pivoted_data AS (
    SELECT 
        station_complex,
        MAX(CASE WHEN year = 2022 THEN omny_percentage ELSE NULL END) AS omny_2022,
        MAX(CASE WHEN year = 2023 THEN omny_percentage ELSE NULL END) AS omny_2023,
        MAX(CASE WHEN year = 2024 THEN omny_percentage ELSE NULL END) AS omny_2024
    FROM 
        yearly_data
    GROUP BY 
        station_complex
)
SELECT 
    station_complex,
    omny_2022,
    omny_2023,
    omny_2024,
    CASE 
        WHEN omny_2022 IS NOT NULL AND omny_2023 IS NOT NULL THEN 
            (omny_2023 - omny_2022) / omny_2022
        ELSE NULL
    END AS omny_2023_growth,
    CASE 
        WHEN omny_2023 IS NOT NULL AND omny_2024 IS NOT NULL THEN 
            (omny_2024 - omny_2023) / omny_2023
        ELSE NULL
    END AS omny_2024_growth
FROM 
    pivoted_data
ORDER BY 
    station_complex
    );
  
  
[0m20:15:46.298522 [debug] [Thread-12 ]: SQL status: OK in 0.083 seconds
[0m20:15:46.299575 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:15:46.300151 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */

  
    
    

    create  table
      "mtastats"."main"."subway_station_stats__dbt_tmp"
  
    as (
      WITH ridership_by_day AS (
    -- Calculate daily ridership for each station, weekday/weekend split
    SELECT 
        station_complex,
        CAST(transit_timestamp AS DATE) AS day,
        DAYOFWEEK(transit_timestamp) AS weekday,
        SUM(ridership) AS daily_ridership,
        SUM(transfers) AS daily_transfers,
        MIN(latitude) AS latitude,   -- Add latitude
        MIN(longitude) AS longitude  -- Add longitude
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) = 2024
    GROUP BY 
        station_complex, 
        CAST(transit_timestamp AS DATE), 
        DAYOFWEEK(transit_timestamp)
),
total_riders_ytd AS (
    -- Total riders in 2023 and 2024 YTD
    SELECT 
        station_complex,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2023 THEN ridership ELSE 0 END) AS total_riders_2023,
        SUM(CASE WHEN YEAR(transit_timestamp) = 2024 THEN ridership ELSE 0 END) AS total_riders_2024,
        MIN(latitude) AS latitude,   -- Add latitude
        MIN(longitude) AS longitude  -- Add longitude
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        YEAR(transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex
),
weekday_weekend AS (
    -- Calculate weekday vs. weekend ridership and percentage of transfers
    SELECT 
        station_complex,
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END AS day_type,
        AVG(daily_ridership) AS avg_ridership,
        SUM(daily_transfers) * 1.0 / SUM(daily_ridership) AS transfer_percentage
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex, 
        CASE 
            WHEN weekday IN (2, 3, 4, 5, 6) THEN 'weekday'
            ELSE 'weekend'
        END
),
single_day_stats AS (
    -- Highest and lowest single day ridership
    SELECT 
        station_complex,
        MAX(daily_ridership) AS highest_single_day_ridership,
        MIN(daily_ridership) AS lowest_single_day_ridership
    FROM 
        ridership_by_day
    GROUP BY 
        station_complex
),
highest_single_day AS (
    -- Day with highest single day ridership
    SELECT 
        station_complex,
        day AS highest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MAX(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
),
lowest_single_day AS (
    -- Day with lowest single day ridership
    SELECT 
        station_complex,
        day AS lowest_single_day_ridership_day
    FROM 
        ridership_by_day rbd
    WHERE 
        rbd.daily_ridership = (
            SELECT 
                MIN(daily_ridership)
            FROM 
                ridership_by_day
            WHERE 
                station_complex = rbd.station_complex
        )
)
SELECT 
    rbd.station_complex,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS avg_weekday_ridership,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) AS avg_weekend_ridership,
    sds.highest_single_day_ridership,
    hsd.highest_single_day_ridership_day,
    sds.lowest_single_day_ridership,
    lsd.lowest_single_day_ridership_day,
    MAX(ty.total_riders_2023) AS total_riders_2023,
    MAX(ty.total_riders_2024) AS total_riders_2024,
    MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekday_transfer_percentage,
    MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.avg_ridership END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.avg_ridership END) AS weekend_ridership_percentage_change,
    (MAX(CASE WHEN wwd.day_type = 'weekend' THEN wwd.transfer_percentage END) - MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END)) * 1.0 / MAX(CASE WHEN wwd.day_type = 'weekday' THEN wwd.transfer_percentage END) AS weekend_transfer_percentage_change,
    MAX(rbd.latitude) AS latitude,   -- Add latitude
    MAX(rbd.longitude) AS longitude  -- Add longitude
FROM 
    ridership_by_day rbd
JOIN 
    weekday_weekend wwd ON rbd.station_complex = wwd.station_complex
JOIN 
    single_day_stats sds ON rbd.station_complex = sds.station_complex
JOIN 
    highest_single_day hsd ON rbd.station_complex = hsd.station_complex
JOIN 
    lowest_single_day lsd ON rbd.station_complex = lsd.station_complex
JOIN 
    total_riders_ytd ty ON rbd.station_complex = ty.station_complex
GROUP BY 
    rbd.station_complex, 
    sds.highest_single_day_ridership, 
    sds.lowest_single_day_ridership, 
    hsd.highest_single_day_ridership_day, 
    lsd.lowest_single_day_ridership_day
ORDER BY 
    rbd.station_complex
    );
  
  
[0m20:15:46.356927 [debug] [Thread-11 ]: SQL status: OK in 0.141 seconds
[0m20:15:46.359095 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:15:46.359579 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */

  
    
    

    create  table
      "mtastats"."main"."omny_adoption_increase__dbt_tmp"
  
    as (
      WITH omny_ridership_by_station_year AS (
    -- Calculate the OMNY ridership and total ridership for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        EXTRACT(YEAR FROM transit_timestamp) AS year, 
        SUM(CASE WHEN payment_method = 'omny' THEN ridership ELSE 0 END) AS omny_ridership,
        SUM(ridership) AS total_ridership
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    WHERE 
        EXTRACT(YEAR FROM transit_timestamp) IN (2023, 2024)
    GROUP BY 
        station_complex_id, station_complex, latitude, longitude, year
),
omny_percentage_by_station AS (
    -- Calculate the OMNY percentage for each station in 2023 and 2024, including latitude and longitude
    SELECT 
        station_complex_id, 
        station_complex, 
        latitude, 
        longitude, 
        year, 
        (omny_ridership / total_ridership) AS omny_percentage
    FROM 
        omny_ridership_by_station_year
)
SELECT 
    s2023.station_complex_id AS station_id, 
    s2023.station_complex AS station_name,
    s2023.latitude,
    s2023.longitude,
    s2023.omny_percentage AS omny_percentage_2023,
    s2024.omny_percentage AS omny_percentage_2024,
    (s2024.omny_percentage - s2023.omny_percentage) AS omny_percentage_increase
FROM 
    omny_percentage_by_station s2023
JOIN 
    omny_percentage_by_station s2024 
    ON s2023.station_complex_id = s2024.station_complex_id
    AND s2023.latitude = s2024.latitude
    AND s2023.longitude = s2024.longitude
    AND s2023.year = 2023
    AND s2024.year = 2024
WHERE 
    s2024.omny_percentage > s2023.omny_percentage
ORDER BY 
    omny_percentage_increase DESC
    );
  
  
[0m20:15:46.429227 [debug] [Thread-13 ]: SQL status: OK in 0.212 seconds
[0m20:15:46.430124 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:15:46.430526 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."total_riders_per_station__dbt_tmp"
  
    as (
      SELECT 
    station_complex_id, 
    station_complex, 
    latitude, 
    longitude, 
    SUM(ridership) AS total_ridership
FROM 
    "mtastats"."main"."mta_hourly_subway_socrata"
GROUP BY 
    station_complex_id, station_complex, latitude, longitude
ORDER BY 
    total_ridership DESC
    );
  
  
[0m20:15:46.435065 [debug] [Thread-14 ]: SQL status: OK in 0.215 seconds
[0m20:15:46.435805 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:15:46.436546 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */

  
    
    

    create  table
      "mtastats"."main"."weekly_riders_per_station__dbt_tmp"
  
    as (
      WITH weekly_ridership AS (
    SELECT 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp) AS week_start,
        SUM(ridership) AS total_weekly_ridership,
        MIN(latitude) AS latitude,  -- Assuming latitude is the same for each station complex, use MIN() or MAX()
        MIN(longitude) AS longitude  -- Assuming longitude is the same for each station complex, use MIN() or MAX()
    FROM 
        "mtastats"."main"."mta_hourly_subway_socrata"
    GROUP BY 
        station_complex, 
        DATE_TRUNC('week', transit_timestamp)
),
weekly_weather AS (
    SELECT 
        DATE_TRUNC('week', date) AS week_start,
        AVG(temperature_mean) AS avg_weekly_temperature,
        SUM(precipitation_sum) AS total_weekly_precipitation
    FROM 
        daily_weather_asset
    GROUP BY 
        DATE_TRUNC('week', date)
)
SELECT 
    wr.station_complex, 
    wr.week_start, 
    wr.total_weekly_ridership,
    wr.latitude,
    wr.longitude,
    ww.avg_weekly_temperature,
    ww.total_weekly_precipitation
FROM 
    weekly_ridership wr
LEFT JOIN 
    weekly_weather ww
ON 
    wr.week_start = ww.week_start
WHERE 
    wr.week_start < '2024-09-17'
ORDER BY 
    wr.station_complex, 
    wr.week_start
    );
  
  
[0m20:15:48.410922 [debug] [Thread-5 (]: SQL status: OK in 2.173 seconds
[0m20:15:48.490191 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:15:48.491887 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
alter table "mtastats"."main"."expense_type_per_year__dbt_tmp" rename to "expense_type_per_year"
[0m20:15:48.494321 [debug] [Thread-5 (]: SQL status: OK in 0.001 seconds
[0m20:15:48.776150 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m20:15:48.777929 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:15:48.780190 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: COMMIT
[0m20:15:48.793216 [debug] [Thread-5 (]: SQL status: OK in 0.012 seconds
[0m20:15:48.817662 [debug] [Thread-5 (]: Using duckdb connection "model.mta.expense_type_per_year"
[0m20:15:48.818440 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.expense_type_per_year"} */
drop table if exists "mtastats"."main"."expense_type_per_year__dbt_backup" cascade
[0m20:15:48.819600 [debug] [Thread-5 (]: SQL status: OK in 0.001 seconds
[0m20:15:48.823667 [debug] [Thread-5 (]: On model.mta.expense_type_per_year: Close
[0m20:15:48.826794 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea65a2f0>]}
[0m20:15:48.828377 [info ] [Thread-5 (]: 5 of 14 OK created sql table model main.expense_type_per_year .................. [[32mOK[0m in 2.84s]
[0m20:15:48.829788 [debug] [Thread-5 (]: Finished running node model.mta.expense_type_per_year
[0m20:15:48.853041 [debug] [Thread-8 (]: SQL status: OK in 2.595 seconds
[0m20:15:48.861474 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:15:48.862796 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
alter table "mtastats"."main"."labor_expenses_per_agency__dbt_tmp" rename to "labor_expenses_per_agency"
[0m20:15:48.867733 [debug] [Thread-8 (]: SQL status: OK in 0.003 seconds
[0m20:15:48.871434 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m20:15:48.872941 [debug] [Thread-2 (]: SQL status: OK in 2.645 seconds
[0m20:15:48.874253 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:15:48.884878 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:15:48.886443 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: COMMIT
[0m20:15:48.887029 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
alter table "mtastats"."main"."bond_payment_info__dbt_tmp" rename to "bond_payment_info"
[0m20:15:48.888669 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m20:15:48.891239 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m20:15:48.892325 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:15:48.893257 [debug] [Thread-2 (]: On model.mta.bond_payment_info: COMMIT
[0m20:15:48.895828 [debug] [Thread-8 (]: SQL status: OK in 0.008 seconds
[0m20:15:48.901870 [debug] [Thread-8 (]: Using duckdb connection "model.mta.labor_expenses_per_agency"
[0m20:15:48.903395 [debug] [Thread-2 (]: SQL status: OK in 0.008 seconds
[0m20:15:48.904237 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.labor_expenses_per_agency"} */
drop table if exists "mtastats"."main"."labor_expenses_per_agency__dbt_backup" cascade
[0m20:15:48.952563 [debug] [Thread-2 (]: Using duckdb connection "model.mta.bond_payment_info"
[0m20:15:48.953882 [debug] [Thread-2 (]: On model.mta.bond_payment_info: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.bond_payment_info"} */
drop table if exists "mtastats"."main"."bond_payment_info__dbt_backup" cascade
[0m20:15:48.954909 [debug] [Thread-9 (]: SQL status: OK in 2.686 seconds
[0m20:15:48.955875 [debug] [Thread-8 (]: SQL status: OK in 0.019 seconds
[0m20:15:48.969811 [debug] [Thread-8 (]: On model.mta.labor_expenses_per_agency: Close
[0m20:15:48.986695 [debug] [Thread-8 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea65aad0>]}
[0m20:15:48.987762 [debug] [Thread-2 (]: SQL status: OK in 0.031 seconds
[0m20:15:48.994940 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:15:49.000223 [debug] [Thread-2 (]: On model.mta.bond_payment_info: Close
[0m20:15:48.996671 [info ] [Thread-8 (]: 8 of 14 OK created sql table model main.labor_expenses_per_agency .............. [[32mOK[0m in 3.00s]
[0m20:15:49.001627 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
alter table "mtastats"."main"."largest_expense_differences_2023__dbt_tmp" rename to "largest_expense_differences_2023"
[0m20:15:49.002693 [debug] [Thread-7 (]: SQL status: OK in 2.760 seconds
[0m20:15:49.003977 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea659090>]}
[0m20:15:49.005280 [debug] [Thread-8 (]: Finished running node model.mta.labor_expenses_per_agency
[0m20:15:49.026066 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:15:49.027531 [info ] [Thread-2 (]: 2 of 14 OK created sql table model main.bond_payment_info ...................... [[32mOK[0m in 3.03s]
[0m20:15:49.028786 [debug] [Thread-9 (]: SQL status: OK in 0.023 seconds
[0m20:15:49.029971 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
alter table "mtastats"."main"."forecast_accuracy_2023__dbt_tmp" rename to "forecast_accuracy_2023"
[0m20:15:49.031173 [debug] [Thread-2 (]: Finished running node model.mta.bond_payment_info
[0m20:15:49.034590 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m20:15:49.042351 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:15:49.043389 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: COMMIT
[0m20:15:49.044884 [debug] [Thread-7 (]: SQL status: OK in 0.009 seconds
[0m20:15:49.048591 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m20:15:49.049579 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:15:49.050604 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: COMMIT
[0m20:15:49.052418 [debug] [Thread-9 (]: SQL status: OK in 0.008 seconds
[0m20:15:49.056536 [debug] [Thread-9 (]: Using duckdb connection "model.mta.largest_expense_differences_2023"
[0m20:15:49.057442 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.largest_expense_differences_2023"} */
drop table if exists "mtastats"."main"."largest_expense_differences_2023__dbt_backup" cascade
[0m20:15:49.093129 [debug] [Thread-7 (]: SQL status: OK in 0.041 seconds
[0m20:15:49.098600 [debug] [Thread-7 (]: Using duckdb connection "model.mta.forecast_accuracy_2023"
[0m20:15:49.099433 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.forecast_accuracy_2023"} */
drop table if exists "mtastats"."main"."forecast_accuracy_2023__dbt_backup" cascade
[0m20:15:49.104826 [debug] [Thread-9 (]: SQL status: OK in 0.046 seconds
[0m20:15:49.108202 [debug] [Thread-9 (]: On model.mta.largest_expense_differences_2023: Close
[0m20:15:49.109533 [debug] [Thread-9 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea65ac80>]}
[0m20:15:49.110751 [info ] [Thread-9 (]: 9 of 14 OK created sql table model main.largest_expense_differences_2023 ....... [[32mOK[0m in 3.12s]
[0m20:15:49.111773 [debug] [Thread-7 (]: SQL status: OK in 0.012 seconds
[0m20:15:49.113346 [debug] [Thread-9 (]: Finished running node model.mta.largest_expense_differences_2023
[0m20:15:49.116106 [debug] [Thread-7 (]: On model.mta.forecast_accuracy_2023: Close
[0m20:15:49.117803 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea65a530>]}
[0m20:15:49.118851 [info ] [Thread-7 (]: 7 of 14 OK created sql table model main.forecast_accuracy_2023 ................. [[32mOK[0m in 3.13s]
[0m20:15:49.119872 [debug] [Thread-7 (]: Finished running node model.mta.forecast_accuracy_2023
[0m20:15:50.135590 [debug] [Thread-4 (]: SQL status: OK in 3.901 seconds
[0m20:15:50.147664 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:15:50.150345 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
alter table "mtastats"."main"."daily_ridership__dbt_tmp" rename to "daily_ridership"
[0m20:15:50.152011 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m20:15:50.155599 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m20:15:50.165519 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:15:50.166898 [debug] [Thread-4 (]: On model.mta.daily_ridership: COMMIT
[0m20:15:50.181825 [debug] [Thread-4 (]: SQL status: OK in 0.014 seconds
[0m20:15:50.187122 [debug] [Thread-4 (]: Using duckdb connection "model.mta.daily_ridership"
[0m20:15:50.187830 [debug] [Thread-4 (]: On model.mta.daily_ridership: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.daily_ridership"} */
drop table if exists "mtastats"."main"."daily_ridership__dbt_backup" cascade
[0m20:15:50.189164 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m20:15:50.191789 [debug] [Thread-4 (]: On model.mta.daily_ridership: Close
[0m20:15:50.192983 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ce84406a0>]}
[0m20:15:50.194647 [info ] [Thread-4 (]: 4 of 14 OK created sql table model main.daily_ridership ........................ [[32mOK[0m in 4.21s]
[0m20:15:50.196223 [debug] [Thread-4 (]: Finished running node model.mta.daily_ridership
[0m20:15:58.237502 [debug] [Thread-3 (]: SQL status: OK in 12.007 seconds
[0m20:15:58.270510 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:15:58.271760 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
alter table "mtastats"."main"."busiest_specific_times__dbt_tmp" rename to "busiest_specific_times"
[0m20:15:58.273070 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m20:15:58.286638 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m20:15:58.304970 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:15:58.305830 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: COMMIT
[0m20:15:58.298192 [debug] [Thread-1 (]: SQL status: OK in 12.069 seconds
[0m20:15:58.321143 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:15:58.322082 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
alter table "mtastats"."main"."avg_riders_per_day__dbt_tmp" rename to "avg_riders_per_day"
[0m20:15:58.331252 [debug] [Thread-3 (]: SQL status: OK in 0.024 seconds
[0m20:15:58.338898 [debug] [Thread-3 (]: Using duckdb connection "model.mta.busiest_specific_times"
[0m20:15:58.355524 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.busiest_specific_times"} */
drop table if exists "mtastats"."main"."busiest_specific_times__dbt_backup" cascade
[0m20:15:58.357278 [debug] [Thread-1 (]: SQL status: OK in 0.034 seconds
[0m20:15:58.362887 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m20:15:58.363951 [debug] [Thread-3 (]: SQL status: OK in 0.007 seconds
[0m20:15:58.379564 [debug] [Thread-3 (]: On model.mta.busiest_specific_times: Close
[0m20:15:58.381378 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea658df0>]}
[0m20:15:58.382951 [info ] [Thread-3 (]: 3 of 14 OK created sql table model main.busiest_specific_times ................. [[32mOK[0m in 12.40s]
[0m20:15:58.375036 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:15:58.385074 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: COMMIT
[0m20:15:58.384235 [debug] [Thread-3 (]: Finished running node model.mta.busiest_specific_times
[0m20:15:58.414804 [debug] [Thread-1 (]: SQL status: OK in 0.029 seconds
[0m20:15:58.463424 [debug] [Thread-1 (]: Using duckdb connection "model.mta.avg_riders_per_day"
[0m20:15:58.464636 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.avg_riders_per_day"} */
drop table if exists "mtastats"."main"."avg_riders_per_day__dbt_backup" cascade
[0m20:15:58.466522 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m20:15:58.469467 [debug] [Thread-1 (]: On model.mta.avg_riders_per_day: Close
[0m20:15:58.470655 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea658d30>]}
[0m20:15:58.471814 [info ] [Thread-1 (]: 1 of 14 OK created sql table model main.avg_riders_per_day ..................... [[32mOK[0m in 12.50s]
[0m20:15:58.472843 [debug] [Thread-1 (]: Finished running node model.mta.avg_riders_per_day
[0m20:16:01.131964 [debug] [Thread-10 ]: SQL status: OK in 14.853 seconds
[0m20:16:01.139131 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:16:01.155438 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
alter table "mtastats"."main"."omny_adoption_by_station__dbt_tmp" rename to "omny_adoption_by_station"
[0m20:16:01.157220 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m20:16:01.162084 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m20:16:01.163790 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:16:01.164605 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: COMMIT
[0m20:16:01.197131 [debug] [Thread-10 ]: SQL status: OK in 0.032 seconds
[0m20:16:01.205609 [debug] [Thread-10 ]: Using duckdb connection "model.mta.omny_adoption_by_station"
[0m20:16:01.207005 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_by_station"} */
drop table if exists "mtastats"."main"."omny_adoption_by_station__dbt_backup" cascade
[0m20:16:01.209512 [debug] [Thread-10 ]: SQL status: OK in 0.001 seconds
[0m20:16:01.219170 [debug] [Thread-10 ]: On model.mta.omny_adoption_by_station: Close
[0m20:16:01.222341 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea658e50>]}
[0m20:16:01.224523 [info ] [Thread-10 ]: 10 of 14 OK created sql table model main.omny_adoption_by_station .............. [[32mOK[0m in 15.23s]
[0m20:16:01.226740 [debug] [Thread-10 ]: Finished running node model.mta.omny_adoption_by_station
[0m20:16:07.325880 [debug] [Thread-14 ]: SQL status: OK in 20.888 seconds
[0m20:16:07.335275 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:16:07.336310 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
alter table "mtastats"."main"."weekly_riders_per_station__dbt_tmp" rename to "weekly_riders_per_station"
[0m20:16:07.337972 [debug] [Thread-14 ]: SQL status: OK in 0.001 seconds
[0m20:16:07.343024 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m20:16:07.344130 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:16:07.365037 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: COMMIT
[0m20:16:07.450834 [debug] [Thread-14 ]: SQL status: OK in 0.085 seconds
[0m20:16:07.455941 [debug] [Thread-14 ]: Using duckdb connection "model.mta.weekly_riders_per_station"
[0m20:16:07.457017 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.weekly_riders_per_station"} */
drop table if exists "mtastats"."main"."weekly_riders_per_station__dbt_backup" cascade
[0m20:16:07.458304 [debug] [Thread-14 ]: SQL status: OK in 0.000 seconds
[0m20:16:07.460983 [debug] [Thread-14 ]: On model.mta.weekly_riders_per_station: Close
[0m20:16:07.462284 [debug] [Thread-14 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea664520>]}
[0m20:16:07.494794 [info ] [Thread-14 ]: 14 of 14 OK created sql table model main.weekly_riders_per_station ............. [[32mOK[0m in 21.44s]
[0m20:16:07.496544 [debug] [Thread-14 ]: Finished running node model.mta.weekly_riders_per_station
[0m20:16:07.668949 [debug] [Thread-11 ]: SQL status: OK in 21.309 seconds
[0m20:16:07.675674 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:16:07.676824 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
alter table "mtastats"."main"."omny_adoption_increase__dbt_tmp" rename to "omny_adoption_increase"
[0m20:16:07.678616 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m20:16:07.681981 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m20:16:07.683036 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:16:07.705387 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: COMMIT
[0m20:16:07.714413 [debug] [Thread-11 ]: SQL status: OK in 0.007 seconds
[0m20:16:07.720531 [debug] [Thread-11 ]: Using duckdb connection "model.mta.omny_adoption_increase"
[0m20:16:07.721587 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.omny_adoption_increase"} */
drop table if exists "mtastats"."main"."omny_adoption_increase__dbt_backup" cascade
[0m20:16:07.723320 [debug] [Thread-11 ]: SQL status: OK in 0.001 seconds
[0m20:16:07.726601 [debug] [Thread-11 ]: On model.mta.omny_adoption_increase: Close
[0m20:16:07.745377 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea6594b0>]}
[0m20:16:07.747290 [info ] [Thread-11 ]: 11 of 14 OK created sql table model main.omny_adoption_increase ................ [[32mOK[0m in 21.75s]
[0m20:16:07.749655 [debug] [Thread-11 ]: Finished running node model.mta.omny_adoption_increase
[0m20:16:08.696264 [debug] [Thread-13 ]: SQL status: OK in 22.264 seconds
[0m20:16:08.703401 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:16:08.704473 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
alter table "mtastats"."main"."total_riders_per_station__dbt_tmp" rename to "total_riders_per_station"
[0m20:16:08.710741 [debug] [Thread-13 ]: SQL status: OK in 0.005 seconds
[0m20:16:08.746251 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m20:16:08.747340 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:16:08.748166 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: COMMIT
[0m20:16:08.763459 [debug] [Thread-13 ]: SQL status: OK in 0.014 seconds
[0m20:16:08.771656 [debug] [Thread-13 ]: Using duckdb connection "model.mta.total_riders_per_station"
[0m20:16:08.772726 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.total_riders_per_station"} */
drop table if exists "mtastats"."main"."total_riders_per_station__dbt_backup" cascade
[0m20:16:08.777732 [debug] [Thread-13 ]: SQL status: OK in 0.002 seconds
[0m20:16:08.817430 [debug] [Thread-13 ]: On model.mta.total_riders_per_station: Close
[0m20:16:08.819155 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea658310>]}
[0m20:16:08.821515 [info ] [Thread-13 ]: 13 of 14 OK created sql table model main.total_riders_per_station .............. [[32mOK[0m in 22.81s]
[0m20:16:08.831790 [debug] [Thread-13 ]: Finished running node model.mta.total_riders_per_station
[0m20:16:08.919196 [debug] [Thread-12 ]: SQL status: OK in 22.596 seconds
[0m20:16:08.932388 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:16:08.939432 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
alter table "mtastats"."main"."subway_station_stats__dbt_tmp" rename to "subway_station_stats"
[0m20:16:08.941672 [debug] [Thread-12 ]: SQL status: OK in 0.001 seconds
[0m20:16:08.945617 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m20:16:08.946607 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:16:08.948060 [debug] [Thread-12 ]: On model.mta.subway_station_stats: COMMIT
[0m20:16:08.965743 [debug] [Thread-12 ]: SQL status: OK in 0.017 seconds
[0m20:16:08.974080 [debug] [Thread-12 ]: Using duckdb connection "model.mta.subway_station_stats"
[0m20:16:08.975386 [debug] [Thread-12 ]: On model.mta.subway_station_stats: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.subway_station_stats"} */
drop table if exists "mtastats"."main"."subway_station_stats__dbt_backup" cascade
[0m20:16:08.976779 [debug] [Thread-12 ]: SQL status: OK in 0.000 seconds
[0m20:16:08.979592 [debug] [Thread-12 ]: On model.mta.subway_station_stats: Close
[0m20:16:08.980978 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea658520>]}
[0m20:16:08.984124 [info ] [Thread-12 ]: 12 of 14 OK created sql table model main.subway_station_stats .................. [[32mOK[0m in 22.97s]
[0m20:16:08.986724 [debug] [Thread-12 ]: Finished running node model.mta.subway_station_stats
[0m20:16:09.251060 [debug] [Thread-6 (]: SQL status: OK in 22.988 seconds
[0m20:16:09.257409 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:16:09.258097 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
alter table "mtastats"."main"."fare_class_boro__dbt_tmp" rename to "fare_class_boro"
[0m20:16:09.259782 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m20:16:09.262282 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m20:16:09.262852 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:16:09.263308 [debug] [Thread-6 (]: On model.mta.fare_class_boro: COMMIT
[0m20:16:09.269866 [debug] [Thread-6 (]: SQL status: OK in 0.006 seconds
[0m20:16:09.273333 [debug] [Thread-6 (]: Using duckdb connection "model.mta.fare_class_boro"
[0m20:16:09.273935 [debug] [Thread-6 (]: On model.mta.fare_class_boro: /* {"app": "dbt", "dbt_version": "1.8.7", "profile_name": "default", "target_name": "dev", "node_id": "model.mta.fare_class_boro"} */
drop table if exists "mtastats"."main"."fare_class_boro__dbt_backup" cascade
[0m20:16:09.275276 [debug] [Thread-6 (]: SQL status: OK in 0.001 seconds
[0m20:16:09.277215 [debug] [Thread-6 (]: On model.mta.fare_class_boro: Close
[0m20:16:09.487187 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c349bea0-e27d-4f0a-842d-f4338a976378', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cac2fcc70>]}
[0m20:16:09.488608 [info ] [Thread-6 (]: 6 of 14 OK created sql table model main.fare_class_boro ........................ [[32mOK[0m in 23.50s]
[0m20:16:09.490119 [debug] [Thread-6 (]: Finished running node model.mta.fare_class_boro
[0m20:16:09.499460 [debug] [MainThread]: Using duckdb connection "master"
[0m20:16:09.500838 [debug] [MainThread]: On master: BEGIN
[0m20:16:09.501361 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:16:09.529809 [debug] [MainThread]: SQL status: OK in 0.028 seconds
[0m20:16:09.530454 [debug] [MainThread]: On master: COMMIT
[0m20:16:09.530854 [debug] [MainThread]: Using duckdb connection "master"
[0m20:16:09.531166 [debug] [MainThread]: On master: COMMIT
[0m20:16:09.531953 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m20:16:09.532400 [debug] [MainThread]: On master: Close
[0m20:16:09.536150 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:16:09.536732 [debug] [MainThread]: Connection 'model.mta.expense_type_per_year' was properly closed.
[0m20:16:09.537067 [debug] [MainThread]: Connection 'model.mta.avg_riders_per_day' was properly closed.
[0m20:16:09.537345 [debug] [MainThread]: Connection 'model.mta.bond_payment_info' was properly closed.
[0m20:16:09.537610 [debug] [MainThread]: Connection 'model.mta.busiest_specific_times' was properly closed.
[0m20:16:09.537864 [debug] [MainThread]: Connection 'model.mta.daily_ridership' was properly closed.
[0m20:16:09.538122 [debug] [MainThread]: Connection 'model.mta.fare_class_boro' was properly closed.
[0m20:16:09.538369 [debug] [MainThread]: Connection 'model.mta.forecast_accuracy_2023' was properly closed.
[0m20:16:09.538617 [debug] [MainThread]: Connection 'model.mta.labor_expenses_per_agency' was properly closed.
[0m20:16:09.538859 [debug] [MainThread]: Connection 'model.mta.largest_expense_differences_2023' was properly closed.
[0m20:16:09.539106 [debug] [MainThread]: Connection 'model.mta.omny_adoption_by_station' was properly closed.
[0m20:16:09.539360 [debug] [MainThread]: Connection 'model.mta.omny_adoption_increase' was properly closed.
[0m20:16:09.539607 [debug] [MainThread]: Connection 'model.mta.subway_station_stats' was properly closed.
[0m20:16:09.539883 [debug] [MainThread]: Connection 'model.mta.total_riders_per_station' was properly closed.
[0m20:16:09.540131 [debug] [MainThread]: Connection 'model.mta.weekly_riders_per_station' was properly closed.
[0m20:16:09.540825 [info ] [MainThread]: 
[0m20:16:09.541343 [info ] [MainThread]: Finished running 14 table models in 0 hours 0 minutes and 23.79 seconds (23.79s).
[0m20:16:09.543491 [debug] [MainThread]: Command end result
[0m20:16:09.587217 [info ] [MainThread]: 
[0m20:16:09.588203 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:16:09.588914 [info ] [MainThread]: 
[0m20:16:09.590176 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m20:16:09.600032 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 24.777311, "process_user_time": 248.249, "process_kernel_time": 64.28364, "process_mem_max_rss": "784164", "process_in_blocks": "40", "process_out_blocks": "22968"}
[0m20:16:09.601497 [debug] [MainThread]: Command `dbt run` succeeded at 20:16:09.601274 after 24.78 seconds
[0m20:16:09.602218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cf7487400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cf67af1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cea5a9b70>]}
[0m20:16:09.602718 [debug] [MainThread]: Flushing usage events
