nfl/__init__.py
import os
from dagster import Definitions
from dagster_duckdb_polars import DuckDBPolarsIOManager
from .assets import nfl, update_pbp, bloombet_nfl
from .assets.bloombet_nfl import bloombet_nfl_data
from .assets.nfl import nfl_pbp_2024, nfl_players, nfl_weekly_rosters_2024, nfl_officials
from .assets.update_pbp import update_pbp_with_players 
from .assets.dbt import dbt_project_assets  # Import the dbt assets
from .assets.bi_export import analytics_cube  # Import the analytics_cube asset
from dagster_dbt import DbtCliResource
from .dbt_project import dbt_project

# Set the database path for raw data
DATABASE_PATH = os.getenv("DATABASE_PATH", "data/database_raw.duckdb")

# Define the assets (Parquet data and Bloombet API data)
assets = [
    nfl_pbp_2024,
    bloombet_nfl_data,
    nfl_players,
    nfl_weekly_rosters_2024, 
    nfl_officials,
    update_pbp_with_players,
    dbt_project_assets,  # Add the dbt assets here
    analytics_cube  # Add the analytics_cube asset here
]

# Resource definitions, including the I/O manager for DuckDB
resources = {
    "dbt": DbtCliResource(project_dir=dbt_project),
    "io_manager": DuckDBPolarsIOManager(database=DATABASE_PATH, schema="main"),
}

# Dagster Definitions for assets and resources
defs = Definitions(
    assets=assets,
    resources=resources
)

nfl/assets/nfl.py
import io
import zipfile

import httpx
import polars as pl
from dagster import asset
from slugify import slugify


@asset(
    compute_kind="Polars",  # Specify the compute kind
)
def nfl_pbp_2024() -> pl.DataFrame:
    """
    NFL pbp data from NFLFastR
    """
    nfl_pbp_2024_url = (
        "https://github.com/nflverse/nflverse-data/releases/download/pbp/play_by_play_2024.parquet"
    )

    return pl.read_parquet(nfl_pbp_2024_url)

@asset(
    group_name="nfl_seeds",
    compute_kind="Polars",  # Specify the compute kind
)
def nfl_players() -> pl.DataFrame:
    """
    NFL player ids from NFLFastR
    """
    nfl_pbp_2024_url = (
        "https://github.com/nflverse/nflverse-data/releases/download/players/players.parquet"
    )

    return pl.read_parquet(nfl_pbp_2024_url)

@asset(
    compute_kind="Polars",
    group_name="nfl_seeds",  # Specify the compute kind
)
def nfl_weekly_rosters_2024() -> pl.DataFrame:
    """
    NFL weekly rosters for the 2024 season
    """
    weekly_rosters_url = (
        "https://github.com/nflverse/nflverse-data/releases/download/weekly_rosters/roster_weekly_2024.parquet"
    )

    return pl.read_parquet(weekly_rosters_url)

@asset(
    group_name="nfl_seeds",
    compute_kind="Polars",  # Specify the compute kind
)
def nfl_officials() -> pl.DataFrame:
    """
    NFL officials' data
    """
    officials_url = (
        "https://github.com/nflverse/nflverse-data/releases/download/officials/officials.parquet"
    )

    return pl.read_parquet(officials_url)

nfl/assets/update_pbp.py
import os
import duckdb
from dagster import asset, get_dagster_logger, AssetIn

DATABASE_PATH = os.getenv("DATABASE_PATH", "data/database_raw.duckdb")

@asset(
    ins={
        "nfl_pbp_2024": AssetIn(),  # Define input from the nfl_pbp_2024 asset
        "nfl_players": AssetIn()     # Define input from the nfl_players asset
    },
    compute_kind="DuckDB",  # Specify the compute kind
)
def update_pbp_with_players(nfl_pbp_2024, nfl_players) -> None:  # Explicitly indicate no return
    """
    Joining Player Names and Positions to nfl_pbp_2024
    """
    logger = get_dagster_logger()
    
    logger.info("Connecting to DuckDB...")
    con = duckdb.connect(DATABASE_PATH)

    # Step 1: Add columns one by one (DuckDB limitations)
    columns_to_add = [
        "passer_display_name STRING",
        "passer_position STRING",
        "rusher_display_name STRING",
        "rusher_position STRING",
        "receiver_display_name STRING",
        "receiver_position STRING"
    ]

    for column in columns_to_add:
        try:
            con.execute(f"ALTER TABLE nfl_pbp_2024 ADD COLUMN {column}")
            logger.info(f"Column {column} added.")
        except Exception as e:
            logger.warning(f"Column {column} already exists or error: {e}")

    # Step 2: Perform updates for each new column
    logger.info("Updating player-related columns in nfl_pbp_2024...")

    # Update passer columns
    con.execute(f"""
        UPDATE nfl_pbp_2024
        SET passer_display_name = p.display_name, 
            passer_position = p.position
        FROM nfl_players p
        WHERE nfl_pbp_2024.passer_player_id = p.gsis_id
    """)

    # Update rusher columns
    con.execute(f"""
        UPDATE nfl_pbp_2024
        SET rusher_display_name = p.display_name, 
            rusher_position = p.position
        FROM nfl_players p
        WHERE nfl_pbp_2024.rusher_player_id = p.gsis_id
    """)

    # Update receiver columns
    con.execute(f"""
        UPDATE nfl_pbp_2024
        SET receiver_display_name = p.display_name, 
            receiver_position = p.position
        FROM nfl_players p
        WHERE nfl_pbp_2024.receiver_player_id = p.gsis_id
    """)

    # Step 3: Verify the results (Optional logging)
    result = con.execute("SELECT * FROM nfl_pbp_2024 LIMIT 10").fetchdf()
    logger.info(f"Updated PBP data: {result}")

    # Close the connection
    con.close()

nfl/assets/bloombet_nfl.py
import os
from datetime import datetime, timedelta
import polars as pl
from dotenv import load_dotenv
from dlt.sources.helpers import requests
from dagster import asset

# Load environment variables from .env file
load_dotenv()

@asset
def bloombet_nfl_data() -> pl.DataFrame:
    """
    Fetches and aggregates historical NFL data from the Bloombet API starting from
    September 24th, 2024, with 1-hour intervals.
    """
    api_key = os.getenv("BLOOMBET_API_KEY")
    sport = "nfl"
    start_time = datetime(2024, 9, 24, 0, 0, 0)
    time_interval = timedelta(hours=1)

    current_time = start_time
    aggregated_df = pl.DataFrame()

    while current_time <= datetime.now():
        print(f"Starting API call for {sport} at {current_time.strftime('%Y-%m-%d %H:%M:%S')}")

        response = requests.get(
            f'https://getbloombet.com/api/historical',
            params={
                'api_key': api_key,
                'sport': sport,
                'date': current_time.strftime('%Y-%m-%d %H:%M:%S')
            }
        )
        
        response.raise_for_status()
        print(f"API call successful for {sport} at {current_time.strftime('%Y-%m-%d %H:%M:%S')}")

        # Convert the JSON object to a Polars DataFrame
        data = response.json()
        df = pl.DataFrame(data)

        # Append the individual DataFrame to the aggregated DataFrame
        aggregated_df = pl.concat([aggregated_df, df], rechunk=True)

        # Drop the individual DataFrame to conserve memory
        del df

        print(f"Data for {current_time.strftime('%Y-%m-%d %H:%M:%S')} added to aggregated dataframe")

        # Move to the next interval
        current_time += time_interval

    return aggregated_df

nfl/assets/bi_export.py
from dagster import asset
from dagster_dbt import get_asset_key_for_model
from ..resources import DuckDBBIExport  # Ensure this is correctly imported
from .dbt import dbt_project_assets  # Importing dbt_project_assets

@asset(
    compute_kind="DuckDB",
    deps=[
        get_asset_key_for_model([dbt_project_assets], "penalties_per_team"),
        get_asset_key_for_model([dbt_project_assets], "penalty_totals"),
    ]
)
def analytics_cube():
    # Hardcoded values for testing
    db_raw_path = "/home/christianocean/quiv/data/database_raw.duckdb"
    sql_folder = "/home/christianocean/quiv/nfl/dbt/models"
    new_duckdb_path = "/home/christianocean/quiv/nflstats/sources/nflstats/analytics.duckdb"
    
    # Initialize the DuckDBBIExport class
    exporter = DuckDBBIExport(db_raw_path, sql_folder, new_duckdb_path)
    exporter.export_tables()  # This method should handle the export logic
    
    # Instead of returning the exporter, return a summary or DataFrame
    return {"status": "Export completed", "path": new_duckdb_path}  # Returning a dict as output

nfl/resources.py
import dlt
from dlt.sources.helpers import requests
from datetime import datetime, timedelta
import polars as pl
from dagster import ConfigurableResource
import duckdb
import os
from dagster import resource, Field, String

class BloombetAPI(ConfigurableResource):
    api_key: str
    sport: str
    start_time: datetime
    time_interval: timedelta

    def fetch_bloombet_data(self, date: datetime):
        """Fetch historical data from Bloombet API for a specific sport and date."""
        print(f"Starting API call for {self.sport} at {date.strftime('%Y-%m-%d %H:%M:%S')}")
        
        response = requests.get(
            f'https://getbloombet.com/api/historical',
            params={
                'api_key': self.api_key,
                'sport': self.sport,
                'date': date.strftime('%Y-%m-%d %H:%M:%S')
            }
        )
        
        response.raise_for_status()
        print(f"API call successful for {self.sport} at {date.strftime('%Y-%m-%d %H:%M:%S')}")
        
        return response.json()

    def aggregate_data(self):
        """Run the process and aggregate all results into one Polars DataFrame."""
        current_time = self.start_time
        aggregated_df = pl.DataFrame()  # Initialize an empty Polars DataFrame for aggregation

        while current_time <= datetime.now():
            print(f"Fetching data for {current_time.strftime('%Y-%m-%d %H:%M:%S')}")

            try:
                data = self.fetch_bloombet_data(current_time)
                
                # Convert the JSON object to a Polars DataFrame
                df = pl.DataFrame(data)
                
                # Append the individual DataFrame to the aggregated DataFrame
                aggregated_df = pl.concat([aggregated_df, df], rechunk=True)
                
                # Drop the individual DataFrame to conserve memory
                del df
                
                print(f"Data for {current_time.strftime('%Y-%m-%d %H:%M:%S')} added to aggregated dataframe")

            except Exception as e:
                print(f"Failed to fetch data for {current_time.strftime('%Y-%m-%d %H:%M:%S')} due to {e}")
            
            # Move to the next interval
            current_time += self.time_interval

        # Return the aggregated DataFrame
        return aggregated_df


class DuckDBBIExport:
    def __init__(self, db_raw_path: str, sql_folder: str, new_duckdb_path: str):
        self.db_raw_path = db_raw_path
        self.sql_folder = sql_folder
        self.new_duckdb_path = new_duckdb_path

    def export_tables(self):
        # Your logic for exporting tables
        pass  # Implement the existing logic here

@resource(config_schema={
    "db_raw_path": Field(String, is_required=True),
    "sql_folder": Field(String, is_required=True),
    "new_duckdb_path": Field(String, is_required=True),
})
def duckdb_bi_export(context):
    return DuckDBBIExport(
        db_raw_path=context.resource_config["db_raw_path"],
        sql_folder=context.resource_config["sql_folder"],
        new_duckdb_path=context.resource_config["new_duckdb_path"]
    )
nfl/assets/dbt.py

import io
import zipfile

import httpx
import polars as pl
from dagster import asset
from slugify import slugify


@asset(
    compute_kind="Polars",  # Specify the compute kind
)
def nfl_pbp_2024() -> pl.DataFrame:
    """
    NFL pbp data from NFLFastR
    """
    nfl_pbp_2024_url = (
        "https://github.com/nflverse/nflverse-data/releases/download/pbp/play_by_play_2024.parquet"
    )

    return pl.read_parquet(nfl_pbp_2024_url)

@asset(
    group_name="nfl_seeds",
    compute_kind="Polars",  # Specify the compute kind
)
def nfl_players() -> pl.DataFrame:
    """
    NFL player ids from NFLFastR
    """
    nfl_pbp_2024_url = (
        "https://github.com/nflverse/nflverse-data/releases/download/players/players.parquet"
    )

    return pl.read_parquet(nfl_pbp_2024_url)

@asset(
    compute_kind="Polars",
    group_name="nfl_seeds",  # Specify the compute kind
)
def nfl_weekly_rosters_2024() -> pl.DataFrame:
    """
    NFL weekly rosters for the 2024 season
    """
    weekly_rosters_url = (
        "https://github.com/nflverse/nflverse-data/releases/download/weekly_rosters/roster_weekly_2024.parquet"
    )

    return pl.read_parquet(weekly_rosters_url)

@asset(
    group_name="nfl_seeds",
    compute_kind="Polars",  # Specify the compute kind
)
def nfl_officials() -> pl.DataFrame:
    """
    NFL officials' data
    """
    officials_url = (
        "https://github.com/nflverse/nflverse-data/releases/download/officials/officials.parquet"
    )

    return pl.read_parquet(officials_url)

nfl/dbt_project.py
from pathlib import Path
from dagster_dbt import DbtProject

dbt_project = DbtProject(
    project_dir=Path(__file__).parent.joinpath("dbt").resolve(),  # Correctly reference the dbt directory
)

dbt_project.prepare_if_dev()



nfl/dbt/dbt_project.yml
import dlt
from dlt.sources.helpers import requests
from datetime import datetime, timedelta
import polars as pl
from dagster import ConfigurableResource
import duckdb
import os
from dagster import resource, Field, String

class BloombetAPI(ConfigurableResource):
    api_key: str
    sport: str
    start_time: datetime
    time_interval: timedelta

    def fetch_bloombet_data(self, date: datetime):
        """Fetch historical data from Bloombet API for a specific sport and date."""
        print(f"Starting API call for {self.sport} at {date.strftime('%Y-%m-%d %H:%M:%S')}")
        
        response = requests.get(
            f'https://getbloombet.com/api/historical',
            params={
                'api_key': self.api_key,
                'sport': self.sport,
                'date': date.strftime('%Y-%m-%d %H:%M:%S')
            }
        )
        
        response.raise_for_status()
        print(f"API call successful for {self.sport} at {date.strftime('%Y-%m-%d %H:%M:%S')}")
        
        return response.json()

    def aggregate_data(self):
        """Run the process and aggregate all results into one Polars DataFrame."""
        current_time = self.start_time
        aggregated_df = pl.DataFrame()  # Initialize an empty Polars DataFrame for aggregation

        while current_time <= datetime.now():
            print(f"Fetching data for {current_time.strftime('%Y-%m-%d %H:%M:%S')}")

            try:
                data = self.fetch_bloombet_data(current_time)
                
                # Convert the JSON object to a Polars DataFrame
                df = pl.DataFrame(data)
                
                # Append the individual DataFrame to the aggregated DataFrame
                aggregated_df = pl.concat([aggregated_df, df], rechunk=True)
                
                # Drop the individual DataFrame to conserve memory
                del df
                
                print(f"Data for {current_time.strftime('%Y-%m-%d %H:%M:%S')} added to aggregated dataframe")

            except Exception as e:
                print(f"Failed to fetch data for {current_time.strftime('%Y-%m-%d %H:%M:%S')} due to {e}")
            
            # Move to the next interval
            current_time += self.time_interval

        # Return the aggregated DataFrame
        return aggregated_df


class DuckDBBIExport:
    def __init__(self, db_raw_path: str, sql_folder: str, new_duckdb_path: str):
        self.db_raw_path = db_raw_path
        self.sql_folder = sql_folder
        self.new_duckdb_path = new_duckdb_path

    def export_tables(self):
        # Your logic for exporting tables
        pass  # Implement the existing logic here

@resource(config_schema={
    "db_raw_path": Field(String, is_required=True),
    "sql_folder": Field(String, is_required=True),
    "new_duckdb_path": Field(String, is_required=True),
})
def duckdb_bi_export(context):
    return DuckDBBIExport(
        db_raw_path=context.resource_config["db_raw_path"],
        sql_folder=context.resource_config["sql_folder"],
        new_duckdb_path=context.resource_config["new_duckdb_path"]
    )

nfl/dbt/profiles.yml
default:
  outputs:
    dev:
      type: duckdb
      path: "{{ env_var('DATABASE_PATH', '../../data/database_raw.duckdb') }}"
      threads: 16
      settings:
        enable_object_cache: true
        enable_http_metadata_cache: true
  target: dev

nfl/dbt/models/sources.yml
default:
  outputs:
    dev:
      type: duckdb
      path: "{{ env_var('DATABASE_PATH', '../../data/database_raw.duckdb') }}"
      threads: 16
      settings:
        enable_object_cache: true
        enable_http_metadata_cache: true
  target: dev





 