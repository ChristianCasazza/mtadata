{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell \n",
    "#Run this cell \n",
    "import sys\n",
    "from great_tables import GT\n",
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    # IPython is present only if a Jupyter environment is installed\n",
    "    from IPython.display import display, HTML\n",
    "    IN_NOTEBOOK = True\n",
    "except ImportError:\n",
    "    IN_NOTEBOOK = False\n",
    "\n",
    "try:\n",
    "    # Great Tables provides styling support for Polars\n",
    "    from great_tables import loc, style\n",
    "    HAS_GREAT_TABLES = True\n",
    "except ImportError:\n",
    "    HAS_GREAT_TABLES = False\n",
    "\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import sys\n",
    "try:\n",
    "    # IPython is present only if a Jupyter environment is installed\n",
    "    from IPython.display import display, HTML\n",
    "    IN_NOTEBOOK = True\n",
    "except ImportError:\n",
    "    IN_NOTEBOOK = False\n",
    "\n",
    "\n",
    "class DuckDBWrapper:\n",
    "    def __init__(self, duckdb_path=None):\n",
    "        \"\"\"\n",
    "        Initialize a DuckDB connection.\n",
    "        If duckdb_path is provided, a persistent DuckDB database will be used.\n",
    "        Otherwise, it creates an in-memory database.\n",
    "        \"\"\"\n",
    "        if duckdb_path:\n",
    "            self.con = duckdb.connect(str(duckdb_path), read_only=False)\n",
    "        else:\n",
    "            self.con = duckdb.connect(database=':memory:', read_only=False)\n",
    "        self.registered_tables = []\n",
    "\n",
    "        # Enable httpfs for remote paths if needed\n",
    "        self.con.execute(\"INSTALL httpfs;\")\n",
    "        self.con.execute(\"LOAD httpfs;\")\n",
    "\n",
    "    def register_data(self, paths, table_names):\n",
    "        \"\"\"\n",
    "        Registers local data files (Parquet, CSV, JSON) in DuckDB by creating views.\n",
    "        Automatically detects the file type based on the file extension.\n",
    "\n",
    "        Args:\n",
    "            paths (list): List of paths (strings or Path objects) to data files.\n",
    "            table_names (list): List of table names corresponding to the paths.\n",
    "        \"\"\"\n",
    "        if len(paths) != len(table_names):\n",
    "            raise ValueError(\"The number of paths must match the number of table names.\")\n",
    "\n",
    "        for path, table_name in zip(paths, table_names):\n",
    "            path_str = str(path)\n",
    "            file_extension = Path(path_str).suffix.lower()\n",
    "\n",
    "            if file_extension == \".parquet\":\n",
    "                query = f\"CREATE VIEW {table_name} AS SELECT * FROM read_parquet('{path_str}')\"\n",
    "            elif file_extension == \".csv\":\n",
    "                query = f\"CREATE VIEW {table_name} AS SELECT * FROM read_csv_auto('{path_str}')\"\n",
    "            elif file_extension == \".json\":\n",
    "                query = f\"CREATE VIEW {table_name} AS SELECT * FROM read_json_auto('{path_str}')\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type '{file_extension}' for file: {path_str}\")\n",
    "\n",
    "            self.con.execute(query)\n",
    "            self.registered_tables.append(table_name)\n",
    "\n",
    "    def bulk_register_data(self, repo_root, base_path, table_names, wildcard=\"*.parquet\"):\n",
    "        \"\"\"\n",
    "        Constructs paths for each table based on a shared base path plus the table name,\n",
    "        then appends a wildcard for file matching (e.g., '*.parquet'), and registers the data.\n",
    "\n",
    "        Args:\n",
    "            repo_root (Path): The root path of your repository.\n",
    "            base_path (str): The relative path from repo_root to your data directory.\n",
    "            table_names (list): The table names (and implicitly the folder names) to register.\n",
    "            wildcard (str, optional): A wildcard pattern for the files (default '*.parquet').\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "        for table_name in table_names:\n",
    "            path = Path(repo_root) / base_path / table_name / wildcard\n",
    "            paths.append(path)\n",
    "\n",
    "        self.register_data(paths, table_names)\n",
    "\n",
    "    def run_query(self, sql_query, show_results=False):\n",
    "        \"\"\"\n",
    "        Runs a SQL query on the registered tables in DuckDB and returns a Polars DataFrame.\n",
    "        Optionally displays the result. If running in a .ipynb environment and show_results=True,\n",
    "        we attempt to style the Polars DataFrame using Great Tables for better visuals.\n",
    "\n",
    "        Args:\n",
    "            sql_query (str): The SQL query string to execute.\n",
    "            show_results (bool): If True, display the result in a table (notebook or terminal).\n",
    "\n",
    "        Returns:\n",
    "            pl.DataFrame: Query result as a Polars DataFrame.\n",
    "        \"\"\"\n",
    "        # Convert the DuckDB query results to Polars via Arrow\n",
    "        arrow_table = self.con.execute(sql_query).arrow()\n",
    "        df = pl.DataFrame(arrow_table)\n",
    "\n",
    "        if show_results:\n",
    "            if IN_NOTEBOOK and HAS_GREAT_TABLES:\n",
    "                # 1) Create a styled table with Great Tables\n",
    "                styled = (\n",
    "                    df.style\n",
    "                    # Add a title & subtitle\n",
    "                    .tab_header(\n",
    "                        title=\"DuckDB Query Results\",\n",
    "                        subtitle=f\"{sql_query[:50]}...\"\n",
    "                    )\n",
    "                    # Example format: limit decimal places for numeric columns\n",
    "                    .fmt_number(cs.numeric(), decimals=3)\n",
    "                )\n",
    "\n",
    "                # 2) Convert styled table to HTML\n",
    "                styled_html = styled._repr_html_()\n",
    "\n",
    "                # 3) Wrap the HTML in a scrollable <div> for horizontal scroll\n",
    "                scrollable_html = f\"\"\"\n",
    "                <div style=\"max-width:100%; overflow-x:auto; white-space:nowrap;\">\n",
    "                    {styled_html}\n",
    "                </div>\n",
    "                \"\"\"\n",
    "\n",
    "                # 4) Display in notebook\n",
    "                display(HTML(scrollable_html))\n",
    "            else:\n",
    "                # If not in a notebook environment or great_tables isn't installed,\n",
    "                # fallback to existing Rich-based table printing\n",
    "                self.print_query_results(df, title=f\"Query: {sql_query[:50]}...\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def print_query_results(self, df, title=\"Query Results\"):\n",
    "        \"\"\"\n",
    "        Prints a Polars DataFrame as a Rich table. Uses a pager for vertical/horizontal scrolling in the terminal.\n",
    "\n",
    "        Args:\n",
    "            df (pl.DataFrame): The Polars DataFrame to display.\n",
    "            title (str): Title for the Rich table.\n",
    "        \"\"\"\n",
    "        console = Console()\n",
    "\n",
    "        # Use a pager for scrolling (works in many terminals, not always in notebooks)\n",
    "        with console.pager(styles=True):\n",
    "            table = Table(title=title, title_style=\"bold green\", show_lines=True)\n",
    "            # Add columns with some styling\n",
    "            for column in df.columns:\n",
    "                table.add_column(str(column), style=\"bold cyan\", overflow=\"fold\")\n",
    "\n",
    "            # Add rows\n",
    "            for row in df.iter_rows(named=True):\n",
    "                values = [str(row[col]) for col in df.columns]\n",
    "                table.add_row(*values, style=\"white on black\")\n",
    "\n",
    "            console.print(table)\n",
    "\n",
    "    def _construct_path(self, path, base_path, file_name, extension):\n",
    "        \"\"\"\n",
    "        Constructs the full file path based on input parameters.\n",
    "        \"\"\"\n",
    "        if path:\n",
    "            return Path(path)\n",
    "        elif base_path and file_name:\n",
    "            return Path(base_path) / f\"{file_name}.{extension}\"\n",
    "        else:\n",
    "            # Default file path: \"output.<extension>\" in the current directory\n",
    "            return Path(f\"output.{extension}\")\n",
    "\n",
    "    def export(self, result, file_type, path=None, base_path=None, file_name=None, with_header=True):\n",
    "        \"\"\"\n",
    "        Exports a Polars DataFrame (or anything convertible to Polars via .to_arrow()) \n",
    "        to the specified file type.\n",
    "\n",
    "        Args:\n",
    "            result (pl.DataFrame or DuckDB query result): The data to export.\n",
    "            file_type (str): Type of file to export ('parquet', 'csv', 'json').\n",
    "            path (str): Full path to the file (optional).\n",
    "            base_path (str): Directory path (optional).\n",
    "            file_name (str): Name of the file (without extension) (optional).\n",
    "            with_header (bool): Include header row for CSV (default: True).\n",
    "        \"\"\"\n",
    "        file_type = file_type.lower()\n",
    "        if file_type not in [\"parquet\", \"csv\", \"json\"]:\n",
    "            raise ValueError(\"file_type must be one of 'parquet', 'csv', or 'json'.\")\n",
    "\n",
    "        full_path = self._construct_path(path, base_path, file_name, file_type)\n",
    "        full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Convert result to a Polars DataFrame if needed\n",
    "        if isinstance(result, pl.DataFrame):\n",
    "            df = result\n",
    "        elif hasattr(result, \"to_arrow\"):\n",
    "            # e.g., a DuckDB result object\n",
    "            df = pl.DataFrame(result.to_arrow())\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported result type. Must be a Polars DataFrame or have a 'to_arrow()' method.\")\n",
    "\n",
    "        # Export based on file type\n",
    "        if file_type == \"parquet\":\n",
    "            df.write_parquet(str(full_path))\n",
    "        elif file_type == \"csv\":\n",
    "            # Use Polars parameter include_header instead of header or has_header\n",
    "            df.write_csv(str(full_path), separator=\",\", include_header=with_header)\n",
    "        elif file_type == \"json\":\n",
    "            df.write_ndjson(str(full_path))  # NDJSON format\n",
    "\n",
    "        print(f\"File written to: {full_path}\")\n",
    "\n",
    "    def show_tables(self):\n",
    "        \"\"\"\n",
    "        Displays the table names and types currently registered in the catalog \n",
    "        in a Rich-styled table (similar to show_schema).\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, table_type\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema='main'\n",
    "        \"\"\"\n",
    "        df = self.run_query(query)  # Polars DataFrame\n",
    "        console = Console()\n",
    "        table = Table(title=\"Registered Tables\", title_style=\"bold green\", show_lines=True)\n",
    "        table.add_column(\"Table Name\", justify=\"left\", style=\"bold yellow\")\n",
    "        table.add_column(\"Table Type\", justify=\"left\", style=\"bold cyan\")\n",
    "\n",
    "        for row in df.to_dicts():\n",
    "            table.add_row(row[\"table_name\"], row[\"table_type\"], style=\"white on black\")\n",
    "\n",
    "        console.print(table)\n",
    "\n",
    "    def show_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Displays the schema of the specified DuckDB table or view, \n",
    "        using Polars for the query result and printing in a Rich table.\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): Name of the table/view whose schema is to be displayed.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            column_name, \n",
    "            data_type\n",
    "        FROM \n",
    "            information_schema.columns \n",
    "        WHERE \n",
    "            table_name = '{table_name}'\n",
    "        \"\"\"\n",
    "        df = self.run_query(query)\n",
    "        console = Console()\n",
    "        schema_table = Table(title=f\"Schema for '{table_name}'\", title_style=\"bold green\")\n",
    "        schema_table.add_column(\"Column Name\", justify=\"left\", style=\"bold yellow\", no_wrap=True)\n",
    "        schema_table.add_column(\"Data Type\", justify=\"left\", style=\"bold cyan\")\n",
    "\n",
    "        for row in df.to_dicts():\n",
    "            col_name = row[\"column_name\"]\n",
    "            col_type = row[\"data_type\"]\n",
    "            schema_table.add_row(col_name, str(col_type), style=\"white on black\")\n",
    "\n",
    "        console.print(schema_table)\n",
    "\n",
    "    def show_parquet_schema(self, file_path):\n",
    "        \"\"\"\n",
    "        Reads a Parquet file directly using Polars, and prints its schema \n",
    "        in a visually appealing table using Rich. Includes row count info.\n",
    "        \"\"\"\n",
    "        df = pl.read_parquet(file_path)\n",
    "\n",
    "        console = Console()\n",
    "        schema_table = Table(title=\"Parquet Schema\", title_style=\"bold green\")\n",
    "        schema_table.add_column(\"Column Name\", justify=\"left\", style=\"bold yellow\", no_wrap=True)\n",
    "        schema_table.add_column(\"Data Type\", justify=\"left\", style=\"bold cyan\")\n",
    "\n",
    "        for col_name, col_dtype in df.schema.items():\n",
    "            schema_table.add_row(col_name, str(col_dtype), style=\"white on black\")\n",
    "\n",
    "        console.print(schema_table)\n",
    "        console.print(f\"[bold magenta]\\nNumber of rows:[/] [bold white]{df.height}[/]\")\n",
    "\n",
    "    def register_partitioned_data(self, base_path, table_name, wildcard=\"*/*/*.parquet\"):\n",
    "        \"\"\"\n",
    "        Registers partitioned Parquet data using Hive partitioning by creating a view.\n",
    "\n",
    "        Args:\n",
    "            base_path (str or Path): The base directory where partitioned files are located.\n",
    "            table_name (str): Name of the view to be created.\n",
    "            wildcard (str, optional): Glob pattern to locate the parquet files (default '*/*/*.parquet').\n",
    "        \"\"\"\n",
    "        path_str = str(Path(base_path) / wildcard)\n",
    "        query = f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {table_name} AS \n",
    "        SELECT * FROM read_parquet('{path_str}', hive_partitioning=true)\n",
    "        \"\"\"\n",
    "        self.con.execute(query)\n",
    "        self.registered_tables.append(table_name)\n",
    "        print(f\"Partitioned view '{table_name}' created for files at '{path_str}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DuckDBWrapper (in-memory DuckDB instance) You can connect directly to a DuckDB file by adding the path like con = DuckDBWrapper()\n",
    "con = DuckDBWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register data\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "repo_root = Path.cwd().resolve().parents[0]  # Adjust to locate the repo root\n",
    "\n",
    "BASE_PATH = \"data/opendata\"\n",
    "\n",
    "# Define table names that match the folder names under BASE_PATH\n",
    "bulk_table_names = [\n",
    "    \"mta_operations_statement\",\n",
    "    \"mta_hourly_subway_socrata\",\n",
    "    \"mta_daily_ridership\",\n",
    "    \"mta_bus_wait_time\",\n",
    "    \"mta_bus_speeds\",    \n",
    "    \"daily_weather_asset\",\n",
    "    \"hourly_weather_asset\",\n",
    "\n",
    "]\n",
    "\n",
    "# Use bulk_register_data to register them all with a single call\n",
    "con.bulk_register_data(repo_root=repo_root, base_path=BASE_PATH, table_names=bulk_table_names, wildcard=\"*.parquet\")\n",
    "\n",
    "#Example of registing random CSV, parquet, and JSON files based on a path\n",
    "#paths = [\n",
    "#    \"your/computer/data/exports/row_count.csv\",\n",
    "#    \"your/computer/data/exports/row_count.json\",\n",
    "#    \"your/computer/data/exports/row_count.parquet\"\n",
    "#]\n",
    "#table_names = [\n",
    "#   \"row_count_csv_table\",\n",
    "#   \"row_count_json_table\",\n",
    "#   \"row_count_parquet_table\"\n",
    "#]\n",
    "#con.register_data(paths, table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "\n",
    "SELECT * from mta_hourly_subway_socrata  limit 20000\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want a better looking table, set show_results=True. I'd recomend capping the limit at about 50 rows\n",
    "#T\n",
    "\n",
    "query = f\"\"\"\n",
    "\n",
    "SELECT * from mta_hourly_subway_socrata limit 50\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query,show_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More complicated query\n",
    "\n",
    "query = f\"\"\"\n",
    "\n",
    "WITH weekly_ridership AS (\n",
    "    SELECT \n",
    "        station_complex, \n",
    "        DATE_TRUNC('week', transit_timestamp) AS week_start,\n",
    "        SUM(ridership) AS total_weekly_ridership,\n",
    "        MIN(latitude) AS latitude,  -- Assuming latitude is the same for each station complex, use MIN() or MAX()\n",
    "        MIN(longitude) AS longitude  -- Assuming longitude is the same for each station complex, use MIN() or MAX()\n",
    "    FROM \n",
    "        mta_hourly_subway_socrata\n",
    "    GROUP BY \n",
    "        station_complex, \n",
    "        DATE_TRUNC('week', transit_timestamp)\n",
    "),\n",
    "weekly_weather AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('week', date) AS week_start,\n",
    "        AVG(temperature_mean) AS avg_weekly_temperature,\n",
    "        SUM(precipitation_sum) AS total_weekly_precipitation\n",
    "    FROM \n",
    "        daily_weather_asset\n",
    "    GROUP BY \n",
    "        DATE_TRUNC('week', date)\n",
    ")\n",
    "SELECT \n",
    "    wr.station_complex, \n",
    "    wr.week_start, \n",
    "    wr.total_weekly_ridership,\n",
    "    wr.latitude,\n",
    "    wr.longitude,\n",
    "    ww.avg_weekly_temperature,\n",
    "    ww.total_weekly_precipitation\n",
    "FROM \n",
    "    weekly_ridership wr\n",
    "LEFT JOIN \n",
    "    weekly_weather ww\n",
    "ON \n",
    "    wr.week_start = ww.week_start\n",
    "WHERE \n",
    "    wr.week_start < '2024-09-17'\n",
    "ORDER BY \n",
    "    wr.station_complex, \n",
    "    wr.week_start\n",
    " \n",
    "\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query,show_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tables registered\n",
    "con.show_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema of a specific table\n",
    "con.show_schema(\"mta_hourly_subway_socrata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = Path.cwd().resolve().parents[0]  # Adjust to locate the repo root\n",
    "base_path = repo_root / \"data/exports\"\n",
    "file_name = \"mta_hourly_subway_socrata_data_sample\"\n",
    "file_type= \"csv\"\n",
    "# Export the query result to CSV\n",
    "con.export(result, file_type=file_type, base_path=base_path, file_name=file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
