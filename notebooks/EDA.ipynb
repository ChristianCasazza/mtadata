{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from pathlib import Path\n",
    "import pandas as pd  # Ensure pandas is imported\n",
    "\n",
    "class DuckDBWrapper:\n",
    "    def __init__(self, duckdb_path=None):\n",
    "        \"\"\"\n",
    "        Initialize a DuckDB connection.\n",
    "        If duckdb_path is provided, a persistent DuckDB database will be used.\n",
    "        Otherwise, it creates an in-memory database.\n",
    "        \"\"\"\n",
    "        if duckdb_path:\n",
    "            self.con = duckdb.connect(str(duckdb_path), read_only=False)\n",
    "        else:\n",
    "            self.con = duckdb.connect(database=':memory:', read_only=False)\n",
    "        self.registered_tables = []\n",
    "        \n",
    "        # Enable httpfs for remote paths if needed\n",
    "        self.con.execute(\"INSTALL httpfs;\")\n",
    "        self.con.execute(\"LOAD httpfs;\")\n",
    "\n",
    "    def register_data(self, paths, table_names):\n",
    "        \"\"\"\n",
    "        Registers local data files (Parquet, CSV, JSON) in DuckDB by creating views.\n",
    "        Automatically detects the file type based on the file extension.\n",
    "\n",
    "        Args:\n",
    "            paths (list): List of paths (strings or Path objects) to data files.\n",
    "            table_names (list): List of table names corresponding to the paths.\n",
    "        \"\"\"\n",
    "        if len(paths) != len(table_names):\n",
    "            raise ValueError(\"The number of paths must match the number of table names.\")\n",
    "\n",
    "        for path, table_name in zip(paths, table_names):\n",
    "            path_str = str(path)\n",
    "            file_extension = Path(path_str).suffix.lower()\n",
    "\n",
    "            if file_extension == \".parquet\":\n",
    "                query = f\"CREATE VIEW {table_name} AS SELECT * FROM read_parquet('{path_str}')\"\n",
    "            elif file_extension == \".csv\":\n",
    "                query = f\"CREATE VIEW {table_name} AS SELECT * FROM read_csv_auto('{path_str}')\"\n",
    "            elif file_extension == \".json\":\n",
    "                query = f\"CREATE VIEW {table_name} AS SELECT * FROM read_json_auto('{path_str}')\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type '{file_extension}' for file: {path_str}\")\n",
    "\n",
    "            self.con.execute(query)\n",
    "            self.registered_tables.append(table_name)\n",
    "\n",
    "    def bulk_register_data(self, repo_root, base_path, table_names, wildcard=\"*.parquet\"):\n",
    "        \"\"\"\n",
    "        Constructs paths for each table based on a shared base path plus the table name,\n",
    "        then appends a wildcard for file matching (e.g., '*.parquet'), and registers the data.\n",
    "\n",
    "        Args:\n",
    "            repo_root (Path): The root path of your repository.\n",
    "            base_path (str): The relative path from repo_root to your data directory.\n",
    "            table_names (list): The table names (and implicitly the folder names) to register.\n",
    "            wildcard (str, optional): A wildcard pattern for the files (default '*.parquet').\n",
    "\n",
    "        Example:\n",
    "            If repo_root=/path/to/repo and base_path='data/opendata/nyc/mta', \n",
    "            and table_name='mta_operations_statement', then the full path is:\n",
    "                /path/to/repo/data/opendata/nyc/mta/mta_operations_statement/*.parquet\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "        for table_name in table_names:\n",
    "            # Build the folder path plus wildcard\n",
    "            path = Path(repo_root) / base_path / table_name / wildcard\n",
    "            paths.append(path)\n",
    "\n",
    "        self.register_data(paths, table_names)\n",
    "\n",
    "    def run_query(self, sql_query):\n",
    "        \"\"\"\n",
    "        Runs a SQL query on the registered tables in DuckDB.\n",
    "        \n",
    "        Args:\n",
    "            sql_query (str): The SQL query string to execute.\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Query result as a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        return self.con.execute(sql_query).fetchdf()\n",
    "\n",
    "    def _construct_path(self, path, base_path, file_name, extension):\n",
    "        \"\"\"\n",
    "        Constructs the full file path based on input parameters.\n",
    "        \"\"\"\n",
    "        if path:\n",
    "            return Path(path)\n",
    "        elif base_path and file_name:\n",
    "            return Path(base_path) / f\"{file_name}.{extension}\"\n",
    "        else:\n",
    "            # Default file path: \"output.<extension>\" in the current directory\n",
    "            return Path(f\"output.{extension}\")\n",
    "\n",
    "    def export(self, result, file_type, path=None, base_path=None, file_name=None, with_header=True):\n",
    "        \"\"\"\n",
    "        Exports a query result to the specified file type.\n",
    "        Handles Arrow Tables, Pandas DataFrames, and DuckDB query results.\n",
    "\n",
    "        Args:\n",
    "            result (any): Query result to export (e.g., DuckDB query result, Pandas DataFrame, or Arrow Table).\n",
    "            file_type (str): Type of file to export ('parquet', 'csv', 'json').\n",
    "            path (str): Full path to the file (optional).\n",
    "            base_path (str): Directory path (optional).\n",
    "            file_name (str): Name of the file (without extension) (optional).\n",
    "            with_header (bool): Include header row for CSV files (default: True).\n",
    "        \"\"\"\n",
    "        file_type = file_type.lower()\n",
    "        if file_type not in [\"parquet\", \"csv\", \"json\"]:\n",
    "            raise ValueError(\"file_type must be one of 'parquet', 'csv', or 'json'.\")\n",
    "\n",
    "        full_path = self._construct_path(path, base_path, file_name, file_type)\n",
    "        full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Convert result to a Pandas DataFrame if needed\n",
    "        if isinstance(result, pa.Table):\n",
    "            # Arrow Table to Pandas DataFrame\n",
    "            dataframe = result.to_pandas()\n",
    "        elif hasattr(result, \"to_pandas\"):\n",
    "            # DuckDB result to Pandas DataFrame\n",
    "            dataframe = result.to_pandas()\n",
    "        elif isinstance(result, pd.DataFrame):\n",
    "            # Already a Pandas DataFrame\n",
    "            dataframe = result\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported result type. Must be a Pandas DataFrame, Arrow Table, or DuckDB query result.\")\n",
    "\n",
    "        # Export based on file type\n",
    "        if file_type == \"parquet\":\n",
    "            dataframe.to_parquet(full_path, index=False)\n",
    "        elif file_type == \"csv\":\n",
    "            dataframe.to_csv(full_path, index=False, header=with_header)\n",
    "        elif file_type == \"json\":\n",
    "            dataframe.to_json(full_path, orient='records', lines=True)\n",
    "\n",
    "        print(f\"File written to: {full_path}\")\n",
    "\n",
    "\n",
    "    def show_tables(self):\n",
    "        \"\"\"\n",
    "        Displays the table names and types currently registered in the catalog.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, table_type\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema='main'\n",
    "        \"\"\"\n",
    "        result_df = self.run_query(query)\n",
    "        print(result_df)\n",
    "\n",
    "    def show_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Displays the schema of the specified table.\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): Name of the table whose schema is to be displayed.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            table_name, \n",
    "            column_name, \n",
    "            data_type\n",
    "        FROM \n",
    "            information_schema.columns \n",
    "        WHERE \n",
    "            table_name = '{table_name}'\n",
    "        \"\"\"\n",
    "        result_df = self.run_query(query)\n",
    "        print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DuckDBWrapper (in-memory DuckDB instance)\n",
    "con = DuckDBWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd().resolve().parents[0]  # Adjust to locate the repo root\n",
    "\n",
    "BASE_PATH = \"data/opendata\"\n",
    "\n",
    "# Define table names that match the folder names under BASE_PATH\n",
    "bulk_table_names = [\n",
    "    \"mta_operations_statement\",\n",
    "    \"mta_hourly_subway_socrata\",\n",
    "    \"mta_daily_ridership\",\n",
    "    \"mta_bus_wait_time\",\n",
    "    \"daily_weather_asset\",\n",
    "    \"hourly_weather_asset\",\n",
    "    \"mta_bus_speeds\",\n",
    "]\n",
    "\n",
    "paths = [\n",
    "    repo_root / \"data/exports/row_count.csv\",\n",
    "    \"/home/christiandata/mtadata/data/exports/row_count.json\",\n",
    "    repo_root / \"data/exports/row_count.parquet\"\n",
    "]\n",
    "table_names = [\n",
    "    \"row_count_csv_table\",\n",
    "    \"row_count_json_table\",\n",
    "    \"row_count_parquet_table\"\n",
    "]\n",
    "con.register_data(paths, table_names)\n",
    "\n",
    "# Use bulk_register_data to register them all with a single call\n",
    "con.bulk_register_data(repo_root=repo_root, base_path=BASE_PATH, table_names=bulk_table_names, wildcard=\"*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  table_name table_type\n",
      "0        daily_weather_asset       VIEW\n",
      "1       hourly_weather_asset       VIEW\n",
      "2             mta_bus_speeds       VIEW\n",
      "3          mta_bus_wait_time       VIEW\n",
      "4        mta_daily_ridership       VIEW\n",
      "5  mta_hourly_subway_socrata       VIEW\n",
      "6   mta_operations_statement       VIEW\n",
      "7        row_count_csv_table       VIEW\n",
      "8       row_count_json_table       VIEW\n",
      "9    row_count_parquet_table       VIEW\n"
     ]
    }
   ],
   "source": [
    "# Show the tables registered\n",
    "con.show_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             table_name                             column_name data_type\n",
      "0   mta_daily_ridership                                    date      DATE\n",
      "1   mta_daily_ridership                 subways_total_ridership    DOUBLE\n",
      "2   mta_daily_ridership                subways_pct_pre_pandemic    DOUBLE\n",
      "3   mta_daily_ridership                   buses_total_ridership    DOUBLE\n",
      "4   mta_daily_ridership                  buses_pct_pre_pandemic    DOUBLE\n",
      "5   mta_daily_ridership                    lirr_total_ridership    DOUBLE\n",
      "6   mta_daily_ridership                   lirr_pct_pre_pandemic    DOUBLE\n",
      "7   mta_daily_ridership             metro_north_total_ridership    DOUBLE\n",
      "8   mta_daily_ridership            metro_north_pct_pre_pandemic    DOUBLE\n",
      "9   mta_daily_ridership               access_a_ride_total_trips    DOUBLE\n",
      "10  mta_daily_ridership          access_a_ride_pct_pre_pandemic    DOUBLE\n",
      "11  mta_daily_ridership           bridges_tunnels_total_traffic    DOUBLE\n",
      "12  mta_daily_ridership        bridges_tunnels_pct_pre_pandemic    DOUBLE\n",
      "13  mta_daily_ridership   staten_island_railway_total_ridership    DOUBLE\n",
      "14  mta_daily_ridership  staten_island_railway_pct_pre_pandemic    DOUBLE\n"
     ]
    }
   ],
   "source": [
    "# Show the schema of a specific table\n",
    "con.show_schema(\"mta_daily_ridership\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 fare_class_category   latitude transit_mode  \\\n",
      "0                  Metrocard - Other  40.756805       subway   \n",
      "1   Metrocard - Seniors & Disability  40.775593       subway   \n",
      "2        Metrocard - Unlimited 7-Day  40.706608       subway   \n",
      "3               Metrocard - Students  40.811108       subway   \n",
      "4              Metrocard - Full Fare  40.817894       subway   \n",
      "..                               ...        ...          ...   \n",
      "95       Metrocard - Unlimited 7-Day  40.856094       subway   \n",
      "96             Metrocard - Fair Fare  40.845900       subway   \n",
      "97       Metrocard - Unlimited 7-Day  40.714565       subway   \n",
      "98       Metrocard - Unlimited 7-Day  40.682830       subway   \n",
      "99                 Metrocard - Other  40.811108       subway   \n",
      "\n",
      "    station_complex_id  longitude              station_complex payment_method  \\\n",
      "0                    5  -73.92957                  36 Av (N,W)      metrocard   \n",
      "1                  160  -73.97641                  72 St (C,B)      metrocard   \n",
      "2                  126  -73.92291             Jefferson St (L)      metrocard   \n",
      "3                  153  -73.95235             125 St (A,C,B,D)      metrocard   \n",
      "4                  152  -73.94765                 135 St (C,B)      metrocard   \n",
      "..                 ...        ...                          ...            ...   \n",
      "95                 214  -73.90074            182-183 Sts (B,D)      metrocard   \n",
      "96                 216  -73.91013            174-175 Sts (B,D)      metrocard   \n",
      "97                 122  -73.94405                Graham Av (L)      metrocard   \n",
      "98                 131  -73.90525  Bushwick Av-Aberdeen St (L)      metrocard   \n",
      "99                 153  -73.95235             125 St (A,C,B,D)      metrocard   \n",
      "\n",
      "    ridership transit_timestamp  transfers    borough  \\\n",
      "0           1        2022-02-01          0     Queens   \n",
      "1           3        2022-02-01          2  Manhattan   \n",
      "2           2        2022-02-01          0   Brooklyn   \n",
      "3           2        2022-02-01          0  Manhattan   \n",
      "4           1        2022-02-01          0  Manhattan   \n",
      "..        ...               ...        ...        ...   \n",
      "95          1        2022-02-01          0      Bronx   \n",
      "96          2        2022-02-01          0      Bronx   \n",
      "97         12        2022-02-01          0   Brooklyn   \n",
      "98          1        2022-02-01          0   Brooklyn   \n",
      "99          4        2022-02-01          0  Manhattan   \n",
      "\n",
      "                      geom_wkt  \n",
      "0   POINT(-73.92957 40.756805)  \n",
      "1   POINT(-73.97641 40.775593)  \n",
      "2   POINT(-73.92291 40.706608)  \n",
      "3   POINT(-73.95235 40.811108)  \n",
      "4   POINT(-73.94765 40.817894)  \n",
      "..                         ...  \n",
      "95  POINT(-73.90074 40.856094)  \n",
      "96    POINT(-73.91013 40.8459)  \n",
      "97  POINT(-73.94405 40.714565)  \n",
      "98   POINT(-73.90525 40.68283)  \n",
      "99  POINT(-73.95235 40.811108)  \n",
      "\n",
      "[100 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "\n",
    "SELECT * from row_count_csv_table\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written to: /home/christiandata/mtadata/data/exports/wildcardd/file_2.parquet\n"
     ]
    }
   ],
   "source": [
    "repo_root = Path.cwd().resolve().parents[0]  # Adjust to locate the repo root\n",
    "base_path = repo_root / \"data/exports/wildcardd\"\n",
    "file_name = \"file_2\"\n",
    "file_type= \"parquet\"\n",
    "# Export the query result to CSV\n",
    "con.export(result, file_type=file_type, base_path=base_path, file_name=file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
